[
  {
    "objectID": "glm-practical-logistic-binary.html#libraries-and-functions",
    "href": "glm-practical-logistic-binary.html#libraries-and-functions",
    "title": "Binary response",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\ntidyverse\n\n\n\n\n\n\n\n\n\nLibrary\nDescription\n\n\n\n\ntidyverse\nA collection of R packages designed for data science\n\n\ntidymodels\nA collection of packages for modelling and machine learning using tidyverse principles"
  },
  {
    "objectID": "glm-practical-logistic-binary.html#datasets",
    "href": "glm-practical-logistic-binary.html#datasets",
    "title": "Binary response",
    "section": "Datasets",
    "text": "Datasets\n\nDiabetes\n\n\nThe example in this section uses the following data set:\ndata/diabetes.csv\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose test and the diastolic blood pressure for each of 767 women. The variables are called test_result, glucose and diastolic."
  },
  {
    "objectID": "glm-practical-logistic-binary.html#visualise-the-data",
    "href": "glm-practical-logistic-binary.html#visualise-the-data",
    "title": "Binary response",
    "section": "Visualise the data",
    "text": "Visualise the data\nFirst we load the data, then we visualise it.\n\ntidyverse\n\n\nFirst, we load and inspect the data:\n\ndiabetes <- read_csv(\"data/diabetes.csv\")\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are test result outcomes and not actually numeric representations.\nThis will cause problems later, so we need to tell R to see these values as factors. For good measure we’ll also improve the information in test_result by classifying it as ‘negative’ (0) or ‘positive’ (1).\n\ndiabetes <- \ndiabetes %>% \n  # replace 0 with 'negative' and 1 with 'positive'\n  mutate(test_result = case_when(test_result == 0 ~ \"negative\",\n                                 TRUE ~ \"positive\")) %>% \n  # convert character columns to factor\n  mutate_if(is.character, factor)\n\nWe can plot the data:\n\ndiabetes %>% \n  ggplot(aes(x = test_result, y = glucose)) +\n  geom_boxplot()\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_point()"
  },
  {
    "objectID": "glm-practical-logistic-binary.html#model-building",
    "href": "glm-practical-logistic-binary.html#model-building",
    "title": "Binary response",
    "section": "Model building",
    "text": "Model building\nThere are different ways to construct a logistic model.\n\ntidyverse\n\n\nIn tidymodels we have access to a very useful package: parsnip, which provides a common syntax for a whole range of modelling libraries. This means that the syntax will stay the same as you do different kind of model comparisons. So, the learning curve might be a bit steeper to start with, but this will pay dividend in the long-term (just like when you started using R!).\nFirst, we need to load tidymodels (install it first, if needed):\n\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\n\nThe workflow in parsnip is a bit different to what we’re used to so far. Up until now, we’ve directly used the relevant model functions to analyse our data, for example using the lm() function to create linear models.\nUsing parsnip we approach things in a more systematic manner. At first this might seem unnecessarily verbose, but there are clear advantages to approaching your analysis in a systematic way. For example, it will be straightforward to implement other types of models using the same workflow, which you’ll definitely find useful when moving on to more difficult modelling tasks.\nUsing tidymodels we specify a model in three steps:\n\nSpecify the type of model based on its mathematical structure (e.g., linear regression, random forest, K-nearest neighbors, etc).\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model can only create one type of model, such as logistic regression, the mode is already set.\nSpecify the engine for fitting the model. This usually is the software package or library that should be used.\n\nSo, we can create the model as follows:\n\ndia_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\n\nNote that we are not actually specifying any of the variables just yet! All we’ve done is tell R what kind of model we’re planning to use. If we want to see how parsnip converts this code to the package syntax, we can check this with translate():\n\ndia_mod %>% translate()\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\nModel fit template:\nstats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    family = stats::binomial)\n\n\nThis shows that we have a logistic regression model, where the outcome is going to be a classification (in our case, that’s a positive or negative test result). The model fit template tells us that we’ll be using the glm() function from the stats package, which can take a formula, data, weights and family argument. The family argument is already set to binomial.\nNow we’ve specified what kind of model we’re planning to use, we can fit our data to it, using the fit() function:\n\ndia_fit <- dia_mod %>% \n  fit(test_result ~ glucose,\n      data = diabetes)\n\nWe can look at the output directly, but I prefer to tidy the data up using the tidy() function from broom package:\n\ndia_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -5.61     0.442       -12.7 6.90e-37\n2 glucose       0.0395   0.00340      11.6 2.96e-31\n\n\nThe estimate column gives you the coefficients of the logistic model equation. We could use these to calculate the probability of having a positive diabetes test, for any given glucose level, using the following equation:\n\\[\\begin{equation}\nP(positive \\ test \\ result) = \\frac{1}{1 + {e}^{-(-5.61 +  0.040 \\cdot glucose)}}\n\\end{equation}\\]\nBut of course we’re not going to do it that way. We’ll let R deal with that in the next section.\nThe std.error column gives you the error associated with the coefficients and the statistic column tells you the statistic value.\nThe values in p.value merely show whether that particular coefficient is significantly different from zero. This is similar to the p-values obtained in the summary output of a linear model, and as before, for continuous predictors these p-values can be used as a rough guide as to whether that predictor is important (so in this case glucose appears to be significant). However, these p-values aren’t great when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a p-value for each level rather than for the predictor as a whole)."
  },
  {
    "objectID": "glm-practical-logistic-binary.html#model-predictions",
    "href": "glm-practical-logistic-binary.html#model-predictions",
    "title": "Binary response",
    "section": "Model predictions",
    "text": "Model predictions\nWhat if we got some new glucose level data and we wanted to predict if people might have diabetes or not?\nWe could use the existing model and feed it the some data:\n\ntidyverse\n\n\n\n# create a dummy data set using some hypothetical glucose measurements\ndiabetes_newdata <- tibble(glucose = c(188, 122, 83, 76, 144))\n\n# predict if the patients have diabetes or not\naugment(dia_fit,\n        new_data = diabetes_newdata)\n\n# A tibble: 5 × 4\n  glucose .pred_class .pred_negative .pred_positive\n    <dbl> <fct>                <dbl>          <dbl>\n1     188 positive             0.140         0.860 \n2     122 negative             0.688         0.312 \n3      83 negative             0.912         0.0885\n4      76 negative             0.931         0.0686\n5     144 positive             0.481         0.519 \n\n\nAlthough you are able to get the predicted outcomes (in .pred_class), I would like to stress that this is not the point of running the model. It is important to realise that the model (as with all statistical models) creates a predicted outcome based on certain probabilities. It is therefore much more informative to look at how probable these predicted outcomes are. They are encoded in .pred_negative and .pred_positive.\nFor the first value this means that there is a 14% chance that the diabetes test will return a negative result and around 86% chance that it will return a positive result."
  },
  {
    "objectID": "glm-practical-logistic-binary.html#exercise-penguins",
    "href": "glm-practical-logistic-binary.html#exercise-penguins",
    "title": "Binary response",
    "section": "Exercise: Penguins",
    "text": "Exercise: Penguins\nTo practice this a bit more, we’ll be using a data set about penguins. The data are from the palmerpenguins package, which is included with tidymodels. The data set contains information on penguins at the Palmer Station on Antarctica. Chilly.\nHave a look at the plot below, where we are comparing the bill length (bill_length_mm) of three species of penguins (species) against flipper length (flipper_length_mm).\nWe are also colouring the data based on sex (sex) and for good measure we’re also including information on the body size (body_mass_g).\n\n\n\n\n\nIt looks like female penguins are smaller with different sized bills and it would be interesting (yes, it would!) to investigate this further.\nI would like you to do the following:\n\nload the data into an object called penguins using data(\"penguins\")\ncreate a logistic model and fit the data to it, using sex as a classifier\nis bill length an important indicator of sex?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntidyverse\n\n\nFirst, we load the data:\n\ndata(\"penguins\")\n\nWe already have a reasonably good idea of what we’re looking at, but it can never hurt to understand your data better, so:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  \n  <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n1 Adelie  Torge…           39.1          18.7              181        3750 male \n2 Adelie  Torge…           39.5          17.4              186        3800 fema…\n3 Adelie  Torge…           40.3          18                195        3250 fema…\n4 Adelie  Torge…           NA            NA                 NA          NA <NA> \n5 Adelie  Torge…           36.7          19.3              193        3450 fema…\n6 Adelie  Torge…           39.3          20.6              190        3650 male \n# … with 1 more variable: year <int>\n\n\nThis shows that there are a few other columns in our data set, namely island, indicating the island where the penguins are residing and bill_depth_mm which records the bill depth.\nWe also notice that there are some missing values. It would be good to get rid of these, at least for the rows where there sex isn’t scored:\n\npenguins <- penguins %>% \n  filter(!is.na(sex))\n\nNext, we specify the type of model. Notice that it can be useful to use a prefix in the naming of these objects to indicate which data set your model belongs to. Here we’re using pgn to denote penguins.\n\npgn_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\n\nRemember, that setting the model specification does not yet define the model itself. We do that as follows:\n\npgn_fit <- pgn_mod %>% \n  fit(sex ~ bill_length_mm,\n      data = penguins)\n\nOnce we’ve fitted the data to the model, we can have a look at the model parameters:\n\npgn_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term           estimate std.error statistic       p.value\n  <chr>             <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)      -6.04     1.01       -5.96 0.00000000247\n2 bill_length_mm    0.138    0.0229      6.02 0.00000000176\n\n\nThe model parameters tell us that both the intercept and the coefficient for bill_length_mm are significantly different from zero. So it seems that bill length is an important predictor of the sex of these penguins. Who knew?!"
  },
  {
    "objectID": "glm-practical-logistic-binary.html#model-evaluation",
    "href": "glm-practical-logistic-binary.html#model-evaluation",
    "title": "Binary response",
    "section": "Model evaluation",
    "text": "Model evaluation\nSo far we’ve constructed the logistic model and fed it some new data to make predictions to the possible outcome of a diabetes test, depending on the glucose level of a given patient. This gave us some diabetes test predictions and, importantly, the probabilities of whether the test could come back negative or positive.\nThe question we’d like to ask ourselves at this point: how reliable is the model?\nTo explore this, we need to take a step back.\n\nSplit the data\nWhen we created the model, we used all of the data. However, a good way of assessing a model fit is to actually split the data into two:\n\na training data set that you use to fit your model\na test data set to validate your model and measure model performance\n\nBefore we split the data, let’s have a closer look at the data set. If we count how many diabetes test results are negative and positive, we see that these counts are not evenly split.\n\ntidyverse\n\n\n\ndiabetes %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  test_result     n  prop\n  <fct>       <int> <dbl>\n1 negative      478 0.657\n2 positive      250 0.343\n\n\nThis can have some consequences if we start splitting our data into a training and test set. By splitting the data into two parts - where most of the data goes into your training set - you have data left afterwards that you can use to test how good the predictions of your model are. However, we need to make sure that the proportion of negative and positive diabetes test outcomes remains roughly the same.\nThe rsample package has a couple of useful functions that allow us to do just that and we can use the strata argument to keep these proportions more or less constant.\n\n# ensures random data split is reproducible\nset.seed(123)\n\n# split the data, basing the proportions on the diabetes test results\ndata_split <- initial_split(diabetes, strata = test_result)\n\n# create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\nWe can check what the initial_split() function has done:\n\n# proportion of data allocated to the training set\nnrow(train_data) / nrow(diabetes)\n\n[1] 0.7486264\n\n# proportion of diabetes test results for the training data set\ntrain_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  test_result     n  prop\n  <fct>       <int> <dbl>\n1 negative      358 0.657\n2 positive      187 0.343\n\n# proportion of diabetes test results for the test data set\ntest_data %>% \n  count(test_result) %>% \n  mutate(prop = n/sum(n))\n\n# A tibble: 2 × 3\n  test_result     n  prop\n  <fct>       <int> <dbl>\n1 negative      120 0.656\n2 positive       63 0.344\n\n\nFrom the output we can see that around 75% of the data set has been used to create a training data set, with the remaining 25% kept as a test set.\nFurthermore, the proportions of negative:positive are kept more or less constant.\n\nCreate a recipe\n\n# Create a recipe\ndia_rec <- \n  recipe(test_result ~ glucose, data = train_data)\n\n# Look at the recipe summary\nsummary(dia_rec)\n\n# A tibble: 2 × 4\n  variable    type    role      source  \n  <chr>       <chr>   <chr>     <chr>   \n1 glucose     numeric predictor original\n2 test_result nominal outcome   original\n\n\n\n\nBuild a model specification\n\ndia_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\n\n\nUse recipe as we train and test our model\n\ndia_wflow <- \n  workflow() %>% \n  add_model(dia_mod) %>% \n  add_recipe(dia_rec)\n\ndia_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nAlthough it seems a bit of overkill, we now have a single function that can we can use to prepare the recipe and train the model from the resulting predictors:\n\ndia_fit <- \n  dia_wflow %>% \n  fit(data = train_data)\n\nThis creates an object called dia_fit, which contains the final recipe and fitted model objects. We can extract the model and recipe objects with several helper functions:\n\ndia_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -5.72     0.513       -11.2 6.84e-29\n2 glucose       0.0406   0.00397      10.2 1.46e-24\n\n\n\n\nUse trained workflow for predictions\nSo far, we have done the following:\n\nBuilt the model (dia_mod),\nCreated a pre-processing recipe (dia_rec),\nCombined the model and recipe into a workflow (dia_wflow)\nTrained our workflow using the fit() function (dia_fit)\n\nThe results we generated above do not differ much from the values we obtained with the entire data set. However, these are based on 3/4 of the data (our training data set). Because of this, we still have our test data set available to apply this workflow to data the model has not yet seen.\n\ndia_aug <- \naugment(dia_fit, test_data)\n\ndia_aug\n\n# A tibble: 183 × 6\n   glucose diastolic test_result .pred_class .pred_negative .pred_positive\n     <dbl>     <dbl> <fct>       <fct>                <dbl>          <dbl>\n 1      85        66 negative    negative            0.906          0.0938\n 2     183        64 positive    positive            0.152          0.848 \n 3     168        74 positive    positive            0.249          0.751 \n 4     166        72 positive    positive            0.264          0.736 \n 5     115        70 positive    negative            0.740          0.260 \n 6      99        84 negative    negative            0.845          0.155 \n 7     196        90 positive    positive            0.0959         0.904 \n 8     119        80 positive    negative            0.708          0.292 \n 9     143        94 positive    positive            0.478          0.522 \n10      97        66 negative    negative            0.856          0.144 \n# … with 173 more rows\n\n\n\n\nEvaluate the model\nWe can now evaluate the model. One way of doing this is by using the area under the ROC curve as a metric.\nAn ROC curve (receiver operating characteristic curve - the name being a strange relic of WWII where developed for operators of military radar receivers) plots the true-positive rate (TPR) against the false-positive rate (FPR) at varying thresholds.\nThe true-positive rate is also known as sensitivity, whereas the false-positive rate is 1 - sensitivity (which, if you recall from the session of Power Analysis is also known as the power.)\n\ndia_aug %>% \n  roc_curve(truth = test_result, .pred_negative) %>% \n  autoplot()\n\n\n\n\nThe area under the ROC curve, which is known as the AUC provides an aggregate measure of performance across all possible classification thresholds.\nIt ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0. A model whose predictions are 100% correct has an AUC of 1.0.\n\ndia_aug %>% \n  roc_auc(truth = test_result, .pred_negative)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.766\n\n\nIn addition to the ROC curve and AUC we also have a whole range of model parameters associated with the fitted model. We’re not going through all of them at this point, but one in particular should be familiar.\nWe extract these parameters as follows:\n\ndia_fit %>% glance()\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          701.     544  -277.  558.  567.     554.         543   545\n\n\nHere we see the Akaike Information Criterion (AIC) as an output. Remember, the value of the AIC in itself is meaningless, but it’s useful to compare relative to AICs of other models. We covered how to do this in the Power analysis session of the Core statistics course.\nHere we see that the AIC for this model that uses the glucose level as a single predictor for the diabetes test result is 558."
  },
  {
    "objectID": "glm-practical-logistic-binary.html#exercise---diabetes-predictors",
    "href": "glm-practical-logistic-binary.html#exercise---diabetes-predictors",
    "title": "Binary response",
    "section": "Exercise - Diabetes predictors",
    "text": "Exercise - Diabetes predictors\nUsing the training and test diabetes data sets, investigate the relationship between test_result and both glucose and diastolic. Try to answer the following:\n\ndoes adding diastolic to the model markedly improve the reliability of the predictions?\nwhat do the AICs for the two models tell you?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntidyverse\n\n\n\n# Update the recipe\ndia_rec <- \n  recipe(test_result ~ glucose + diastolic,\n         data = train_data)\n\n# Look at the recipe summary\nsummary(dia_rec)\n\n# A tibble: 3 × 4\n  variable    type    role      source  \n  <chr>       <chr>   <chr>     <chr>   \n1 glucose     numeric predictor original\n2 diastolic   numeric predictor original\n3 test_result nominal outcome   original\n\n\nBuild the model, if needed (we have done this already and it stays the same):\n\ndia_mod <- \n  logistic_reg() %>% \n  set_engine(\"glm\")\n\nCreate a workflow…\n\ndia_wflow <- \n  workflow() %>% \n  add_model(dia_mod) %>% \n  add_recipe(dia_rec)\n\n… and fit the data:\n\ndia_fit <- \n  dia_wflow %>% \n  fit(data = train_data)\n\nExtract the model parameters to have a look:\n\ndia_fit %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  -6.99     0.790       -8.85 8.60e-19\n2 glucose       0.0394   0.00398      9.88 5.19e-23\n3 diastolic     0.0195   0.00877      2.22 2.61e- 2\n\n\nApply the fitted model to the test data set:\n\ndia_aug <- \naugment(dia_fit, test_data)\n\ndia_aug\n\n# A tibble: 183 × 6\n   glucose diastolic test_result .pred_class .pred_negative .pred_positive\n     <dbl>     <dbl> <fct>       <fct>                <dbl>          <dbl>\n 1      85        66 negative    negative            0.914          0.0862\n 2     183        64 positive    positive            0.189          0.811 \n 3     168        74 positive    positive            0.257          0.743 \n 4     166        72 positive    positive            0.280          0.720 \n 5     115        70 positive    negative            0.751          0.249 \n 6      99        84 negative    negative            0.811          0.189 \n 7     196        90 positive    positive            0.0776         0.922 \n 8     119        80 positive    negative            0.679          0.321 \n 9     143        94 positive    positive            0.385          0.615 \n10      97        66 negative    negative            0.869          0.131 \n# … with 173 more rows\n\n\nPlot the ROC curve:\n\ndia_aug %>% \n  roc_curve(truth = test_result, .pred_negative) %>% \n  autoplot()\n\n\n\n\nAnd get the area under the ROC curve:\n\ndia_aug %>% \n  roc_auc(truth = test_result, .pred_negative)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.761\n\n\nAnother way to assess the model fit is to look at the Akaike Information Criterion (AIC).\n\ndia_fit %>% glance()\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          701.     544  -275.  555.  568.     549.         542   545\n\n\nWe get an AIC of 555, which is lower than the AIC of 558 that we got with just glucose as a predictor variable.\n\n\n\n\nConclusions\nAdding the diastolic variable as a predictor to the model does not seem to have much of an effect on the model reliability, since the AUC is 0.761 with the extra parameter, versus 0.766 without.\nThe AIC on the other hand suggests that the additive model we’ve analysed here has a better fit than the original model (AIC of 555 vs 558).\n\n\nFood for thought\nPerhaps there is an interaction between glucose and diastolic, which would be interesting to investigate."
  },
  {
    "objectID": "glm-practical-logistic-binary.html#key-points",
    "href": "glm-practical-logistic-binary.html#key-points",
    "title": "Binary response",
    "section": "Key points",
    "text": "Key points\n\n\n\n\n\n\nNote\n\n\n\n\nWe use a logistic regression to model a binary response\nModel suitability can be checked by splitting the data into a training and test data set. The logistic model is then created based on the training data, and the reliability can be checked against the (known) values in the test data set\nThe ROC curve shows the performance of a classification model at all thresholds, whereas the area under the ROC curve provides an aggregate measure of performance of all possible classifications thresholds"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Generalised linear models",
    "section": "",
    "text": "Welcome to the wonderful world of generalised linear models!\nThese sessions are intended to enable you to construct and use generalised linear models confidently.\nAs with all of our statistics courses, we use:\nAt the same time this is not a “how to mindlessly use a stats program” course!"
  },
  {
    "objectID": "index.html#core-aims",
    "href": "index.html#core-aims",
    "title": "Generalised linear models",
    "section": "Core aims",
    "text": "Core aims\nTo introduce sufficient understanding and coding experience for analysing data with non-continuous response variables.\n\n\n\n\n\n\nCourse aims\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nConstruct\n\na logistic model for binary response variables\na logistic model for proportion response variables\na Poisson model for count response variables\na Negative Binomial model for count response variables\n\nPlot the data and the fitted curve in each case for both continuous and categorical predictors\nAssess the significance of fit\nAssess assumption of the model"
  },
  {
    "objectID": "index.html#index-datasets",
    "href": "index.html#index-datasets",
    "title": "Generalised linear models",
    "section": "Datasets",
    "text": "Datasets\nThis course uses various data sets. Download the data folder here by right-clicking on the link and Save as…. Next unzip the file and copy it into your working directory. Your data should then be accessible via <working-directory-name>/data/."
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#libraries-and-functions",
    "href": "glm-practical-logistic-proportion.html#libraries-and-functions",
    "title": "Proportional response",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\ntidyverse\n\n\n\n\n\n\n\n\n\nLibrary\nDescription\n\n\n\n\ntidyverse\nA collection of R packages designed for data science\n\n\ntidymodels\nA collection of packages for modelling and machine learning using tidyverse principles"
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#datasets",
    "href": "glm-practical-logistic-proportion.html#datasets",
    "title": "Proportional response",
    "section": "Datasets",
    "text": "Datasets\n\nChallenger\n\n\nThe example in this section uses the following data set:\ndata/challenger.csv\nThese data, obtained from the faraway package, contain information related to the explosion of the USA Space Shuttle Challenger on 28 January, 1986. An investigation after the disaster traced back to certain joints on one of the two solid booster rockets, each containing two O-rings (primary and secondary) that ensured no exhaust gases could escape from the booster.\nThe night before the launch was unusually cold, with temperatures below freezing. The final report suggested that the cold snap during the night made the o-rings stiff, and unable to adjust to changes in pressure. As a result, exhaust gases leaked away from the solid booster rockets, causing one of them to break loose and rupture the main fuel tank, leading to the final explosion.\nThe question we’re trying to answer in this session is: based on the data from the previous flights, would it have been possible to predict the failure of most both o-rings on the Challenger flight?"
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#visualise-the-data",
    "href": "glm-practical-logistic-proportion.html#visualise-the-data",
    "title": "Proportional response",
    "section": "Visualise the data",
    "text": "Visualise the data\nFirst, we read in the data:\n\ntidyverse\n\n\n\nchallenger <- read_csv(\"data/challenger.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nchallenger\n\n# A tibble: 23 × 2\n    temp damage\n   <dbl>  <dbl>\n 1    53      5\n 2    57      1\n 3    58      1\n 4    63      1\n 5    66      0\n 6    67      0\n 7    67      0\n 8    67      0\n 9    68      0\n10    69      0\n# … with 13 more rows\n\n\n\n\n\nThe data set contains several columns:\n\ntemp, the launch temperature in degrees Fahrenheit\ndamage, the number of o-rings that showed erosion\n\nBefore we have a further look at the data, let’s calculate the proportion of damaged o-rings (prop_damaged) and the total number of o-rings (total) and update our data set.\n\ntidyverse\n\n\n\nchallenger <-\nchallenger %>%\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nchallenger\n\n# A tibble: 23 × 5\n    temp damage total intact prop_damaged\n   <dbl>  <dbl> <dbl>  <dbl>        <dbl>\n 1    53      5     6      1        0.833\n 2    57      1     6      5        0.167\n 3    58      1     6      5        0.167\n 4    63      1     6      5        0.167\n 5    66      0     6      6        0    \n 6    67      0     6      6        0    \n 7    67      0     6      6        0    \n 8    67      0     6      6        0    \n 9    68      0     6      6        0    \n10    69      0     6      6        0    \n# … with 13 more rows\n\n\n\n\n\nPlotting the proportion of damaged o-rings against the launch temperature shows the following picture:\n\ntidyverse\n\n\n\nggplot(challenger, aes(x = temp, y = prop_damaged)) +\n  geom_point()\n\n\n\n\n\n\n\nThe point on the left is the data point corresponding to the coldest flight experienced before the disaster, where five damaged o-rings were found. Fortunately, this did not result in a disaster.\nHere we’ll explore if we could have predicted the failure of both o-rings on the Challenger flight, where the launch temperature was 31 degrees Fahrenheit."
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#model-building",
    "href": "glm-practical-logistic-proportion.html#model-building",
    "title": "Proportional response",
    "section": "Model building",
    "text": "Model building\nThere is little point in evaluating the model using a training/test data set, since there are only 23 data points in total. So we’re building a model and testing that on the available data.\n\ntidyverse\n\n\nWe are using a logistic regression for a proportion response in this case, since we’re interested in the proportion of o-rings that are damaged.\nThe logistic_reg() function we used in the binary response section does not work here, because it expects a binary (yes/no; positive/negative; 0/1 etc) response.\nTo deal with that, we are using the standard linear_reg() function, still using the glm or generalised linear model engine, with the family or error distribution set to binomial (as before).\nFirst we set the model specification:\n\nchl_mod <- linear_reg(mode = \"regression\") %>%\n  set_engine(\"glm\", family = \"binomial\")\n\nThen we fit the data. Fitting the data for proportion responses is a bit annoying, where you have to give the glm model a two-column matrix to specify the response variable.\nHere, the first column corresponds to the number of damaged o-rings, whereas the second column refers to the number of intact o-rings. We use the cbind() function to bind these two together into a matrix.\n\nchl_fit <- chl_mod %>% \n  fit(cbind(damage, intact) ~ temp,\n      data = challenger)\n\nNext, we can have a closer look at the results:\n\nchl_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   11.7      3.30        3.54 0.000403 \n2 temp          -0.216    0.0532     -4.07 0.0000478\n\n\nWe can see that the p-values of the intercept and temp are significant. We can also use the intercept and temp coefficients to construct the logistic equation, which we can use to sketch the logistic curve.\n\\[\\begin{equation}\nP(o-ring \\ failure) = \\frac{1}{1 + {e}^{-(11.66 -  0.22 \\cdot temp)}}\n\\end{equation}\\]\nLet’s see how well our model would have performed if we would have fed it the data from the ill-fated Challenger launch.\nFirst we generate a table with data for a range of temperatures, from 25 to 85 degrees Fahrenheit, in steps of 1. We can then use these data to generate the logistic curve, based on the fitted model.\n\nmodel <- tibble(temp = seq(25, 85, 1))\n\n\n# get the predicted proportions for the curve\ncurve <- chl_fit %>% augment(new_data = model)\n\n# plot the curve and the original data\nggplot(curve, aes(temp, .pred)) +\n  geom_line(colour = \"red\") +\n  geom_point(data = challenger, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")\n\n\n\n\nIt seems that there was a high probability of both o-rings failing at that launch temperature. One thing that the graph shows is that there is a lot of uncertainty involved in this model."
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#exercise---predicting-failure",
    "href": "glm-practical-logistic-proportion.html#exercise---predicting-failure",
    "title": "Proportional response",
    "section": "Exercise - predicting failure",
    "text": "Exercise - predicting failure\nThe data point at 53 degrees Fahrenheit is quite influential for the analysis. Remove this data point and repeat the analysis. Is there still a predicted link between launch temperature and o-ring failure?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntidyverse\n\n\nFirst, we need to remove the influential data point:\n\nchallenger_new <- challenger %>% filter(temp != 53)\n\nWe can reuse the model specification, but we do have to update our fit:\n\nchl_new_fit <- chl_mod %>% \n  fit(cbind(damage, intact) ~ temp,\n      data = challenger_new)\n\n\n# get the predicted proportions for the curve\ncurve_new <- chl_new_fit %>% augment(new_data = model)\n\n# plot the curve and the original data\nggplot(curve_new, aes(temp, .pred)) +\n  geom_line(colour = \"red\") +\n  geom_point(data = challenger_new, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")\n\n\n\n\nThe prediction proportion of damaged o-rings is markedly less in this scenario, with a failure rate of around 80%. The original fitted curve already had quite some uncertainty associated with it, but the uncertainty of this model is much greater."
  },
  {
    "objectID": "glm-practical-logistic-proportion.html#key-points",
    "href": "glm-practical-logistic-proportion.html#key-points",
    "title": "Proportional response",
    "section": "Key points",
    "text": "Key points\n\n\n\n\n\n\nNote\n\n\n\n\nWe can use a logistic model for proportion response variables"
  },
  {
    "objectID": "glm-practical-poisson.html#libraries-and-functions",
    "href": "glm-practical-poisson.html#libraries-and-functions",
    "title": "Poisson regression",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\ntidyverse\n\n\n\n\n\n\n\n\n\nLibrary\nDescription\n\n\n\n\ntidyverse\nA collection of R packages designed for data science\n\n\ntidymodels\nA collection of packages for modelling and machine learning using tidyverse principles\n\n\npoissonreg\nEnables the parsnip package to fit various types of Poisson regression models"
  },
  {
    "objectID": "glm-practical-poisson.html#datasets",
    "href": "glm-practical-poisson.html#datasets",
    "title": "Poisson regression",
    "section": "Datasets",
    "text": "Datasets\n\nIslandsSeatbelts\n\n\nThe example in this section uses the following data set:\ndata/islands.csv\nThis is a data set comprising 35 observations of two variables (one dependent and one predictor). This records the number of species recorded on different small islands along with the area (km2)of the islands. The variables are species and area.\n\n\nThe seatbelts data set is a multiple time-series data set that was commissioned by the Department of Transport in 1984 to measure differences in deaths before and after front seatbelt legislation was introduced on 31st January 1983. It provides monthly total numerical data on a number of incidents including those related to death and injury in Road Traffic Accidents (RTA’s). The data set starts in January 1969 and observations run until December 1984.\nYou can find the file in data/seatbelts.csv"
  },
  {
    "objectID": "glm-practical-poisson.html#visualise-the-data",
    "href": "glm-practical-poisson.html#visualise-the-data",
    "title": "Poisson regression",
    "section": "Visualise the data",
    "text": "Visualise the data\nA good first step is always to explore your data prior to any further analysis.\n\ntidyverse\n\n\nFirst, we load and inspect the data:\n\nislands <- read_csv(\"data/island.csv\")\n\nislands\n\n# A tibble: 35 × 2\n   species  area\n     <dbl> <dbl>\n 1     114  12.1\n 2     130  13.4\n 3     113  13.7\n 4     109  14.5\n 5     118  16.8\n 6     136  19.0\n 7     149  19.6\n 8     162  20.6\n 9     145  20.9\n10     148  21.0\n# … with 25 more rows\n\n\nLooking at the data, we can see that there are two columns: species, which contains the number of species recorded on each island and area, which contains the surface area of the island in square kilometers.\nWe can plot the data:\n\nislands %>% \n  ggplot(aes(x = area, y = species)) +\n  geom_point()\n\n\n\n\n\n\n\nIt looks as though area may have an effect on the number of species that we observe on each island. We note that the response variable is count data and so we try to construct a Poisson regression."
  },
  {
    "objectID": "glm-practical-poisson.html#model-building",
    "href": "glm-practical-poisson.html#model-building",
    "title": "Poisson regression",
    "section": "Model building",
    "text": "Model building\nTo create a poisson regression we do the following:\n\ntidyverse\n\n\nAgain, similar to what we’ve done for the logistic models, we will use the parsnip package from the tidymodels library. Yes, the workflow still seems a bit faffy, but it provides a common syntax for a whole range of modelling libraries. This means that the syntax will stay the same as you do different kind of model comparisons.\nIf you haven’t loaded tidymodels yet, now is a really good time. We also need to load poissonreg, which adds extra functionality to parsnip.\n\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\n# install.packages(\"poissonreg\")\nlibrary(poissonreg)\n\nRemember that the workflow in parsnip is a bit different to what we’re used to so far. Using parsnip we approach things in a more systematic manner. We specify a model in three steps:\n\nSpecify the type of model based on its mathematical structure (e.g., linear regression, random forest, K-nearest neighbors, etc).\nWhen required, declare the mode of the model. The mode reflects the type of prediction outcome. For numeric outcomes, the mode is regression; for qualitative outcomes, it is classification. If a model can only create one type of model, such as logistic regression, the mode is already set.\nSpecify the engine for fitting the model. This usually is the software package or library that should be used.\n\nSo, we can create the model as follows:\n\nisl_mod <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\n\nAgain, note that we are not actually specifying any of the variables yet. All we’ve done is tell R what kind of model we’re planning to use. If we want to see how parsnip converts this code to the package syntax, we can check this with translate():\n\nisl_mod %>% translate()\n\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\nModel fit template:\nstats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    family = stats::poisson)\n\n\nThis shows that we have a poisson regression model, where the outcome is going to be a regression. The model fit template tells us that we’ll be using the glm() function from the stats package, which can take a formula, data, weights and family argument. The family argument is already set to poisson.\nNow we’ve specified what kind of model we’re planning to use, we can fit our data to it, using the fit() function:\n\nisl_fit <- isl_mod %>% \n  fit(species ~ area,\n      data = islands)\n\nWe can look at the output directly, but I prefer to tidy the data up using the tidy() function from broom package:\n\nisl_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   4.24     0.0413      103.  0        \n2 area          0.0356   0.00125      28.6 2.73e-179\n\n\nThe output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the two numbers in the estimate column.\n\n\n\nThese are the coefficients of the Poisson model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of island size:\n\\[\\begin{equation}\nE(species) = {e}^{(4.24 + 0.036 \\times area)}\n\\end{equation}\\]\nInterpreting this requires a bit of thought (not much, but a bit).\nThe intercept coefficient (\\(\\beta_0\\)), 4.24, is related to the number of species we would expect on an island of zero area (this is statistics, not real life. You’d do well to remember that before you worry too much about what that even means). But in order to turn this number into something meaningful we have to exponentiate it. Since \\({e}^{(4.24)} \\approx 70\\), we can say that the baseline number of species the model expects on any island is 70. This isn’t actually the interesting bit though.\nThe coefficient of area (\\(\\beta_1\\)) is the fun bit. For starters we can see that it is a positive number which does mean that increasing area leads to increasing numbers of species. Good so far - since this matches what we saw when we plotted our data. But what does the value 0.036 actually mean?\nWell, if we exponentiate it too we get \\({e}^{(0.036)} \\approx 1.04\\). This means that for every increase in area of 1 km2 (the original units of the area variable) the number of species on the island is multiplied by 1.04. So, an island of area 1 km2 is predicted to have \\(1.04 \\times 70 \\approx 72\\) species.\nSo, in order to interpret Poisson coefficients, you have to exponentiate them."
  },
  {
    "objectID": "glm-practical-poisson.html#model-predictions",
    "href": "glm-practical-poisson.html#model-predictions",
    "title": "Poisson regression",
    "section": "Model predictions",
    "text": "Model predictions\nNow that we can interpret the Poisson coefficients, it would be good to see if using a poisson regression to describe these data is actually a good idea.\nVisualisation is always useful, so in order to get an idea of how our data fits a Poisson regression, we’ll plot the Poisson regression curve. Next, we overlay our original data.\n\ntidyverse\n\n\nFirst, we create a table that contains data for the curve, starting for an area with value 1 to 50, in steps of 1.\n\nmodel <- tibble(area = seq(1, 50, 1))\n\nNext, we feed our model these data:\n\ncurve <- isl_fit %>% augment(new_data = model)\n\nThis gives the predicted number of species for each given value of area. If we have a closer look at these data we can see that, for example, for an area with a surface area of 4 km2 the predicted number of species is around 80. Nice.\n\nhead(curve)\n\n# A tibble: 6 × 2\n   area .pred\n  <dbl> <dbl>\n1     1  72.0\n2     2  74.6\n3     3  77.3\n4     4  80.1\n5     5  83.0\n6     6  86.0\n\n\nUsing these data, we can now plot all the predicted number of species and overlay our original measured data.\n\nggplot(curve, aes(area, .pred)) +\n  geom_line(colour = \"red\") +\n  geom_point(data = islands, aes(area, species))\n\n\n\n\n\n\n\nThat looks like a pretty decent fit, really. But of course we want to have a (slightly) less hand-wavy conclusion than that."
  },
  {
    "objectID": "glm-practical-poisson.html#goodness-of-fit",
    "href": "glm-practical-poisson.html#goodness-of-fit",
    "title": "Poisson regression",
    "section": "Goodness-of-fit",
    "text": "Goodness-of-fit\nWe can use the model’s residual deviance to assess how much the predicted values differ from the observed. This gives us an idea of how well-specified the model is. When a model is “true”, i.e. the model makes pretty accurate predictions, then we expect the residual deviance to be distributed as a \\(\\chi^2\\) random variable with degrees of freedom equal to the model’s residual degrees of freedom.\n\ntidyverse\n\n\nWe can get these parameters as follows and we’ll store them in a new object, so we can extract them in a bit.\n\nisl_fit %>% glance()\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          857.      34  -139.  283.  286.     30.4          33    35\n\nisl_parameters <- isl_fit %>% glance()\n\nThe values we are interested in are in the deviance and df.residual columns, respectively.\nNext, we use the pchisq() function to calculate the correct probability.\n\npchisq(isl_parameters$deviance,\n       isl_parameters$df.residual,\n       lower.tail = FALSE)\n\n[1] 0.595347\n\n\nThis gives us a value of around 0.60. This suggests that this model is actually a pretty good one (if it wasn’t then the value would be close to zero) and that the data are pretty well supported by the model.\n\n\n\n\n\n\nImportant\n\n\n\nThe pchisq() function gives the lower tail probability that \\(\\chi^2 \\le x\\) by default. We’re actually interested in the probability that \\(\\chi^2 \\ge x\\). These two probabilities must sum to one, so we get the upper tail probability by setting the argument lower.tail = FALSE. An alternative way would be to use the default, but do 1 - pchisq().\n\n\nFor Poisson models this has an extra interpretation. This can be used to assess whether we have significant overdispersion in our data. For a Poisson model to be appropriate we need that the variance of the data to be exactly the same as the mean of the data. If there is overdispersion then the data would spread out more for higher predicted values of species than for lower ones. Our visualisation shows that this isn’t really happening. The spread is unlikely to be perfectly homogeneous, but we don’t want the data to spread out too much.\nThe easy way to check this is to look at the ratio of the residual deviance to the residual degrees of freedom (in this case 0.922). For a Poisson model to be valid, this ratio should be about 1. If the ratio is significantly bigger than 1 then we say that we have over-dispersion in the model and we wouldn’t be able to trust any of the significance testing using a Poisson regression."
  },
  {
    "objectID": "glm-practical-poisson.html#confidence-intervals",
    "href": "glm-practical-poisson.html#confidence-intervals",
    "title": "Poisson regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can also assess how reliable our model is by looking at the confidence intervals of the estimated parameters.\n\ntidyverse\n\n\nWe extracted the parameters of the model by using\n\nisl_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   4.24     0.0413      103.  0        \n2 area          0.0356   0.00125      28.6 2.73e-179\n\n\nAlthough we focussed on the estimate column, we can see that the associated standard errors for each estimate is also given in the std.error column. We can use these values to calculate the 95% confidence intervals.\nWe can either do this by hand through multiplying the standard errors by 1.96. We can then subtract from (giving the lower confidence estimate) or add to (giving the higher confidence estimate) the estimated parameter. This gives a pretty decent approximation.\nBut then again, life is short, so we can just use the additional argument that is available for the tidy() function. You can look at what columns are returned, but I’m selecting the relevant ones here:\n\nisl_fit %>% tidy(conf.int = TRUE,        # default is FALSE\n                 conf.level = 0.95) %>%  # is the default\n  select(term, estimate, conf.low, conf.high)\n\n# A tibble: 2 × 4\n  term        estimate conf.low conf.high\n  <chr>          <dbl>    <dbl>     <dbl>\n1 (Intercept)   4.24     4.16      4.32  \n2 area          0.0356   0.0332    0.0381\n\n\nWhat we’re interested in here is the confidence intervals for the area parameter. Before we delve into that, I’m also going to calculate the exponent for these confidence intervals. We can do this using the exp() function.\n\nisl_fit %>% tidy(conf.int = TRUE,        # default is FALSE\n                 conf.level = 0.95) %>%  # is the default\n  select(term, estimate, conf.low, conf.high) %>% \n  mutate(conf.low_exp = exp(conf.low),\n         conf.high_exp = exp(conf.high))\n\n# A tibble: 2 × 6\n  term        estimate conf.low conf.high conf.low_exp conf.high_exp\n  <chr>          <dbl>    <dbl>     <dbl>        <dbl>         <dbl>\n1 (Intercept)   4.24     4.16      4.32          64.1          75.3 \n2 area          0.0356   0.0332    0.0381         1.03          1.04\n\n\nThese values are a bit familiar, since we’ve previously determined based on the area coefficient that for each increase in square kilometer, the number of species increases by approximately 1.04.\nWhat these values tell us is that we can be 95% certain that for every increase in square kilometer of island area size, the number of species increases somewhere between 1.034 and 1.039.\n\n\n\n\n\n\nNote\n\n\n\nIf there was no association between area and species, then the \\(\\beta_1\\) coefficient would be zero. That would mean that the exponent would be \\({e}^{\\beta_1}=1\\). The interval that we calculated for \\({e}^{\\beta_1}\\) lies between 1.034 and 1.039 and therefore does not include 1, so the model with area is preferred over the model without area.\nSimilarly, the interval for \\(\\beta_1\\) (0.033 - 0.038) does not include 0, again showing the significance of area as a predictor of species."
  },
  {
    "objectID": "glm-practical-poisson.html#exercise-seatbelts",
    "href": "glm-practical-poisson.html#exercise-seatbelts",
    "title": "Poisson regression",
    "section": "Exercise: Seatbelts",
    "text": "Exercise: Seatbelts\nI’d like you to do the following:\n\nLoad the data\nVisualise the data and create a poisson regression model\nPlot the regression model on top of the data\nAssess if the model is a decent predictor for the number of fatalities\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLoad the data\nFirst, we load the data.\n\ntidyverse\n\n\n\nseatbelts <- read_csv(\"data/seatbelts.csv\")\n\nRows: 192 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): month\ndbl (9): drivers_killed, drivers, front, rear, kms, petrol_price, van_killed...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\nVisualise the data\nThe data tracks the number of drivers killed in road traffic accidents, before and after the seatbelt law was introduced. The information on whether the law was in place is encoded in the law column as 0 (law not in place) or 1 (law in place).\nThere are many more observations when the law was not in place, so we need to keep this in mind when we’re interpreting the data.\nFirst we have a look at the data comparing no law vs law:\n\ntidyverse\n\n\nWe have to convert the law column to a factor, otherwise R will see it as numerical.\n\nseatbelts %>% \n  ggplot(aes(as_factor(law), drivers_killed)) +\n  geom_boxplot()\n\n\n\n\nThe data are recorded by month and year, so we can also display the number of drivers killed by year:\n\nseatbelts %>% \n  ggplot(aes(year, drivers_killed)) +\n  geom_point()\n\n\n\n\n\n\n\nThe data look a bit weird. There is quite some variation within years (keeping in mind that the data are aggregated monthly). The data also seems to wave around a bit… with some vague peaks (e.g. 1972 - 1973) and some troughs (e.g. around 1976).\nSo my initial thought is that these data are going to be a bit tricky to interpret. But that’s OK.\n\n\nModel building\nWe’re dealing with count data, so we’re going to use a poisson regression.\n\ntidyverse\n\n\nAs before, we first define the model type.\n\nstb_mod <- poisson_reg() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glm\")\n\nAnd check that everything is in order.\n\nstb_mod %>% translate()\n\nPoisson Regression Model Specification (regression)\n\nComputational engine: glm \n\nModel fit template:\nstats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n    family = stats::poisson)\n\n\nNext, we fit our data to the model we just specified:\n\nstb_fit <- stb_mod %>% \n  fit(drivers_killed ~ year,\n      data = seatbelts)\n\nAnd we can extract the estimated coefficients from these fitted data:\n\nstb_fit %>% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  37.2      2.80         13.3 2.62e-40\n2 year         -0.0164   0.00142     -11.6 5.88e-31\n\n\n\n\n\n\n\nVisualise the model\nTo see if the model is actually a decent prediction for our data, we can plot it.\n\ntidyverse\n\n\nTo do this, we create modelled values for our predictor variable year. Because we have monthly data, we create a sequence of “years” in 1/12th intervals - one for each month.\nNext, we ask the model to predict the number of drivers killed based on these values.\nLastly, we can plot those predicted values against the observed values in our data set.\n\n# create the sequence of values that are used\n# in predicting number of deaths\nmodel <- tibble(year = seq(1969, 1984, (1/12)))\n\n# fit these data to the model\nstb_curve <- stb_fit %>% augment(new_data = model)\n\n# plot the predicted values\n# and overlay the observed values\nggplot(seatbelts, aes(year, drivers_killed)) +\n  geom_point() +\n  geom_point(data = stb_curve, aes(x = year, y = .pred),\n             colour = \"red\")\n\n\n\n\n\n\n\nThat does not look like a very good fit, but then again the data look rather messy as well.\n\n\nGoodness-of-fit\nLet’s check the goodness-of-fit.\n\ntidyverse\n\n\nFirst we store the parameter estimates in an object. Then we use the pchisq() function to calculate the probability that the residual deviance is actually distributed as a \\(\\chi^2\\) random variable with degrees of freedom equal to the model’s residual degrees of freedom.\nYes, you can read that sentence three times and still wonder what that really means. Suffice to say is that the outcome gives us a measure of how well-specified the model is.\n\nstb_parameter <- stb_fit %>% glance()\n\nstb_parameter\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          984.     191 -1062. 2127. 2134.     850.         190   192\n\npchisq(stb_parameter$deviance,\n       stb_parameter$df.residual,\n       lower.tail = FALSE)\n\n[1] 3.12987e-84\n\n\n\n\n\nWell, that’s a bit of a blow. The probability value is extremely low, suggesting that the model is not very well-specified. Which kind of matches what we already saw in the plot. It might still be better than the null model (“the data can be modelled as the average across all the observations”), but we seem to be missing some parameters here.\n\n\nConfidence intervals\nSimilar to the islands example, we can also calculate the confidence intervals associated with our estimated parameters.\n\ntidyverse\n\n\n\nstb_fit %>% tidy(conf.int = TRUE,        # default is FALSE\n                 conf.level = 0.95) %>%  # is the default\n  select(term, estimate, conf.low, conf.high) %>% \n  mutate(conf.low_exp = exp(conf.low),\n         conf.high_exp = exp(conf.high))\n\n# A tibble: 2 × 6\n  term        estimate conf.low conf.high conf.low_exp conf.high_exp\n  <chr>          <dbl>    <dbl>     <dbl>        <dbl>         <dbl>\n1 (Intercept)  37.2     31.7      42.7        5.78e+13      3.34e+18\n2 year         -0.0164  -0.0191   -0.0136     9.81e- 1      9.86e- 1\n\n\nWe’re interested in the confidence interval for the year variable. Remember that if there is no association at all between year and drivers_killed then the parameter \\(e^{\\beta_1} = 1\\).\nIn our case the interval we calculated for \\(e^{\\beta_1}\\) lies between 0.981 and 0.986. This does not include 1, so it seems that the model that takes year into account is still preferred over a model that doesn’t."
  },
  {
    "objectID": "glm-practical-poisson.html#key-points",
    "href": "glm-practical-poisson.html#key-points",
    "title": "Poisson regression",
    "section": "Key points",
    "text": "Key points\n\n\n\n\n\n\nNote\n\n\n\n\nPoisson regression is useful when dealing with count data"
  }
]