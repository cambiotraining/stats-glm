[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course overview",
    "section": "",
    "text": "Overview\nWelcome to the wonderful world of generalised linear models! These sessions teach you how to analyse non-continuous responses, such as binary (yes/no) and count data. Our primary focus is not on mathematical derivations, but on developing an intuitive understanding of the underlying statistical concepts and applying these to data. We use programming languages to help us with this and hope that, by the end of this course, you’ll be confident to apply these concepts to your own research data.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Course overview",
    "section": "",
    "text": "Learning Objectives\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nConstruct\n\na logistic model for binary response variables\na logistic model for proportion response variables\na Poisson model for count response variables\na Negative binomial model for count response variables\n\nPlot the data and the fitted curve in each case for both continuous and categorical predictors\nAssess the significance of fit\nAssess the goodness-of-fit\nAssess assumption of the model\n\n\n\n\nTarget Audience\nThese materials are aimed at people who have to analyse (research) data and encounter non-continuous responses.\n\n\nPrerequisites\nA solid understanding of statistics, ideally through attending our Core statistics course. Additionally, a good working ability for coding is required - either in R or Python. See our Data analysis courses as an example.\nAll current scheduled events can be found here.\n\n\n\nExercises\nExercises in these materials are labelled according to their level of difficulty:\n\n\n\n\n\n\n\nLevel\nDescription\n\n\n\n\n  \nExercises in level 1 are simpler and designed to get you familiar with the concepts and syntax covered in the course.\n\n\n  \nExercises in level 2 combine different concepts together and apply it to a given task.\n\n\n  \nExercises in level 3 require going beyond the concepts and syntax introduced to solve new problems.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Course overview",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\n\nYou can cite these materials as:\n\nHodgson, V., Castle, M., Nicholls, R., van Rongen, M. (2025). Generalised linear models. https://cambiotraining.github.io/stats-glm\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {Hodgson, Vicki and Castle, Matt and Nicholls, Rob and van Rongen, Martin},\n  month = {7},\n  title = {Generalised linear models},\n  url = {https://cambiotraining.github.io/stats-glm},\n  year = {2025}\n}\nAbout the authors:\nVicki Hodgson  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - review & editing; conceptualisation; software\n\nMatt Castle  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: conceptualisation; writing\n\nRob Nicholls  \nAffiliation: Science and Technology Facilities Council, Rutherford Appleton Laboratory, Didcot Roles: conceptualisation\n\nMartin van Rongen  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - review & editing; conceptualisation; software",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Course overview",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\nWe thank Hugo Tavares for constructive feedback on the manuscript.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Working with R\nIf you’re using R for these sessions, please follow the instructions for installing R and RStudio.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#working-with-python",
    "href": "setup.html#working-with-python",
    "title": "Setup",
    "section": "Working with Python",
    "text": "Working with Python\nIf you’re using Python for these sessions, please follow the instructions for installing Python and Visual Studio Code.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "3  Data",
    "section": "",
    "text": "The data we use throughout all the sessions are contained in a single ZIP file. They are all small CSV files (comma separated values). You can download the data below:\n Download\n\n\n\n\n\n\nWarning\n\n\n\nThe data we use throughout the course is varied, covering many different topics. In some cases the data on medical topics or historical events may feel uncomfortable to some, since they can touch on diseases or death.\nAll the data are chosen for their pedagogical effectiveness.\n\n\nSome of the data have been synthesised using simulations. This allows us to highlight specific challenges you can encounter in real research data, without overcomplicating the analysis. For example, in real research data you will often have to deal with missing data, mislabelled data etc. Here, it would detract from what we are trying to achieve.\nThe code used to generate the synthesised data can be found here (Quarto markdown file).",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "4  References",
    "section": "",
    "text": "Glen, Stephanie. 2021. “Link Function.” Statistics How\nTo: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant,\nPeter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow\nBetween Two Species of Darwin’s Finches.” Nature Ecology\n& Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "materials/glm-05-intro-lm.html",
    "href": "materials/glm-05-intro-lm.html",
    "title": "\n5  Linear models\n",
    "section": "",
    "text": "5.1 Data\nFor this example, we’ll be using the several data sets about Darwin’s finches. They are part of a long-term genetic and phenotypic study on the evolution of several species of finches. The exact details are less important for now, but there are data on multiple species where several phenotypic characteristics were measured (see Figure 5.1).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-05-intro-lm.html#data",
    "href": "materials/glm-05-intro-lm.html#data",
    "title": "\n5  Linear models\n",
    "section": "",
    "text": "Figure 5.1: Finches phenotypes (courtesy of HHMI BioInteractive)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-05-intro-lm.html#exploring-data",
    "href": "materials/glm-05-intro-lm.html#exploring-data",
    "title": "\n5  Linear models\n",
    "section": "\n5.2 Exploring data",
    "text": "5.2 Exploring data\nIt’s always a good idea to explore your data visually. Here we are focussing on the (potential) relationship between beak length (blength) and beak depth (bdepth).\nOur data contains measurements from two years (year) and two species (species). If we plot beak depth against beak length, colour our data by species and look across the two time points (1975 and 2012), we get the following graph:\n\n\n\n\n\n\n\nFigure 5.2: Beak depth and length for G. fortis and G. scandens\n\n\n\n\nIt seems that there is a potential linear relationship between beak depth and beak length. There are some differences between the two species and two time points with, what seems, more spread in the data in 2012. The data for both species also seem to be less separated than in 1975.\nFor the current purpose, we’ll focus on one group of data: those of Geospiza fortis in 1975.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-05-intro-lm.html#linear-model",
    "href": "materials/glm-05-intro-lm.html#linear-model",
    "title": "\n5  Linear models\n",
    "section": "\n5.3 Linear model",
    "text": "5.3 Linear model\nLet’s look at the G. fortis data more closely, assuming that the have a linear relationship. We can visualise that as follows:\n\n\n\n\n\n\n\nFigure 5.3: Beak depth vs beak length G. fortis (1975)\n\n\n\n\nIf you recall from the Core statistics linear regression session, what we’re doing here is assuming that there is a linear relationship between the response variable (in this case bdepth) and predictor variable (here, blength).\nWe can get more information on this linear relationship by defining a linear model, which has the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhere \\(Y\\) is the response variable (the thing we’re interested in), \\(X\\) the predictor variable and \\(\\beta_0\\) and \\(\\beta_1\\) are model beta coefficients. More explicitly for our data, we get:\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nBut how do we find this model? The computer uses a method called least-squares regression. There are several steps involved in this.\n\n5.3.1 Line of best fit\nThe computer tries to find the line of best fit. This is a linear line that best describes your data. We could draw a linear line through our cloud of data points in many ways, but the least-squares method converges to a single solution, where the sum of squared residual deviations is at its smallest.\nTo understand this a bit better, it’s helpful to realise that each data point consists of a fitted value (the beak depth predicted by the model at a given beak length), combined with the error. The error is the difference between the fitted value and the data point.\nLet’s look at this for one of the observations, for example finch 473:\n\n\n\n\n\n\n\nFigure 5.4: Beak depth vs beak length (finch 473, 1975)\n\n\n\n\nObtaining the fitted value and error happens for each data point. All these residuals are then squared (to ensure that they are positive), and added together. This is the so-called sum-of-squares.\nYou can imagine that if you draw a line through the data that doesn’t fit the data all that well, the error associated with each data point increases. The sum-of-squares then also increases. Equally, the closer the data are to the line, the smaller the error. This results in a smaller sum-of-squares.\nThe linear line where the sum-of-squares is at its smallest, is called the line of best fit. This line acts as a model for our data.\nFor finch 473 we have the following values:\n\nthe observed beak depth is 9.5 mm\nthe observed beak length is 10.5 mm\nthe fitted value is 9.11 mm\nthe error is 0.39 mm\n\n5.3.2 Linear regression\nOnce we have the line of best fit, we can perform a linear regression. What we’re doing with the regression, is asking:\n\nIs the line of best fit a better predictor of our data than a horizontal line across the average value?\n\nVisually, that looks like this:\n\n\n\n\n\n\n\nFigure 5.5: Regression: is the slope different from zero?\n\n\n\n\nWhat we’re actually testing is whether the slope (\\(\\beta_1\\)) of the line of best fit is any different from zero.\nTo find the answer, we perform an ANOVA. This gives us a p-value of 1.68e-78.\nNeedless to say, this p-value is extremely small, and definitely smaller than any common significance threshold, such as \\(p &lt; 0.05\\). This suggests that beak length is a statistically significant predictor of beak depth.\nIn this case the model has an intercept (\\(\\beta_0\\)) of -0.34 and a slope (\\(\\beta_1\\)) of 0.9. We can use this to write a simple linear equation, describing our data. Remember that this takes the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhich in our case is\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nand gives us\n\\[\nbeak\\ depth = -0.34 + 0.90 \\times beak\\ length\n\\]\n\n5.3.3 Assumptions\nIn example above we just got on with things once we suspected that there was a linear relationship between beak depth and beak length. However, for the linear regression to be valid, several assumptions need to be met. If any of those assumptions are violated, we can’t trust the results. The following four assumptions need to be met, with a 5th point being a case of good scientific practice:\n\nData should be linear\nResiduals are normally distributed\nEquality of variance\nThe residuals are independent\n(no influential points)\n\nAs we did many times during the Core statistics sessions, we mainly rely on diagnostic plots to check these assumptions. For this particular model they look as follows:\n\n\n\n\n\n\n\nFigure 5.6: Diagnostic plots for G. fortis (1975) model\n\n\n\n\nThese plots look very good to me. For a recap on how to interpret these plots, see CS2: ANOVA.\nTaken together, we can see the relationship between beak depth and beak length as a linear one, described by a (linear) model that has a predicted value for each data point, and an associated error.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html",
    "href": "materials/glm-06-intro-glm.html",
    "title": "\n6  Generalise your model\n",
    "section": "",
    "text": "6.1 Putting the “G” into GLM\nIn the previous linear model example all the assumptions were met. But what if we have data where that isn’t the case? For example, what if we have data where we can’t describe the relationship between the predictor and response variables in a linear way?\nOne of the ways we can deal with this is by using a generalised linear model, also abbreviated as GLM. In a way it’s an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nHere, the \\(\\beta_0\\) value is the constant or intercept, whereas each subsequent \\(\\beta_i\\) is a unique regression coefficient for each \\(X_i\\) predictor variable. So far so good.\nHowever, the GLM makes the linear model more flexible in two ways:\nWe’ll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.\nThe link function and different distributions are closely…err, linked. To make sense of what the link function is doing it’s useful to understand the different distributional assumptions. So we’ll start with those.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html#putting-the-g-into-glm",
    "href": "materials/glm-06-intro-glm.html#putting-the-g-into-glm",
    "title": "\n6  Generalise your model\n",
    "section": "",
    "text": "Important\n\n\n\n\nIn a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a link function.\nA standard linear model assumes a continuous, normally distributed outcome, whereas with GLM the outcome can be both continuous or integer. Furthermore, the outcome does not have to be normally distributed. Indeed, the outcome can follow a different kind of distribution, such as binomial, Poisson, exponential etc.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html#distributions",
    "href": "materials/glm-06-intro-glm.html#distributions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.2 Distributions",
    "text": "6.2 Distributions\nIn the examples of a standard linear model we’ve seen that the residuals needed to be normally distributed. We’ve mainly used the Q-Q plot to assess this assumption of normality.\nBut what does “normal” actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.\nWe can see this in Figure 6.1. The mean of the normal distribution is indicated with the dashed blue line.\n\n\n\n\n\n\n\nFigure 6.1: Normal distribution\n\n\n\n\nWe can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of G. fortis beaks in 1975.\nThe individual values of the residuals from this linear model are shown in Figure 6.1, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see Figure 5.6).\nAll this means is that assuming that these residuals may come from a normal distribution isn’t such a daft suggestion after all.\nNow look at the example in Figure 6.2. This shows the classification of beak shape for a number of finches. Their beaks are either classed as blunt or pointed. Various (continuous) measurements were taken from each bird, with the beak length shown here.\n\n\n\n\n\n\n\nFigure 6.2: Classification in beak shape\n\n\n\n\nWe’ll look into this example in more detail later. For now it’s important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (blunt or pointed). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (Figure 6.2 B).\nSo what do we do? Well, the normal distribution is not the only one there is. In Figure 6.3 there are a few examples of distributions (including the normal one).\n\n\n\n\n\n\n\nFigure 6.3: Different distributions\n\n\n\n\nDifferent distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.\nIn order to understand how this can help us, we need to be aware of two more concepts: linear predictors and link functions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html#linear-predictors",
    "href": "materials/glm-06-intro-glm.html#linear-predictors",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.3 Linear predictors",
    "text": "6.3 Linear predictors\nThe nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.\nAs mentioned before, predictors are in the form of a linear combination, where each predictor variable is multiplied by a coefficient and all the terms are added together:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nFortunately, this is no different for generalised linear models! We still have a linear combination but, as we’ll see, if the relationship is not linear then we need an additional step before we can model the data in this way.\nAt this point, we have two options at our disposal (well, there are more, but let’s not muddy the waters too much).\n\n\n\n\n\n\nImportant\n\n\n\n\nTransform our data and use a normal linear model on the transformed data\nTransform the linear predictor\n\n\n\nThe first option, to transform our data, seems like a useful option and can work. It keeps things familiar (we’d still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables (Stevens 1946).\nThe second option would be to transform the linear predictor. This enables us to map a non-linear outcome (or response variable) to a linear model. This transformation is done using a link function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html#link-functions",
    "href": "materials/glm-06-intro-glm.html#link-functions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.4 Link functions",
    "text": "6.4 Link functions\nSimply put: link functions connect the predictors in our model to the response variables in a linear way.\nHowever, and similar to the standard linear model, there are two parts to each model:\n\nthe coefficients for each predictor (linking each parameter to a predictor)\nthe error or random component (which specifies a probability distribution)\n\nWhich link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from (Glen 2021)):\n\n\ndistribution\ndata type\nlink name\n\n\n\nbinomial\nbinary / proportion\nlogit\n\n\nnormal\nany real number\nidentity\n\n\npoisson\ncount data\nlog\n\n\n\nLet’s again look at the earlier example of beak shape.\n\n\n\n\n\n\n\nFigure 6.4: Beak shape classification\n\n\n\n\nWe’ve seen the data in Figure 6.4 A before, where we had information on what beak shape our observed finches had, plotted against their beak length.\nLet’s say we now want to make some predictions about what beak shape we would expect, given a certain beak length. In this scenario we’d need some way of modelling the response variable (beak shape; blunt or pointed) as a function of the predictor variable (beak length).\nThe issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in Figure 6.2 A) but this is really bad practice. Why? Well, what such a linear model represents is the probability - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (Figure 6.4 B).\nSimply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can’t describe these data as a linear relationship.\nInstead, we’ll use a logistic model to analyse these data. We’ll cover the practicalities of how to do this in more detail in a later chapter, but for now it’s sufficient to realise that one of the ways we could model these data could look like this:\n\n\n\n\n\n\n\nFigure 6.5: Logistic model for beak classification\n\n\n\n\nUsing this sigmoidal curve ensures that our predicted probabilities do not exceed the \\([0, 1]\\) range.\nNow, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the logit link function. This links the non-linear response variable (beak shape) to a linear model, using beak length as a predictor.\nWe’ll practice how to perform this analysis in the next section.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-06-intro-glm.html#key-points",
    "href": "materials/glm-06-intro-glm.html#key-points",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.5 Key points",
    "text": "6.5 Key points\n\n\n\n\n\n\nNote\n\n\n\n\nGLMs allow us to map a non-linear outcome to a linear model\nThe link function determines how this occurs, transforming the linear predictor\n\n\n\n\n\n\n\nGlen, Stephanie. 2021. “Link Function.” Statistics How To: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html",
    "href": "materials/glm-07-binary-response.html",
    "title": "\n7  Binary response\n",
    "section": "",
    "text": "7.1 Context\nBinary response data takes only one of two values. This can be 0 or 1, \"yes\" or \"no\" etc. Because these data are not continuous, we can’t analyse them with a standard linear model. Instead, we need to use something called a logistic regression. We illustrate how to do this with an example below.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#section-setup",
    "href": "materials/glm-07-binary-response.html#section-setup",
    "title": "\n7  Binary response\n",
    "section": "\n7.2 Section setup",
    "text": "7.2 Section setup\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\nWe’ll use the following libraries and data:\n\n\nR\nPython\n\n\n\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n# A package for creating panels of diagnostic plots for a model\nlibrary(ggResidpanel)\n\n# Data\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\naphids &lt;- read_csv(\"data/aphids.csv\")\n\n\n\n\n# A maths library\nimport math\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n# Custom function to create diagnostic plots\nfrom dgplots import *\n\n# Data\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\naphids = pd.read_csv(\"data/aphids.csv\")\n\nNote: you can download the dgplots script here. Ensure it’s at the same folder level of your scripts.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#finches-example",
    "href": "materials/glm-07-binary-response.html#finches-example",
    "title": "\n7  Binary response\n",
    "section": "\n7.3 Finches example",
    "text": "7.3 Finches example\nThe example in this section uses the finches_early data. These come from an analysis of gene flow across two finch species (Lamichhaney et al. 2020). They are slightly adapted here for illustrative purposes.\nThe data focus on two species, Geospiza fortis and G. scandens. The original measurements are split by a uniquely timed event: a particularly strong El Niño event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as early (pre-1983) and late (1983 onwards).\nHere we are looking only at the early data. We are specifically focussing on the beak shape classification, which we saw earlier in Figure 6.5.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#load-and-visualise-the-data",
    "href": "materials/glm-07-binary-response.html#load-and-visualise-the-data",
    "title": "\n7  Binary response\n",
    "section": "\n7.4 Load and visualise the data",
    "text": "7.4 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\n\n\n\n\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n\n\n\n\nLooking at the data, we can see that the pointed_beak column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure 7.1: Boxplot of beak shape against beak length\n\n\n\n\n\n\nWe could just give Python the pointed_beak data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a pointed beak (1), and those with a blunt one (0).\nWe can force Python to temporarily covert the data to a factor, by making the pointed_beak column an object type. We can do this directly inside the ggplot() function.\n\np = (ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\nFigure 7.2: Boxplot of beak shape against beak length\n\n\n\n\n\n\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n\n\n\n\n\n\nFigure 7.3: Scatterplot of beak shape against beak length\n\n\n\n\n\n\n\np = (ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 7.4: Scatterplot of beak shape against beak length\n\n\n\n\n\n\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea…\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\nFigure 7.5: Standard linear model of beak shape against beak length\n\n\n\n\n\n\n\np = (ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\np.show()\n\n\n\n\n\n\nFigure 7.6: Standard linear model of beak shape against beak length\n\n\n\n\n\n\n\nOf course this is rubbish - we can’t have a beak classification outside the range of \\([0, 1]\\). It’s either blunt (0) or pointed (1).\nBut for the sake of exploration, let’s look at the assumptions:\n\n\nR\nPython\n\n\n\n\nlm_bks &lt;- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\nFigure 7.7: Diagnostic plots for pointed_beak ~ length\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\nfrom dgplots import *\n\n\ndgplots(lm_bks_py)\n\n\n\n\n\n\nFigure 7.8: Diagnostic plots for pointed_beak ~ length\n\n\n\n\n\n\n\nThey’re pretty extremely bad.\n\nThe response is not linear (Residual Plot, binary response plot, common sense).\nThe residuals do not appear to be distributed normally (Q-Q Plot)\nThe variance is not homogeneous across the predicted values (Location-Scale Plot)\nBut - there is always a silver lining - we don’t have influential data points.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#creating-a-suitable-model",
    "href": "materials/glm-07-binary-response.html#creating-a-suitable-model",
    "title": "\n7  Binary response\n",
    "section": "\n7.5 Creating a suitable model",
    "text": "7.5 Creating a suitable model\nSo far we’ve established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what can we do?\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the \\([0, 1]\\) range.\nWe described our standard linear relationship as follows:\n\\(Y = \\beta_0 + \\beta_1X\\)\nWe can now map this to our non-linear relationship via the logistic link function:\n\\(Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}\\)\nNote that the \\(\\beta_0 + \\beta_1X\\) part is identical to the formula of a straight line.\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n\n\n\n\n\nEuler’s number (\\(\\exp\\)): would you like to know more?\n\n\n\n\n\nIn mathematics, \\(\\rm e\\) represents a constant of around 2.718. Another notation is \\(\\exp\\), which is often used when notations become a bit cumbersome. Here, I exclusively use the \\(\\exp\\) notation for consistency.\n\n\n\n\n\n\n\n\n\nThe logistic function\n\n\n\nThe shape of the logistic function is hugely influenced by the different parameters, in particular \\(\\beta_1\\). The plots below show different situations, where \\(\\beta_0 = 0\\) in all cases, but \\(\\beta_1\\) varies.\nThe first plot shows the logistic function in its simplest form, with the others showing the effect of varying \\(\\beta_1\\).\n\n\n\n\n\n\n\nFigure 7.9: The logistic function with varying \\(\\beta_1\\)\n\n\n\n\n\nwhen \\(\\beta_1 = 1\\), this gives the simplest logistic function\nwhen \\(\\beta_1 = 0\\) gives a horizontal line, with \\(Y = \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\)\n\nwhen \\(\\beta_1\\) is negative flips the curve around, so it slopes down\nwhen \\(\\beta_1\\) is very large then the curve becomes extremely steep\n\n\n\nWe can fit such an S-shaped curve to our early_finches data set, by creating a generalised linear model.\n\n\nR\nPython\n\n\n\nIn R we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks:\n\nglm_bks &lt;- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\nIf you forget to set the family argument, then the glm() function will perform a standard linear model fit, identical to what the lm() function would do.\n\n\nIn Python we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks_py:\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()\n\nThe format of this function is similar to that used by the ols() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial. This is buried deep inside the statsmodels package and needs to be defined as sm.families.Binomial().",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#model-output",
    "href": "materials/glm-07-binary-response.html#model-output",
    "title": "\n7  Binary response\n",
    "section": "\n7.6 Model output",
    "text": "7.6 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\n\nR\nPython\n\n\n\n\nsummary(glm_bks)\n\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nprint(glm_bks_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Wed, 23 Jul 2025   Deviance:                       9.1879\nTime:                        15:40:31   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n\n\n\n\n\nThere’s a lot to unpack here, but let’s start with what we’re familiar with: coefficients!",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#parameter-interpretation",
    "href": "materials/glm-07-binary-response.html#parameter-interpretation",
    "title": "\n7  Binary response\n",
    "section": "\n7.7 Parameter interpretation",
    "text": "7.7 Parameter interpretation\n\n\nR\nPython\n\n\n\nThe coefficients or parameters can be found in the Coefficients block. The main numbers to extract from the output are the two numbers underneath Estimate.Std:\nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the coef column:\n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n\n\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\nThe \\(p\\) values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the \\(p\\) values obtained in the summary output of a linear model. As with continuous predictors in simple models, these \\(p\\) values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these \\(p\\) values aren’t great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a \\(p\\) value for each level rather than for the predictor as a whole).\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\\[ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} \\]\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\n\nR\nPython\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n\n[1] 0.9994131\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n\n[1] 7.410155e-05\n\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n# import the math library\nimport math\n\n\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n\n0.9994130595039192\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n\n7.410155028945912e-05\n\n\n\n\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I’m visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n\n\n\n\n\nCode available here\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nglm_bks |&gt; \n  augment(type.predict = \"response\") |&gt; \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n\n\n\n\np = (ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\np.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.10: Predicted probabilities for beak classification\n\n\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It’s also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#influential-observations",
    "href": "materials/glm-07-binary-response.html#influential-observations",
    "title": "\n7  Binary response\n",
    "section": "\n7.8 Influential observations",
    "text": "7.8 Influential observations\nBy this point, if we were fitting a linear model, we would want to check whether the statistical assumptions have been met.\nThe same is true for a generalised linear model. However, as explained in the background chapter, we can’t really use the standard diagnostic plots to assess assumptions. (And the assumptions of a GLM are not the same as a linear model.)\nThere will be a whole chapter later on that focuses on assumptions and how to check them.\nFor now, there is one thing that we can do that might be familiar: look for influential points using the Cook’s distance plot.\n\n\nR\nPython\n\n\n\n\nresid_panel(glm_bks, plots = \"cookd\")\n\n\n\n\n\n\nFigure 7.11: Influential points for glm_bks\n\n\n\n\n\n\nAs always, there are different ways of doing this. Here we extract the Cook’s d values from the glm object and put them in a Pandas DataFrame. We can then use that to plot them in a lollipop or stem plot.\n\n# extract the Cook's distances\nglm_bks_py_resid = pd.DataFrame(glm_bks_py.\n                                get_influence().\n                                summary_frame()[\"cooks_d\"])\n\n# add row index \nglm_bks_py_resid['obs'] = glm_bks_py_resid.reset_index().index\n\nWe now have two columns:\n\nglm_bks_py_resid.head()\n\n        cooks_d  obs\n0  1.854360e-07    0\n1  3.388262e-07    1\n2  3.217960e-05    2\n3  1.194847e-05    3\n4  6.643975e-06    4\n\n\nWe can use these to create the plot:\n\np = (ggplot(glm_bks_py_resid,\n         aes(x = \"obs\",\n             y = \"cooks_d\")) +\n     geom_segment(aes(x = \"obs\", y = \"cooks_d\", xend = \"obs\", yend = 0)) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 7.12: Influential points for glm_bks_py\n\n\n\n\n\n\n\nThis shows that there are no very obvious influential points. You could regard point 34 as potentially influential (it’s got a Cook’s distance of around 0.8), but I’m not overly worried.\nIf we were worried, we’d remove the troublesome data point, re-run the analysis and see if that changes the statistical outcome. If so, then our entire (statistical) conclusion hinges on one data point, which is not a very robust bit of research. If it doesn’t change our significance, then all is well, even though that data point is influential.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#exercises",
    "href": "materials/glm-07-binary-response.html#exercises",
    "title": "\n7  Binary response\n",
    "section": "\n7.9 Exercises",
    "text": "7.9 Exercises\n\n7.9.1 Diabetes\n\n\n\n\n\n\nExercise 1 - Diabetes\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/diabetes.csv.\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called test_result, glucose and diastolic.\nWe want to look at the relationship between glucose tolerance and diabetes test results. To investigate this, do the following:\n\nLoad and visualise the data\nCreate a suitable model\nCalculate the probability of a positive diabetes test result for a glucose tolerance test value of glucose = 150\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\np = (ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\np.show()\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\np = (ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\n\nR\nPython\n\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Wed, 23 Jul 2025   Deviance:                       752.20\nTime:                        15:40:33   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nRight now, we’re interested in the coefficients (we’ll look at significance more in subsequent chapters).\nWe have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\n\nR\nPython\n\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 150))\n\n[1] 0.5962827\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 150))\n\n0.5962826992967878\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 60 %.\n\n\n\n\n\n\n\n\n\n\n7.9.2 Aphids\n\n\n\n\n\n\nExercise 2 - Aphids\n\n\n\n\n\n\nLevel: \nIn this exercise we’ll use the data/aphids.csv dataset.\nEach row of these data represents a unique rose plant. For each plant, the researcher recorded:\n\nThe number of unbloomed buds\n\nWhich type of cultivated variety (cultivar) the rose was (mozart or harmonie)\nWhether or not aphids were present\n\nYou should:\n\nLoad and visualise the data\nFit an appropriate model\nCalculate the probability of aphids being present on a harmonie rose with 5 unbloomed buds\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLoad and visualise\n\n\nR\nPython\n\n\n\n\naphids &lt;- read_csv(\"data/aphids.csv\")\n\n\n\n\naphids = pd.read_csv(\"data/aphids.csv\")\n\n\n\n\nThere are multiple ways we could visualise these data, but let’s try a scatterplot here:\n\n\nR\nPython\n\n\n\n\nggplot(aphids,\n       aes(x = factor(aphids_present),\n           y = factor(buds),\n           colour = cultivar)) +\n  geom_jitter(width = 0.3)\n\n\n\n\n\n\n\n\n\n\np = (ggplot(aphids, aes(x = aphids.aphids_present.astype(object),\n                    y = aphids.buds.astype(object),\n                    colour = \"cultivar\")) +\n   geom_jitter(width = 0.3))\n\np.show()\n\n\n\n\n\n\n\n\n\n\nThe plot gives the impression that the mozart roses have aphids present more often than the harmonie roses, but it’s hard to tell whether there’s an effect of buds from this graph.\nFit a logistic model\nTo quantify the relationship(s), let’s fit a logistic model.\n\n\nR\nPython\n\n\n\n\nglm_aphids &lt;- glm(aphids_present ~ buds + cultivar,\n                  family = binomial,\n                  data = aphids)\n\nsummary(glm_aphids)\n\n\nCall:\nglm(formula = aphids_present ~ buds + cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)     -2.0687     0.7483  -2.765  0.00570 **\nbuds             0.2067     0.1262   1.638  0.10149   \ncultivarmozart   1.9621     0.6439   3.047  0.00231 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.666  on 51  degrees of freedom\nAIC: 66.666\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n# create the model\nmodel = smf.glm(formula = \"aphids_present ~ buds + cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \n# extract fitted parameters\nglm_aphids = model.fit()\n\n# print summary of model\nprint(glm_aphids.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:         aphids_present   No. Observations:                   54\nModel:                            GLM   Df Residuals:                       51\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -30.333\nDate:                Wed, 23 Jul 2025   Deviance:                       60.666\nTime:                        15:40:33   Pearson chi2:                     54.8\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2140\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept             -2.0687      0.748     -2.765      0.006      -3.535      -0.602\ncultivar[T.mozart]     1.9621      0.644      3.047      0.002       0.700       3.224\nbuds                   0.2067      0.126      1.638      0.101      -0.041       0.454\n======================================================================================\n\n\n\n\n\nIf you were paying close attention when you were learning about regular linear modelling, you might be thinking - isn’t it possible that there’s an interaction between our two predictors?\nYes, it is!\nWe can adapt our logistic model to include a third predictor, the buds:cultivar interaction. This works exactly like it would in a regular linear model - the syntax should be familiar already.\n\n\nR\nPython\n\n\n\n\nglm_aphids2 &lt;- glm(aphids_present ~ buds * cultivar,\n                  family = binomial,\n                  data = aphids)\n\nsummary(glm_aphids2)\n\n\nCall:\nglm(formula = aphids_present ~ buds * cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)         -1.86699    0.93702  -1.992   0.0463 *\nbuds                 0.16533    0.17365   0.952   0.3411  \ncultivarmozart       1.58251    1.27226   1.244   0.2136  \nbuds:cultivarmozart  0.08758    0.25647   0.342   0.7327  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.548  on 50  degrees of freedom\nAIC: 68.548\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n# create the model\nmodel = smf.glm(formula = \"aphids_present ~ buds * cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \n# extract fitted parameters\nglm_aphids2 = model.fit()\n\n# print summary of model\nprint(glm_aphids2.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:         aphids_present   No. Observations:                   54\nModel:                            GLM   Df Residuals:                       50\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -30.274\nDate:                Wed, 23 Jul 2025   Deviance:                       60.548\nTime:                        15:40:33   Pearson chi2:                     54.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2157\nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept                  -1.8670      0.937     -1.992      0.046      -3.704      -0.030\ncultivar[T.mozart]          1.5825      1.272      1.244      0.214      -0.911       4.076\nbuds                        0.1653      0.174      0.952      0.341      -0.175       0.506\nbuds:cultivar[T.mozart]     0.0876      0.256      0.342      0.733      -0.415       0.590\n===========================================================================================\n\n\n\n\n\nThe question of which of these models is better is something we’ll tackle in one of the later chapters.\nCalculate the probability\nFinally, let’s use the coefficients from our model to calculate the probability of aphids being present under specific values of our predictors, specifically:\n\n\nbuds = 8\n\ncultivar = harmonie\n\n\nTo keep things simple, we’ll use the coefficients from our additive model, but you can easily adapt this code to include the interaction with little effort.\nSince we’ve got two predictors, we’re going to be efficient and do this in two stages. First, we’ll build the “linear predictor” (the linear bit of our equation), and then we’ll embed it inside the inverse link function.\n\n\nR\nPython\n\n\n\n\nlin_pred &lt;- -2.069 + 0.267 * 8 + 1.962 * 0\n\n# these will give identical results\nexp(lin_pred) / (1 + exp(lin_pred))\n\n[1] 0.5167437\n\n1 / (1 + exp(-lin_pred))\n\n[1] 0.5167437\n\n\n\n\n\nlin_pred = -2.069 + 0.267 * 8 + 1.962 * 0\n\n# these will give identical results\nmath.exp(lin_pred) / (1 + math.exp(lin_pred))\n\n0.5167437369156502\n\n1 / (1 + math.exp(-lin_pred))\n\n0.5167437369156502\n\n\n\n\n\nWe write a 0 for the cultivar variable, because harmonie is the reference group (and therefore no adjustment is needed). If we had been making a prediction for the mozart rose, we would write a 1 here instead.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-07-binary-response.html#summary",
    "href": "materials/glm-07-binary-response.html#summary",
    "title": "\n7  Binary response\n",
    "section": "\n7.10 Summary",
    "text": "7.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a logistic regression to model a binary response\nThis uses a logit link function\nWe can feed new observations into the model and make predictions about the expected likelihood of “success”, given certain values of the predictor variable(s)\n\n\n\n\n\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant, Peter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow Between Two Species of Darwin’s Finches.” Nature Ecology & Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html",
    "href": "materials/glm-08-proportional-response.html",
    "title": "\n8  Proportional response\n",
    "section": "",
    "text": "8.1 Context\nIn the previous section we’ve seen how we can use logistic regression to analyse binary outcomes. We can use the same approach when we have proportional outcomes. These are variables where we tally the number of times an event happens, out of a fixed number of trials or cases. Below, we illustrate this with the Challenger data.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#libraries-and-functions",
    "href": "materials/glm-08-proportional-response.html#libraries-and-functions",
    "title": "\n8  Proportional response\n",
    "section": "\n8.2 Libraries and functions",
    "text": "8.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n8.2.1 Libraries\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(ggResidpanel)\n\n\n8.2.2 Functions\n\n# create diagnostic plots\nggResidpanel::resid_panel()\n\n\n\n\n\n8.2.3 Libraries\n\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#the-challenger-dataset",
    "href": "materials/glm-08-proportional-response.html#the-challenger-dataset",
    "title": "\n8  Proportional response\n",
    "section": "\n8.3 The Challenger dataset",
    "text": "8.3 The Challenger dataset\nThe example in this section uses the challenger data. These data, obtained from the faraway package in R, contain information related to the explosion of the space shuttle Challenger on 28 January, 1986. An investigation traced the probable cause back to certain joints on one of the two solid booster rockets, each containing six O-rings that ensured no exhaust gases could escape from the booster.\nThe night before the launch was unusually cold, with temperatures below freezing. The final report suggested that the cold snap during the night made the o-rings stiff and unable to adjust to changes in pressure - leading to a proportion of them failing during launch. As a result, exhaust gases leaked away from the solid booster rockets, causing one of them to break loose and rupture the main fuel tank, leading to the final explosion.\nThe question we’re trying to answer in this session is: based on the data from the previous flights, would it have been possible to predict the failure of most o-rings on the Challenger flight?",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#load-and-visualise-the-data",
    "href": "materials/glm-08-proportional-response.html#load-and-visualise-the-data",
    "title": "\n8  Proportional response\n",
    "section": "\n8.4 Load and visualise the data",
    "text": "8.4 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nchallenger &lt;- read_csv(\"data/challenger.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nchallenger_py = pd.read_csv(\"data/challenger.csv\")\n\n\n\n\nThe data set contains several columns:\n\n\ntemp, the launch temperature in degrees Fahrenheit\n\ndamage, the number of o-rings that showed erosion\n\nBefore we have a further look at the data, let’s calculate the proportion of damaged o-rings (prop_damaged) and the total number of o-rings (total) and update our data set.\n\n\nR\nPython\n\n\n\n\nchallenger &lt;-\nchallenger |&gt;\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nchallenger\n\n# A tibble: 23 × 5\n    temp damage total intact prop_damaged\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1    53      5     6      1        0.833\n 2    57      1     6      5        0.167\n 3    58      1     6      5        0.167\n 4    63      1     6      5        0.167\n 5    66      0     6      6        0    \n 6    67      0     6      6        0    \n 7    67      0     6      6        0    \n 8    67      0     6      6        0    \n 9    68      0     6      6        0    \n10    69      0     6      6        0    \n# ℹ 13 more rows\n\n\n\n\n\nchallenger_py['total'] = 6\nchallenger_py['intact'] = challenger_py['total'] - challenger_py['damage']\nchallenger_py['prop_damaged'] = challenger_py['damage'] / challenger_py['total']\n\n\n\n\nPlotting the proportion of damaged o-rings against the launch temperature shows the following picture:\n\n\nR\nPython\n\n\n\n\nggplot(challenger, aes(x = temp, y = prop_damaged)) +\n  geom_point()\n\n\n\n\n\n\nFigure 8.1: Scatterplot of temperature against proportion of damaged o-rings\n\n\n\n\n\n\n\np = (ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point())\n     \np.show()\n\n\n\n\n\n\nFigure 8.2: Scatterplot of temperature against proportion of damaged o-rings\n\n\n\n\n\n\n\nThe point on the left is the data point corresponding to the coldest flight experienced before the disaster, where five damaged o-rings were found. Fortunately, this did not result in a disaster.\nHere we’ll explore if we could have reasonably predicted the failure of both o-rings on the Challenger flight, where the launch temperature was 31 degrees Fahrenheit.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#creating-a-suitable-model",
    "href": "materials/glm-08-proportional-response.html#creating-a-suitable-model",
    "title": "\n8  Proportional response\n",
    "section": "\n8.5 Creating a suitable model",
    "text": "8.5 Creating a suitable model\nWe only have 23 data points in total. So we’re building a model on not that much data - we should keep this in mind when we draw our conclusions!\nWe are using a logistic regression for a proportion response in this case, since we’re interested in the proportion of o-rings that are damaged.\nWe can define this as follows:\n\n\nR\nPython\n\n\n\n\nglm_chl &lt;- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger)\n\nDefining the relationship for proportion responses is a bit annoying, where you have to give the glm model a two-column matrix to specify the response variable.\nHere, the first column corresponds to the number of damaged o-rings, whereas the second column refers to the number of intact o-rings. We use the cbind() function to bind these two together into a matrix.\n\n\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_py = model.fit()\n\n\n\n\nIf we’re using the original observations, we need to supply both the number of damaged o-rings and the number of intact ones.\n\n\n\n\n\n\nWhy don’t we use prop_damaged as our response variable?\n\n\n\nYou might have noticed that when we write our model formula, we are specifically writing cbind(damage, intact) (R) or damage + intact (Python), instead of the prop_damaged variable we used for plotting.\nThis is very deliberate - it’s absolutely essential we do this, in order to fit the correct model.\nWhen modelling our data, we need to provide the function with both the number of successes (damaged o-rings) and the number of failures (intact o-rings) - and therefore, implicitly, the total number of o-rings - in order to properly model our proportional variable.\nIf we provide it with just the prop_damaged, then R/Python will incorrectly think our response variable is fractional.\nA fractional variable is not constructed from a series of trials with successes/failures. It doesn’t have trials - there’s just a single value somewhere between 0 and 1 (or a percentage). An example of a fractional response variable might be what percentage of rocket fuel was used on a launch. We would not model this response variable with a logistic regression.\n\n\n\n\n\nFigure 8.3: Fractional vs proportional variables",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#model-output",
    "href": "materials/glm-08-proportional-response.html#model-output",
    "title": "\n8  Proportional response\n",
    "section": "\n8.6 Model output",
    "text": "8.6 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\n\nR\nPython\n\n\n\nNext, we can have a closer look at the results:\n\nsummary(glm_chl)\n\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe can see that the p-values of the intercept and temp are significant. We can also use the intercept and temp coefficients to construct the logistic equation, which we can use to sketch the logistic curve.\n\n\n\nprint(glm_chl_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -14.837\nDate:                  Thu, 24 Jul 2025   Deviance:                       16.912\nTime:                          08:45:49   Pearson chi2:                     28.1\nNo. Iterations:                       7   Pseudo R-squ. (CS):             0.6155\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.6630      3.296      3.538      0.000       5.202      18.124\ntemp          -0.2162      0.053     -4.066      0.000      -0.320      -0.112\n==============================================================================\n\n\n\n\n\n\\[E(prop \\ failed\\ orings) = \\frac{\\exp{(11.66 -  0.22 \\times temp)}}{1 + \\exp{(11.66 -  0.22 \\times temp)}}\\]\nLet’s see how well our model would have performed if we would have fed it the data from the ill-fated Challenger launch.\n\n\nR\nPython\n\n\n\n\nggplot(challenger, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\nFigure 8.4: Logistic curve of temperature against proportion of damaged o-rings\n\n\n\n\nWe get the warning Warning in eval(family$initialize): non-integer #successes in a binomial glm!. This is because we’re feeding a proportion value between 0 and 1 to a binomial GLM. What it is expecting is the number of successes out of a fixed number of trials. See the explanation above for more information. We’ll revisit this later!\n\n\nWe can get the predicted values for the model as follows:\n\nchallenger_py['predicted_values'] = glm_chl_py.predict()\n\nchallenger_py.head()\n\n   temp  damage  total  intact  prop_damaged  predicted_values\n0    53       5      6       1      0.833333          0.550479\n1    57       1      6       5      0.166667          0.340217\n2    58       1      6       5      0.166667          0.293476\n3    63       1      6       5      0.166667          0.123496\n4    66       0      6       6      0.000000          0.068598\n\n\nThis would only give us the predicted values for the data we already have. Instead we want to extrapolate to what would have been predicted for a wider range of temperatures. Here, we use a range of \\([25, 85]\\) degrees Fahrenheit.\n\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_py.predict(model)\n\nmodel.head()\n\n   temp      pred\n0    25  0.998087\n1    26  0.997626\n2    27  0.997055\n3    28  0.996347\n4    29  0.995469\n\n\n\np = (ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1))\n     \np.show()\n\n\n\n\n\n\nFigure 8.5: Logistic curve of temperature against proportion of damaged o-rings\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating predicted values\n\n\n\n\n\n\n\nR\nPython\n\n\n\nAnother way of doing this it to generate a table with data for a range of temperatures, from 25 to 85 degrees Fahrenheit, in steps of 1. We can then use these data to generate the logistic curve, based on the fitted model.\n\n# create a table with sequential numbers ranging from 25 to 85\nnew_data &lt;- tibble(temp = seq(25, 85, by = 1))\n\nmodel &lt;- new_data |&gt; \n  # add a new column containing the predicted values\n  mutate(.pred = predict(glm_chl, newdata = new_data, type = \"response\"))\n\nggplot(model, aes(temp, .pred)) +\n  geom_line()\n\n\n\n\n\n\nFigure 8.6: Logistic curve of temperature against probability\n\n\n\n\n\n# plot the curve and the original data\nggplot(model, aes(temp, .pred)) +\n  geom_line(colour = \"blue\") +\n  geom_point(data = challenger, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")\n\n\n\n\n\n\nFigure 8.7: Logistic curve of temperature against probability with launch temperature\n\n\n\n\nIt seems that there was a high probability of both o-rings failing at that launch temperature. One thing that the graph shows is that there is a lot of uncertainty involved in this model. We can tell, because the fit of the line is very poor at the lower temperature range. There is just very little data to work on, with the data point at 53 F having a large influence on the fit.\n\n\nWe already did this above, since this is the most straightforward way of plotting the model in Python.",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#exercises",
    "href": "materials/glm-08-proportional-response.html#exercises",
    "title": "\n8  Proportional response\n",
    "section": "\n8.7 Exercises",
    "text": "8.7 Exercises\n\n8.7.1 Rats and levers\n\n\n\n\n\n\nExercise 1 - Rats and levers\n\n\n\n\n\n\nLevel: \nThis exercises uses the dataset levers.csv.\nThese data are from a simple animal behavioural experiment. Prior to testing, rats were trained to press levers for food rewards. On each trial, there were two levers, and the “correct” lever is determined by an audio cue.\nThe rats could vary in three different ways, which may impact their task performance at testing:\n\nThe sex of the rat\nThe age of the rat (in weeks)\nWhether the rat experienced stress during the training phase (being exposed to the smell of a predator)\n\nThe researcher thinks that the effect of stress may differ between male and female rats.\nIn this exercise:\n\nVisualise these data\nFit a model that captures the researcher’s hypotheses\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nLoad and visualise\n\n\nR\nPython\n\n\n\n\nlevers &lt;- read_csv(\"data/levers.csv\")\n\nhead(levers)\n\n# A tibble: 6 × 7\n  rat_id stress_type sex    rat_age trials correct_presses prop_correct\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1      1 control     female      13     20               7         0.35\n2      2 control     female      15     20              11         0.55\n3      3 control     female      11     20               5         0.25\n4      4 control     female      15     20               5         0.25\n5      5 stressed    female      13     20               8         0.4 \n6      6 stressed    male        14     20               8         0.4 \n\n\n\n\n\nlevers = pd.read_csv(\"data/levers.csv\")\n\nlevers.head()\n\n   rat_id stress_type     sex  rat_age  trials  correct_presses  prop_correct\n0       1     control  female       13      20                7          0.35\n1       2     control  female       15      20               11          0.55\n2       3     control  female       11      20                5          0.25\n3       4     control  female       15      20                5          0.25\n4       5    stressed  female       13      20                8          0.40\n\n\n\n\n\nWe can see that this dataset contains quite a few columns. Key ones to pay attention to:\n\n\ntrials; the total number of trials per rat - we’re going to need this when fitting our model\n\ncorrect_presses; the number of successes (out of the total number of trials) - again, we’ll need this for model fitting\n\nprop_correct; this is \\(successes/total\\), which we’ll use for plotting but not model fitting\n\n\n\nR\nPython\n\n\n\n\nggplot(levers, aes(x = rat_age, \n                   y = prop_correct,\n                   colour = sex)) +\n  facet_wrap(~ stress_type) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(\n  ggplot(levers, aes(x='rat_age', y='prop_correct', colour='sex')) +\n  geom_point() +\n  facet_wrap('~stress_type')\n)\n\n&lt;plotnine.ggplot.ggplot object at 0x174f23950&gt;\n\n\n\n\n\nThere’s some visual evidence of an interaction here. In the control group, it seems like older rats perform a little better on the task, but there’s not much effect of sex.\nMeanwhile, in the stressed group, the female rats seem to be performing better than the male ones.\nFit a model\nLet’s assess those relationships by fitting a logistic model.\n\n\nR\nPython\n\n\n\n\n# Create a new variable for the number of incorrect presses\nlevers &lt;- levers |&gt;\n  mutate(incorrect_presses = trials - correct_presses)\n\n# Fit the model\nglm_lev &lt;- glm(cbind(correct_presses, incorrect_presses) ~ stress_type * sex + rat_age,\n               family = binomial,\n               data = levers)\n\n\n\n\n# Create a new variable for the number of incorrect presses\nlevers['incorrect_presses'] = levers['trials'] - levers['correct_presses']\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev = model.fit()\n\n\n\n\nMake model predictions\nUsing this model, let’s make a prediction of the expected proportion of correct lever presses for:\n\na male rat\n8 weeks old\nin the stressed condition\n\nFirst, we look at a summary of the model to extract the beta coefficients we need.\n\n\nR\nPython\n\n\n\n\nsummary(glm_lev)\n\n\nCall:\nglm(formula = cbind(correct_presses, incorrect_presses) ~ stress_type * \n    sex + rat_age, family = binomial, data = levers)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                 -2.51920    0.41956  -6.004 1.92e-09 ***\nstress_typestressed          0.93649    0.16166   5.793 6.92e-09 ***\nsexmale                      0.50633    0.18125   2.794  0.00521 ** \nrat_age                      0.13458    0.03197   4.210 2.56e-05 ***\nstress_typestressed:sexmale -1.43779    0.24921  -5.769 7.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 120.646  on 61  degrees of freedom\nResidual deviance:  60.331  on 57  degrees of freedom\nAIC: 275.8\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_lev.summary())\n\n                            Generalized Linear Model Regression Results                             \n====================================================================================================\nDep. Variable:     ['correct_presses', 'incorrect_presses']   No. Observations:                   62\nModel:                                                  GLM   Df Residuals:                       57\nModel Family:                                      Binomial   Df Model:                            4\nLink Function:                                        Logit   Scale:                          1.0000\nMethod:                                                IRLS   Log-Likelihood:                -132.90\nDate:                                      Thu, 24 Jul 2025   Deviance:                       60.331\nTime:                                              08:45:50   Pearson chi2:                     59.5\nNo. Iterations:                                           4   Pseudo R-squ. (CS):             0.6220\nCovariance Type:                                  nonrobust                                         \n=======================================================================================================\n                                          coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------------\nIntercept                              -2.5192      0.420     -6.004      0.000      -3.342      -1.697\nstress_type[T.stressed]                 0.9365      0.162      5.793      0.000       0.620       1.253\nsex[T.male]                             0.5063      0.181      2.794      0.005       0.151       0.862\nstress_type[T.stressed]:sex[T.male]    -1.4378      0.249     -5.769      0.000      -1.926      -0.949\nrat_age                                 0.1346      0.032      4.210      0.000       0.072       0.197\n=======================================================================================================\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlin_pred &lt;- -2.519 + \n            0.937 * 1 +     # 1 for stressed\n            0.506 * 1 +     # 1 for male\n            -1.438 * 1 +    # 1 for stressed:male\n            0.135 * 8       # 8 weeks old\n\n# these will give identical results\nexp(lin_pred) / (1 + exp(lin_pred))\n\n[1] 0.1924762\n\n1 / (1 + exp(-lin_pred))\n\n[1] 0.1924762\n\n\n\n\n\nlin_pred = (-2.519 + \n           0.937 * 1 +     # 1 for stressed\n           0.506 * 1 +     # 1 for male\n           -1.438 * 1 +    # 1 for stressed:male\n           0.135 * 8)       # 8 weeks old\n\n# these will give identical results\nmath.exp(lin_pred) / (1 + math.exp(lin_pred))\n\n0.19247620289503575\n\n1 / (1 + math.exp(-lin_pred))\n\n0.19247620289503575\n\n\n\n\n\nThis means we would expect an 8 week old stressed male rat to make approximately 19% correct button presses, on average.\n\n\n\n\n\n\n\n\n\n\n\nVisualising glm_lev\n\n\n\n\n\nThis is not part of the exercise - it involves quite a bit of new code - but it’s something you might want to do with your own data, so we’ll show it here.\nWith multiple predictors, you will have multiple logistic curves. What do they look like, and how can you produce them?\n\n\nR\nPython\n\n\n\nThe first thing we do is create a grid covering all the levels of our categorical predictor, and the full range of available values we have for the continuous predictor.\n\n# Create prediction grid\nnew_data &lt;- expand.grid(stress_type = levels(factor(levers$stress_type)),\n                        sex = levels(factor(levers$sex)),\n                        rat_age = seq(min(levers$rat_age), max(levers$rat_age), length.out = 100))\n\nhead(new_data)\n\n  stress_type    sex  rat_age\n1     control female 9.000000\n2    stressed female 9.000000\n3     control   male 9.000000\n4    stressed   male 9.000000\n5     control female 9.080808\n6    stressed female 9.080808\n\n\nThen, we use the predict function to figure out the expected proportion of correct responses at each combination of predictors that exists in that grid.\n\n# Predict proportion of correct responses\nnew_data$predicted_prob &lt;- predict(glm_lev, newdata = new_data, type = \"response\")\n\nWe can now use this grid of predictions to produce some nice lines, on top of the actual data points.\n\nggplot(levers, aes(x = rat_age, y = prop_correct,\n                     color = stress_type, linetype = sex)) +\n  geom_point() +\n  geom_line(data = new_data, aes(y = predicted_prob), linewidth = 1)\n\n\n\n\n\n\n\nYou could facet this plot further if you wanted to, but all the lines together help make the picture quite clear - we can see the interaction between stress:sex quite clearly!\n\n\nThe code to achieve this in Python very quickly becomes long, ugly and unwieldy.\nIf you’re absolutely determined to produce plots like this, you might want to use an IDE that lets you use both R and Python, and switch briefly into R for this!\n\n\n\n\n\n\n\n\n\n\n\n8.7.2 Stem cells\n\n\n\n\n\n\nExercise 2 - Stem cells\n\n\n\n\n\n\nLevel: \nFor this exercise, you will need the dataset stemcells.csv.\nThere’s no worked answer for this dataset - we recommend that you compare and discuss your answer with a neighbour.\nIn this dataset, each row represents a unique culture or population of stem cells (a plate). Each plate is exposed to one of three different growth_factor_conc concentration levels (low, medium or high).\nThe researcher wanted to record how much of a particular bio-marker was being expressed in the plate, to quantify cell differentiation.\nShe measured this outcome in multiple ways:\n\n\nmarker_positive_cells, the number of cells expressing the marker\n\nprop_positive_cells, the proportion of the total_cells count that have the marker\n\nmean_marker_intensity, the normalised average fluorescence intensity across the plate, measured on a scale between 0 and 1\n\nBetween plates, there was also variation in the time (days) before observation. This variable should be included as a control/covariate of no interest.\nYou should:\n\nProduce a scatterplot of the data\nDecide which outcome measurement is appropriate for a logistic regression\nFit an appropriate logistic model (with two predictor variables)\n\n\n\nR\nPython\n\n\n\n\nstemcells &lt;- read_csv(\"data/stemcells.csv\")\n\n\n\n\nstemcells = pd.read_csv(\"data/stemcells.csv\")\n\n\n\n\n\n\n\n\n\n\nHint #1\n\n\n\n\n\nRemember, you’ll need to input & combine two values to make up your response variable.\nWe won’t give away what those variables are (that defeats the point a bit!) but the appropriate code for fitting your logistic regression should look something like this:\n\n\nR\nPython\n\n\n\n\nglm_cells &lt;- glm(cbind(var, var) ~ growth_factor_conc + time, \n                 family = binomial,\n                 data = stemcells)\n\n\n\n\nmodel = smf.glm(formula = \"var + var ~ growth_factor_conc + time\",\n                family = sm.families.Binomial(),\n                data = stemcells)\n\nglm_cells = model.fit()\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint #2\n\n\n\n\n\nIf you’re struggling to figure out which of the outcome measures capture which information, here’s a little visualisation:\n\n\nMeasuring markers of differentiation in stem cells",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-08-proportional-response.html#summary",
    "href": "materials/glm-08-proportional-response.html#summary",
    "title": "\n8  Proportional response\n",
    "section": "\n8.8 Summary",
    "text": "8.8 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can use a logistic model for proportional response variables, just as we did with binary variables\nProportional response variables are made up of a number of trials (success/failure), and should not be confused with fractions/percentages\nUsing the equation of a logistic model, we can predict the expected proportion of “successful” trials",
    "crumbs": [
      "Binary responses",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html",
    "href": "materials/glm-09-significance-testing.html",
    "title": "\n9  Significance testing\n",
    "section": "",
    "text": "9.1 Context\nUp until now we’ve focussed on creating appropriate models for non-continuous data and making model predictions. In this section we’re going to focus on statistical significance testing.\nGeneralised linear models are fitted a little differently to standard linear models - namely, using maximum likelihood estimation instead of ordinary least squares for estimating the model coefficients.\nAs a result, we can no longer use F-tests for significance, or interpret \\(R^2\\) values in quite the same way. This section will introduce likelihood ratio tests, a method for extracting p-values for GLMs.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#libraries-and-functions",
    "href": "materials/glm-09-significance-testing.html#libraries-and-functions",
    "title": "\n9  Significance testing\n",
    "section": "\n9.2 Libraries and functions",
    "text": "9.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(performance)\n\n# You will likely need to install this package first\nlibrary(lmtest)\n\n\n\n\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\nimport numpy as np",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#sec-mat_deviance",
    "href": "materials/glm-09-significance-testing.html#sec-mat_deviance",
    "title": "\n9  Significance testing\n",
    "section": "\n9.3 Deviance",
    "text": "9.3 Deviance\nSeveral of the tests and metrics we’ll discuss below are based heavily on deviance. So, what is deviance, and where does it come from?\nHere’s a few key definitions:\n\n\n\n\n\n\n\nMaximum likelihood estimation\nThis is the method by which we fit the GLM (i.e., find the values for the beta coefficients). As the name suggests, we are trying to find the beta coefficients that maximise the likelihood of the dataset/sample.\n\n\nLikelihood\n\nIn this context, “likelihood” refers to the joint probability of all of the data points in the sample. In other words, how likely is it that you would sample a set of data points like these, if they were being drawn from an underlying population where your model is true?\nEach candidate model fitted to a dataset will have its own unique likelihood.\n\n\n\nSaturated (perfect) model\n\nFor each dataset, there is a “saturated”, or perfect, model. This model has the same number of parameters in it as there are data points, meaning the data are fitted exactly - as if connecting the dots between them.\nThis model has the largest possible likelihood of any model fitted to the dataset.\nOf course, we don’t actually use the saturated model for drawing real conclusions, but we can use it as a baseline for comparison.\n\n\n\nDeviance\n(residual deviance)\n\nEach candidate model is compared back to the saturated model to figure out its deviance.\nDeviance is defined as the difference between the log-likelihood of your fitted model and the log-likelihood of the saturated model (multiplied by 2).\nBecause deviance is all about capturing the discrepancy between fitted and actual values, it’s performing a similar function to the residual sum of squares (RSS) in a standard linear model. In fact, the RSS is really just a specific type of deviance.\nSometimes, the deviance of a candidate (non-null) model is referred to more fully as “residual deviance”.\n\n\n\nNull deviance\n\nOne of the models that we can compare against the saturated model is the null model (a model with no predictors). This gives us the deviance value for the null model.\nThis is the greatest deviance of any possible model that could be fitted to the data, because it explains zero variance in the response variable.\n\n\n\n\n\n\n\n\n\nFigure 9.1: Different models and their deviances",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#revisiting-the-diabetes-dataset",
    "href": "materials/glm-09-significance-testing.html#revisiting-the-diabetes-dataset",
    "title": "\n9  Significance testing\n",
    "section": "\n9.4 Revisiting the diabetes dataset",
    "text": "9.4 Revisiting the diabetes dataset\nAs a worked example, we’ll use a logistic regression fitted to the diabetes dataset that we saw in a previous section.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\ndiabetes_py.head()\n\n   glucose  diastolic  test_result\n0      148         72            1\n1       85         66            0\n2      183         64            1\n3       89         66            0\n4      137         40            1\n\n\n\n\n\nAs a reminder, this dataset contains three variables:\n\n\ntest_result, binary results of a diabetes test result (1 for positive, 0 for negative)\n\nglucose, the results of a glucose tolerance test\n\ndiastolic blood pressure\n\n\n\nR\nPython\n\n\n\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\n\n\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#what-are-the-p-values-in-the-summary",
    "href": "materials/glm-09-significance-testing.html#what-are-the-p-values-in-the-summary",
    "title": "\n9  Significance testing\n",
    "section": "\n9.5 What are the p-values in the summary?",
    "text": "9.5 What are the p-values in the summary?\nYou might have noticed that when you use summary to see the model output, it comes with some p-values automatically.\nWhat are they? Can you use/interpret them?\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      724\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -374.00\nDate:                Thu, 24 Jul 2025   Deviance:                       748.01\nTime:                        08:50:50   Pearson chi2:                     720.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.2282\nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept            -8.5711      2.703     -3.171      0.002     -13.869      -3.273\nglucose               0.0547      0.021      2.614      0.009       0.014       0.096\ndiastolic             0.0424      0.036      1.165      0.244      -0.029       0.114\nglucose:diastolic    -0.0002      0.000     -0.796      0.426      -0.001       0.000\n=====================================================================================\n\n\n\n\n\nEach individual parameter, or coefficient, has its own z-value and associated p-value. In each case, a hypothesis test has been performed - these are formally called Wald tests.\nThe null hypothesis for these Wald tests is that the value of the coefficient = 0. The idea is that if a coefficient isn’t significantly different from 0, then that parameter isn’t useful and could be dropped from the model.\nThese tests are the equivalent of the t-tests that are calculated as part of the summary output for standard linear models.\n\n\n\n\n\n\nWhy can’t we just use these p-values?\n\n\n\nIn some cases, you can. However, there are a few cases where they don’t give you all the info you need.\nFirstly: they don’t tell you about the significance of the model as a whole (versus the null model).\nSecondly: for categorical predictors, you will get a separate Wald p-value for each non-reference group (compared back to the reference group). This is not the same as a p-value for the categorical predictor as a whole. The Wald p-values can also be heavily affected by which group was chosen as the reference.\n\n\nIt’s typically preferable to use a likelihood ratio test instead.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#likelihood-ratio-tests-lrts",
    "href": "materials/glm-09-significance-testing.html#likelihood-ratio-tests-lrts",
    "title": "\n9  Significance testing\n",
    "section": "\n9.6 Likelihood ratio tests (LRTs)",
    "text": "9.6 Likelihood ratio tests (LRTs)\nWhen we were assessing the significance of standard linear models, we were able to use the F-statistic to determine:\n\nthe significance of the model versus a null model, and\nthe significance of individual predictors.\n\nWe can’t use these F-tests for GLMs, but we can use LRTs in a very similar way, to calculate p-values for both the model as a whole, and for individual variables. This work because, in general, if you have a test statistic and you know the distribution of that test statistic, then you can use this to calculate a p-value.\nThese tests are all built on the idea of deviance, or the likelihood ratio, as discussed above on this page. We can compare any two models fitted to the same dataset by looking at the difference in their deviances, also known as the difference in their log-likelihoods, or more simply as a likelihood ratio.\nHelpfully, this likelihood ratio approximately follows a chi-square distribution, which we can capitalise on to calculate a p-value. All we need is the number of degrees of freedom, which is equal to the difference in the number of parameters of the two models you’re comparing.\n\n\n\n\n\n\nWarning\n\n\n\nImportantly, we are only able to use this sort of test when one of the two models that we are comparing is a “simpler” version of the other, i.e., one model has a subset of the parameters of the other model.\nSo while we could perform an LRT just fine between these two models: Y ~ A + B + C and Y ~ A + B + C + D, or between any model and the null (Y ~ 1), we would not be able to use this test to compare Y ~ A + B + C and Y ~ A + B + D.\n\n\n\n9.6.1 Testing the model versus the null\nSince LRTs involve making a comparison between two models, we must first decide which models we’re comparing, and check that one model is a “subset” of the other.\nLet’s use an example from a previous section of the course, where we fitted a logistic regression to the diabetes dataset.\n\n\nR\nPython\n\n\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without predictors, respectively).\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nglm_null &lt;- glm(test_result ~ 1, \n                family = binomial, \n                data = diabetes)\n\nThen, we use the lrtest function from the lmtest package to perform the test itself; we include both the models that we want to compare, listing them one after another.\n\nlrtest(glm_dia, glm_null)\n\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ 1\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -374.0                         \n2   1 -468.3 -3 188.59  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see from the output that our chi-square statistic is significant, with a really small p-value. This tells us that, for the difference in degrees of freedom (here, that’s 3), the change in deviance is actually quite big. (In this case, you can use summary(glm_dia) to see those deviances - 936 versus 748!)\nIn other words, our model is better than the null.\n\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without our predictor, respectively).\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\nmodel = smf.glm(formula = \"test_result ~ 1\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_null_py = model.fit()\n\nUnlike in R, there isn’t a nice neat function for extracting the \\(\\chi^2\\) value, so we have to do a little bit of work by hand.\n\n# calculate the likelihood ratio (i.e. the chi-square value)\nlrstat = -2*(glm_null_py.llf - glm_dia_py.llf)\n\n# calculate the associated p-value\npvalue = chi2.sf(lrstat, glm_null_py.df_resid - glm_dia_py.df_resid)\n\nprint(lrstat, pvalue)\n\n188.5931483744455 1.228870036004379e-40\n\n\nThis gives us the likelihood ratio, based on the log-likelihoods that we’ve extracted directly from the models, which approximates a chi-square distribution.\nWe’ve also calculated the associated p-value, by providing the difference in degrees of freedom between the two models (in this case, that’s simply 1, but for more complicated models it’s easier to extract the degrees of freedom directly from the model as we’ve done here).\nHere, we have a large chi-square statistic and a small p-value. This tells us that, for the difference in degrees of freedom (here, that’s 1), the change in deviance is actually quite big. (In this case, you can use glm_dia_py.summary() to see those deviances - 936 versus 748!)\nIn other words, our model is better than the null.\n\n\n\n\n9.6.2 Testing individual predictors\nAs well as testing the overall model versus the null, we might want to test particular predictors to determine whether they are individually significant.\nThe way to achieve this is essentially to perform a series of “targeted” likelihood ratio tests. In each LRT, we’ll compare two models that are almost identical - one with, and one without, our variable of interest in each case.\n\n\nR\nPython\n\n\n\nThe first step is to construct a new model that doesn’t contain our predictor of interest. Let’s test the glucose:diastolic interaction.\n\nglm_dia_add &lt;- glm(test_result ~ glucose + diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nNow, we can use the lrtest function (or the anova function) to compare the models with and without the interaction:\n\nlrtest(glm_dia, glm_dia_add)\n\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ glucose + diastolic\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1   4 -374.00                     \n2   3 -374.32 -1 0.6288     0.4278\n\n\nThis tells us that our interaction glucose:diastolic isn’t significant - our more complex model doesn’t have a meaningful reduction in deviance.\nThis might, however, seem like a slightly clunky way to test each individual predictor. Luckily, we can also use our trusty anova function with an extra argument to tell us about individual predictors.\nBy specifying that we want to use a chi-squared test, we are able to construct an analysis of deviance table (as opposed to an analysis of variance table) that will perform the likelihood ratio tests for us for each predictor:\n\nanova(glm_dia, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: test_result\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \nNULL                                727     936.60             \nglucose            1  184.401       726     752.20  &lt; 2e-16 ***\ndiastolic          1    3.564       725     748.64  0.05905 .  \nglucose:diastolic  1    0.629       724     748.01  0.42779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou’ll spot that the p-values we get from the analysis of deviance table match the p-values you could calculate yourself using lrtest; this is just more efficient when you have a complex model!\n\n\nThe first step is to construct a new model that doesn’t contain our predictor of interest. Let’s test the glucose:diastolic interaction.\n\nmodel = smf.glm(formula = \"test_result ~ glucose + diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_add_py = model.fit()\n\nWe’ll then use the same code we used above, to compare the models with and without the interaction:\n\nlrstat = -2*(glm_dia_add_py.llf - glm_dia_py.llf)\n\npvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_dia_add_py.df_model)\n\nprint(lrstat, pvalue)\n\n0.6288201373599804 0.42778842576800746\n\n\nThis tells us that our interaction glucose:diastolic isn’t significant - our more complex model doesn’t have a meaningful reduction in deviance.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#exercises",
    "href": "materials/glm-09-significance-testing.html#exercises",
    "title": "\n9  Significance testing\n",
    "section": "\n9.7 Exercises",
    "text": "9.7 Exercises\n\n9.7.1 Predicting failure\n\n\n\n\n\n\nExercise 1 - Predicting failure\n\n\n\n\n\n\nLevel: \nIn the previous chapter, we used the challenger.csv dataset as a worked example.\nOur research question was: should NASA have cancelled the Challenger launch, based on the data they had about o-rings in previous launches?\nLet’s try to come up with an interpretation from these data, with the help of a likelihood ratio test.\nYou should:\n\nRefit the model (if it’s not still in your environment from last chapter)\nFit a null model (no predictors)\nPerform a likelihood ratio test to compare the model to the null model\nDecide what you think the answer is to the research question\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nLet’s read in our data and mutate it to contain the relevant variables (this is borrowed from the last chapter):\n\nchallenger &lt;- read_csv(\"data/challenger.csv\") |&gt;\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe create our logistic model like so:\n\nglm_chl &lt;- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger)\n\nWe can get the model parameters as follows:\n\nsummary(glm_chl)\n\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n\n\nAnd let’s visualise the model, just to make sure it looks sensible.\n\nggplot(challenger, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\nComparing against the null\nThe next question we can ask is: is our model any better than the null model?\nFirst, we define the null model; then we use lrtest to compare them.\n\nglm_chl_null &lt;- glm(cbind(damage, intact) ~ 1,\n                family = binomial,\n                data = challenger)\n\nlrtest(glm_chl, glm_chl_null)\n\nLikelihood ratio test\n\nModel 1: cbind(damage, intact) ~ temp\nModel 2: cbind(damage, intact) ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 -14.837                         \n2   1 -25.830 -1 21.985  2.747e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWith a very small p-value (and a large chi-square statistic), it would seem that the model is indeed significantly better than the null.\nSince there’s only one predictor variable, this is pretty much equivalent to saying that temp does predict the proportion of o-rings that are damaged.\n\n\n\nWe need to make sure we’ve read in our data, and mutated it to contain the relevant variables (this is borrowed from the last chapter):\nOur logistic regression is fitted like so:\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_py = model.fit()\n\nWe can get the model parameters as follows:\n\nprint(glm_chl_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -14.837\nDate:                  Thu, 24 Jul 2025   Deviance:                       16.912\nTime:                          08:50:50   Pearson chi2:                     28.1\nNo. Iterations:                       7   Pseudo R-squ. (CS):             0.6155\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.6630      3.296      3.538      0.000       5.202      18.124\ntemp          -0.2162      0.053     -4.066      0.000      -0.320      -0.112\n==============================================================================\n\n\nGenerate new model data:\n\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_py.predict(model)\n\nmodel.head()\n\n   temp      pred\n0    25  0.998087\n1    26  0.997626\n2    27  0.997055\n3    28  0.996347\n4    29  0.995469\n\n\nAnd let’s visualise the model:\n\np = (ggplot() +\n   geom_point(challenger_py, aes(x = \"temp\", y = \"prop_damaged\")) +\n   geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1))\n\np.show()\n\n\n\n\n\n\n\nComparing against the null\nThe next question we can ask is: is our model any better than the null model?\nFirst we need to define the null model:\n\n# create a linear model\nmodel = smf.glm(formula = \"damage + intact ~ 1\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_null_py = model.fit()\n\nprint(glm_chl_null_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       22\nModel Family:                  Binomial   Df Model:                            0\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -25.830\nDate:                  Thu, 24 Jul 2025   Deviance:                       38.898\nTime:                          08:50:51   Pearson chi2:                     58.5\nNo. Iterations:                       5   Pseudo R-squ. (CS):              0.000\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4463      0.314     -7.783      0.000      -3.062      -1.830\n==============================================================================\n\n\n\nlrstat = -2*(glm_chl_null_py.llf - glm_chl_py.llf)\n\npvalue = chi2.sf(lrstat, glm_chl_null_py.df_resid - glm_chl_py.df_resid)\n\nprint(lrstat, pvalue)\n\n21.985381067707717 2.747351270353041e-06\n\n\nWith a very small p-value (and a large chi-square statistic), it would seem that the model is indeed significantly better than the null.\nSince there’s only one predictor variable, this is pretty much equivalent to saying that temp does predict the proportion of o-rings that are damaged.\n\n\n\n\nSo, could NASA have predicted what happened?\nProbably, yes. They certainly should have listened to the engineers who were raising concerns based on these data. But that’s the subject of many documentaries, if you’re interested in the topic, so we won’t get into it here…\n\n\n\n\n\n\n\n\n\n9.7.2 Predicting failure (with a tweak)\n\n\n\n\n\n\nExercise 2 - Predicting failure (with a tweak)\n\n\n\n\n\n\nLevel: \nIn the challenger dataset, the data point at 53 degrees Fahrenheit is quite influential.\nWould the conclusions from the previous exercise still hold without that point?\nYou should:\n\nFit a model without this data point\nVisualise the new model\nDetermine whether there is a significant link between launch temperature and o-ring failure in the new model\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nFirst, we need to remove the influential data point:\n\nchallenger_new &lt;- challenger |&gt; filter(temp != 53)\n\nNow we can create a new generalised linear model, based on these data:\n\nglm_chl_new &lt;- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger_new)\n\nWe can get the model parameters as follows:\n\nsummary(glm_chl_new)\n\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger_new)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  5.68223    4.43138   1.282   0.1997  \ntemp        -0.12817    0.06697  -1.914   0.0556 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.375  on 21  degrees of freedom\nResidual deviance: 12.633  on 20  degrees of freedom\nAIC: 27.572\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnd let’s visualise the model:\n\nggplot(challenger_new, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85) +\n  # add a vertical line at 53 F temperature\n  geom_vline(xintercept = 53, linetype = \"dashed\")\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\nComparing against the null\nSo is our new model any better than the null?\nWe need to construct a new null model - we can’t use the one from the previous exercise, because it was fitted to a different dataset that had an extra observation.\n\nglm_chl_null_new &lt;- glm(cbind(damage, intact) ~ 1,\n                family = binomial,\n                data = challenger_new)\n\nlrtest(glm_chl_new, glm_chl_null_new)\n\nLikelihood ratio test\n\nModel 1: cbind(damage, intact) ~ temp\nModel 2: cbind(damage, intact) ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)  \n1   2 -11.786                       \n2   1 -13.657 -1 3.7421    0.05306 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe model is not significantly better than the null in this case, with a p-value here of just over 0.05.\n\n\n\nFirst, we need to remove the influential data point:\n\nchallenger_new_py = challenger_py.query(\"temp != 53\")\n\nWe can create a new generalised linear model, based on these data:\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_py = model.fit()\n\nWe can get the model parameters as follows:\n\nprint(glm_chl_new_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       20\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -11.786\nDate:                  Thu, 24 Jul 2025   Deviance:                       12.633\nTime:                          08:50:51   Pearson chi2:                     16.6\nNo. Iterations:                       6   Pseudo R-squ. (CS):             0.1564\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.6822      4.431      1.282      0.200      -3.003      14.368\ntemp          -0.1282      0.067     -1.914      0.056      -0.259       0.003\n==============================================================================\n\n\nGenerate new model data:\n\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_new_py.predict(model)\n\nmodel.head()\n\n   temp      pred\n0    25  0.922585\n1    26  0.912920\n2    27  0.902177\n3    28  0.890269\n4    29  0.877107\n\n\nAnd let’s visualise the model:\n\np = (ggplot() +\n   geom_point(challenger_new_py, aes(x = \"temp\", y = \"prop_damaged\")) +\n   geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1) +\n   # add a vertical line at 53 F temperature\n   geom_vline(xintercept = 53, linetype = \"dashed\"))\n\np.show()\n\n\n\n\n\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\nComparing against the null\nSo is our new model any better than the null?\nWe need to construct a new null model - we can’t use the one from the previous exercise, because it was fitted to a different dataset that had an extra observation.\n\n# create a linear model\nmodel = smf.glm(formula = \"damage + intact ~ 1\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_null_py = model.fit()\n\nprint(glm_chl_new_null_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            0\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -13.657\nDate:                  Thu, 24 Jul 2025   Deviance:                       16.375\nTime:                          08:50:51   Pearson chi2:                     16.8\nNo. Iterations:                       6   Pseudo R-squ. (CS):         -2.220e-16\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.0445      0.418     -7.286      0.000      -3.864      -2.226\n==============================================================================\n\n\n\nlrstat = -2*(glm_chl_new_null_py.llf - glm_chl_new_py.llf)\n\npvalue = chi2.sf(lrstat, glm_chl_new_null_py.df_resid - glm_chl_new_py.df_resid)\n\nprint(lrstat, pvalue)\n\n3.7421161935342973 0.053057208274015694\n\n\nThe model is not significantly better than the null in this case, with a p-value here of just over 0.05.\n\n\n\n\nSo, could NASA have predicted what happened? This model is not significantly different from the null, i.e., temperature is not a significant predictor.\nHowever, note that it’s only marginally non-significant, and this is with a data point removed.\nIt is possible that if more data points were available that followed a similar trend, the story might be different). Even if we did use our non-significant model to make a prediction, it doesn’t give us a value anywhere near 5 failures for a temperature of 53 degrees Fahrenheit. So overall, based on the model we’ve fitted with these data, there was no clear indication that a temperature just a few degrees cooler than previous missions could have been so disastrous for the Challenger.\n\n\n\n\n\n\n\n\n\n9.7.3 Revisiting rats and levers\n\n\n\n\n\n\nExercise 3 - Revisiting rats and levers\n\n\n\n\n\n\nLevel: \nLast chapter, we fitted a model to the levers.csv dataset in Exercise 8.7.1.\nNow, let’s test significance.\nIn this exercise, you should:\n\nFit a model with the predictors ~ stress_type * sex + rat_age\n\nAssess whether this model is significant over the null model\nAssess whether any of the 4 individual predictors (including the interaction) are significant\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nFit the model\nBefore we fit the model, we need to:\n\nRead the data in\nMutate to create an incorrect_presses variable\n\n\n\nR\nPython\n\n\n\n\nlevers &lt;- read_csv(\"data/levers.csv\")\n\nRows: 62 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): stress_type, sex\ndbl (5): rat_id, rat_age, trials, correct_presses, prop_correct\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlevers &lt;- levers |&gt;\n  mutate(incorrect_presses = trials - correct_presses)\n\nglm_lev &lt;- glm(cbind(correct_presses, incorrect_presses) ~ stress_type * sex + rat_age,\n               family = binomial,\n               data = levers)\n\n\n\n\nlevers = pd.read_csv(\"data/levers.csv\")\n\nlevers['incorrect_presses'] = levers['trials'] - levers['correct_presses']\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev = model.fit()\n\n\n\n\nCompare to the null\nWe also need to fit a null model to these data before we can do any comparisons.\n\n\nR\nPython\n\n\n\n\nglm_lev_null &lt;- glm(cbind(correct_presses, incorrect_presses) ~ 1,\n                    family = binomial,\n                    data = levers)\n\n\n\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ 1\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev_null = model.fit()\n\n\n\n\nNow, we run our likelihood ratio test comparing the two models.\n\n\nR\nPython\n\n\n\n\nanova(glm_lev, glm_lev_null)\n\nAnalysis of Deviance Table\n\nModel 1: cbind(correct_presses, incorrect_presses) ~ stress_type * sex + \n    rat_age\nModel 2: cbind(correct_presses, incorrect_presses) ~ 1\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \n1        57     60.331                         \n2        61    120.646 -4  -60.315 2.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nlrstat = -2*(glm_lev_null.llf - glm_lev.llf)\n\npvalue = chi2.sf(lrstat, glm_lev_null.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n\n60.31519356442419 2.490497342946823e-12\n\n\n\n\n\nThis is pretty significant, suggesting that our model is quite a bit better than the null.\nTest individual predictors\nNow, let’s test individual predictors.\n\n\nR\nPython\n\n\n\nThis is extremely easy to do in R. We produce an analysis of deviance table with anova, using the chi-square statistic for our likelihood ratios.\n\nanova(glm_lev, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(correct_presses, incorrect_presses)\n\nTerms added sequentially (first to last)\n\n                Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                               61    120.646              \nstress_type      1    8.681        60    111.965 0.0032150 ** \nsex              1    5.389        59    106.576 0.0202584 *  \nrat_age          1   12.216        58     94.359 0.0004738 ***\nstress_type:sex  1   34.028        57     60.331 5.432e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nLet’s build two new candidate models: one with rat_age removed, and one with the stress:sex interaction removed.\n(If the interaction isn’t significant, then we’ll push on and look at the main effects of stress and sex, if needed.)\nThe age effect\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex\",\n                family = sm.families.Binomial(),\n                data = levers)\nglm_lev_dropage = model.fit()\n\nlrstat = -2*(glm_lev_dropage.llf - glm_lev.llf)\npvalue = chi2.sf(lrstat, glm_lev_dropage.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n\n17.8705812026094 2.364481283464226e-05\n\n\nThe stress:sex interaction\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type + sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\nglm_lev_dropint = model.fit()\n\nlrstat = -2*(glm_lev_dropint.llf - glm_lev.llf)\npvalue = chi2.sf(lrstat, glm_lev_dropint.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n\n34.02832899366183 5.431548556703029e-09\n\n\nSince the interaction is significant, we don’t really need the specifics of the main effects (it becomes hard to interpret!)",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-09-significance-testing.html#summary",
    "href": "materials/glm-09-significance-testing.html#summary",
    "title": "\n9  Significance testing\n",
    "section": "\n9.8 Summary",
    "text": "9.8 Summary\nLikelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and goodness-of-fit. To determine the quality of a model and draw conclusions from it, it’s important to assess both of these things.\n\n\n\n\n\n\nKey points\n\n\n\n\nDeviance is the difference between predicted and actual values, and is calculated by comparing a model’s log-likelihood to that of the perfect “saturated” model\nUsing deviance, likelihood ratio tests can be used in lieu of F-tests for generalised linear models\nThis is distinct from (and often better than) using the Wald p-values that are reported automatically in the model summary",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance testing</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html",
    "href": "materials/glm-10-goodness-of-fit.html",
    "title": "\n10  Goodness-of-fit\n",
    "section": "",
    "text": "10.1 Context\nIn the previous chapter we’ve discussed significance testing. Here we are focusing on the goodness-of-fit of our model. This is all about how well a model fits the data, and typically involves summarising the discrepancy between the actual data points, and the fitted/predicted values that the model produces.\nThough closely linked, it’s important to realise that goodness-of-fit and significance don’t come hand-in-hand automatically: we might find a model that is significantly better than the null, but is still overall pretty rubbish at matching the data. So, to understand the quality of our model better, we should ideally perform both types of test.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#libraries-and-functions",
    "href": "materials/glm-10-goodness-of-fit.html#libraries-and-functions",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.2 Libraries and functions",
    "text": "10.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\ninstall.packages(\"lmtest\")\nlibrary(lmtest)\n\n\n\n\nfrom scipy.stats import *",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#data-and-model",
    "href": "materials/glm-10-goodness-of-fit.html#data-and-model",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.3 Data and model",
    "text": "10.3 Data and model\nWe’ll continue using the data and model for the diabetes dataset, which were defined as follows:\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nglm_null &lt;- glm(test_result ~ 1, \n                family = binomial, \n                data = diabetes)\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\nmodel = smf.glm(formula = \"test_result ~ 1\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_null_py = model.fit()",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#chi-square-tests",
    "href": "materials/glm-10-goodness-of-fit.html#chi-square-tests",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.4 Chi-square tests",
    "text": "10.4 Chi-square tests\nIn Section 9.3 we talked about deviance and chi-square tests, to assess significance.\nWe can use these in a very similar way to assess the goodness-of-fit of a model.\nWhen we compared our model against the null (last chapter), we tested the null hypothesis that the candidate model and the null model had the same deviance.\nNow, however, we will test the null hypothesis that the fitted model and the saturated (perfect) model have the same deviance, i.e., that they both fit the data equally well.\n\n\n\n\n\nFigure 10.1: Using deviance to assess goodness-of-fit\n\n\nIn most hypothesis tests, we want to reject the null hypothesis, but in this case, we’d like it to be true. This is because what checking is whether the model we just fitted is just as good (or good enough) as the saturated model.\n\n\nR\nPython\n\n\n\nRunning a goodness-of-fit chi-square test in R can be done using the pchisq function. We need to include two arguments: 1) the residual deviance, and 2) the residual degrees of freedom. Both of these can be found in the summary output, but you can use the $ syntax to call these properties directly like so:\n\npchisq(glm_dia$deviance, glm_dia$df.residual, lower.tail = FALSE)\n\n[1] 0.2605931\n\n\n\n\nThe syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:\n\npvalue = chi2.sf(glm_dia_py.deviance, glm_dia_py.df_resid)\n\nprint(pvalue)\n\n0.26059314630406843\n\n\n\n\n\nYou can think about this p-value, roughly, as “the probability that this model is good”. We’re not below our significance threshold, which means that we’re not rejecting our null hypothesis (which is a good thing) - but it’s also not a huge probability. This suggests that there’s probably other variables we could measure and include in a future experiment, to give a better overall model.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#aic-values",
    "href": "materials/glm-10-goodness-of-fit.html#aic-values",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.5 AIC values",
    "text": "10.5 AIC values\nYou might remember AIC values from standard linear modelling. AIC values are useful, because they tell us about overall model quality, factoring in both goodness-of-fit and model complexity.\nOne of the best things about the Akaike information criterion (AIC) is that it isn’t specific to linear models - it works for models fitted with maximum likelihood estimation.\nIn fact, if you look at the formula for AIC, you’ll see why:\n\\[\nAIC = 2k - 2ln(\\hat{L})\n\\]\nwhere \\(k\\) represents the number of parameters in the model, and \\(\\hat{L}\\) is the maximised likelihood function. In other words, the two parts of the equation represent the complexity of the model, versus the log-likelihood.\nThis means that AIC can be used for model comparison for GLMs in precisely the same way as it’s used for linear models: lower AIC indicates a better-quality model.\n\n\nR\nPython\n\n\n\nThe AIC value is given as standard, near the bottom of the summary output (just below the deviance values). You can also print it directly using the $ syntax:\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n\nglm_dia$aic\n\n[1] 756.0069\n\n\nIn even better news for R users, the step function works for GLMs just as it does for linear models, so long as you include the test = LRT argument.\n\nstep(glm_dia, test = \"LRT\")\n\nStart:  AIC=756.01\ntest_result ~ glucose * diastolic\n\n                    Df Deviance    AIC     LRT Pr(&gt;Chi)\n- glucose:diastolic  1   748.64 754.64 0.62882   0.4278\n&lt;none&gt;                   748.01 756.01                 \n\nStep:  AIC=754.64\ntest_result ~ glucose + diastolic\n\n            Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;           748.64 754.64                     \n- diastolic  1   752.20 756.20   3.564  0.05905 .  \n- glucose    1   915.52 919.52 166.884  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:  glm(formula = test_result ~ glucose + diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n(Intercept)      glucose    diastolic  \n   -6.49941      0.03836      0.01407  \n\nDegrees of Freedom: 727 Total (i.e. Null);  725 Residual\nNull Deviance:      936.6 \nResidual Deviance: 748.6    AIC: 754.6\n\n\n\n\nThe AIC value isn’t printed as standard with the model summary, but you can access it easily like so:\n\nprint(glm_dia_py.aic)\n\n756.0068586069744",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#pseudo-r2",
    "href": "materials/glm-10-goodness-of-fit.html#pseudo-r2",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.6 Pseudo \\(R^2\\)\n",
    "text": "10.6 Pseudo \\(R^2\\)\n\nRefresher on \\(R^2\\)\n\nIn linear modelling, we could extract and interpret \\(R^2\\) values that summarised our model. \\(R^2\\) in linear modelling represents a few different things:\n\nThe proportion of variance in the response variable, that’s explained by the model (i.e., jointly by the predictors)\nThe improvement of the model over the null model\nThe square of the Pearson’s correlation coefficient\n\nThe first one on that list is the interpretation we usually use it for, in linear modelling.\nWhat is a “pseudo \\(R^2\\)”?\nIt’s not possible to calculate \\(R^2\\) for a GLM like you can for a linear model.\nHowever, because people are fond of using \\(R^2\\), statisticians have developed alternatives that can be used instead.\nThere is no single value that can replace \\(R^2\\) and have all the same interpretations, so several different metrics have been proposed. Depending how they’re calculated, they all have different interpretations.\nThere are many. Some of the most popular are McFadden’s, Nagelkerke’s, Cox & Snell’s, and Tjur’s. This post does a nice job of discussing some of them and providing some comparisons.\nShould you use pseudo \\(R^2\\)?\nWe recommend not to use pseudo \\(R^2\\).\n(Unless you are very statistically-minded and prepared to wade through a lot of mathematical explanation…)\nThis is for a few reasons:\n\nIt’s too easy to fall into the bad habit of treating it like regular \\(R^2\\), and making bad interpretations\nEven if you’ve done a good job, your readers might make their own bad interpretations\nFiguring out which version to use, and what they all mean, is a minefield\nIt doesn’t really tell you anything that a chi-square test and/or AIC can’t tell you\n\nThe main reason we’ve mentioned it here is because you are likely to come across pseudo \\(R^2\\) when reading research papers that use GLMs - we want you to know what they are!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#exercises",
    "href": "materials/glm-10-goodness-of-fit.html#exercises",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.7 Exercises",
    "text": "10.7 Exercises\n\n10.7.1 Revisiting aphids\n\n\n\n\n\n\nExercise 1 - Revisiting aphids\n\n\n\n\n\n\nLevel: \nBack in Exercise 7.9.2, we fitted a logistic model to the aphids dataset - the code is included below in case you need to run it again.\nNow, let’s assess the goodness-of-fit of that model.\nYou should:\n\nCompute a chi-square goodness-of-fit test for the full model (~ buds + cultivar)\nCalculate the AIC value for the full model\nUse backwards stepwise elimination to determine whether dropping the buds and/or cultivar predictors improves the goodness-of-fit\n\n\n\nR\nPython\n\n\n\n\naphids &lt;- read_csv(\"data/aphids.csv\")\n\nglm_aphids &lt;- glm(aphids_present ~ buds + cultivar,\n                  family = binomial,\n                  data = aphids)\n\n\n\n\naphids = pd.read_csv(\"data/aphids.csv\")\n\nmodel = smf.glm(formula = \"aphids_present ~ buds + cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids = model.fit()\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nChi-square goodness-of-fit\nThis is a simple one-function task:\n\n\nR\nPython\n\n\n\n\npchisq(glm_aphids$deviance, glm_aphids$df.residual, lower.tail = FALSE)\n\n[1] 0.1666178\n\n\n\n\nThe syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:\n\npvalue = chi2.sf(glm_aphids.deviance, glm_aphids.df_resid)\n\nprint(pvalue)\n\n0.16661777677427902\n\n\n\n\n\nExtract AIC for full model\n\n\nR\nPython\n\n\n\nWe can access the AIC either in the model summary:\n\nsummary(glm_aphids)\n\n\nCall:\nglm(formula = aphids_present ~ buds + cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)     -2.0687     0.7483  -2.765  0.00570 **\nbuds             0.2067     0.1262   1.638  0.10149   \ncultivarmozart   1.9621     0.6439   3.047  0.00231 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.666  on 51  degrees of freedom\nAIC: 66.666\n\nNumber of Fisher Scoring iterations: 4\n\n\nor directly using the $ syntax:\n\nglm_aphids$aic\n\n[1] 66.66602\n\n\n\n\nThe AIC value is an “attribute” of the model object, which we can access like so:\n\nprint(glm_aphids.aic)\n\n66.66602340347231\n\n\n\n\n\nBackwards stepwise elimination\nLast but not least, let’s see if dropping either or both of the predictors improves the model quality. (Spoiler: it probably won’t!)\n\n\nR\nPython\n\n\n\nWe use the convenient step function for this - don’t forget the test = LRT argument, though.\n\nstep(glm_dia, test = \"LRT\")\n\nStart:  AIC=756.01\ntest_result ~ glucose * diastolic\n\n                    Df Deviance    AIC     LRT Pr(&gt;Chi)\n- glucose:diastolic  1   748.64 754.64 0.62882   0.4278\n&lt;none&gt;                   748.01 756.01                 \n\nStep:  AIC=754.64\ntest_result ~ glucose + diastolic\n\n            Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;           748.64 754.64                     \n- diastolic  1   752.20 756.20   3.564  0.05905 .  \n- glucose    1   915.52 919.52 166.884  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:  glm(formula = test_result ~ glucose + diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n(Intercept)      glucose    diastolic  \n   -6.49941      0.03836      0.01407  \n\nDegrees of Freedom: 727 Total (i.e. Null);  725 Residual\nNull Deviance:      936.6 \nResidual Deviance: 748.6    AIC: 754.6\n\n\nSince neither of our reduced models improve on the AIC versus our original model, we don’t drop either predictor, and the process stops there.\n\n\nWe need to build two new candidate models. In each case, we drop just one variable.\n\n# Dropping buds\nmodel = smf.glm(formula = \"aphids_present ~ cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids_dropbuds = model.fit()\n\n# Dropping cultivar\nmodel = smf.glm(formula = \"aphids_present ~ buds\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids_dropcultivar = model.fit()\n\nNow, we can look at the three AIC values next to each other, to determine which of these is the best option.\n\nprint(glm_aphids.aic,\n      glm_aphids_dropbuds.aic, \n      glm_aphids_dropcultivar.aic)\n\n66.66602340347231 67.51106908352895 75.19195097185758\n\n\nSince neither of our reduced models improve on the AIC versus our original model, we don’t drop either predictor, and the process stops there.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-10-goodness-of-fit.html#summary",
    "href": "materials/glm-10-goodness-of-fit.html#summary",
    "title": "\n10  Goodness-of-fit\n",
    "section": "\n10.8 Summary",
    "text": "10.8 Summary\nLikelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and goodness-of-fit. To determine the quality of a model and draw conclusions from it, it’s important to assess both of these things.\n\n\n\n\n\n\nKey points\n\n\n\n\nA chi-square goodness-of-fit test can also be performed using likelihood/deviance\nThe Akaike information criterion is also based on likelihood, and can be used to compare the quality of GLMs fitted to the same dataset\nOther metrics that may be of use are Wald test p-values and pseudo \\(R^2\\) values (if interpreted thoughtfully)",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html",
    "href": "materials/glm-11-checking-assumptions.html",
    "title": "\n11  Checking assumptions\n",
    "section": "",
    "text": "11.1 Context\nWe can now assess the quality of a generalised linear model. Although we can relax certain assumptions compared to standard linear models (linearity, equality of variance of residuals, and normality of residuals), we cannot relax all of them - some key assumptions still exist. We discuss these below.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html#libraries-and-functions",
    "href": "materials/glm-11-checking-assumptions.html#libraries-and-functions",
    "title": "\n11  Checking assumptions\n",
    "section": "\n11.2 Libraries and functions",
    "text": "11.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(ggResidpanel)\nlibrary(performance)\n\n\n\n\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrix\n\n\n\n\n\n\n\n\n11.2.1 Assumption 1: Distribution of response variable\nAlthough we don’t expect our response variable \\(y\\) to be continuous and normally distributed (as we did in linear modelling), we do still expect its distribution to come from the “exponential family” of distributions.\nThe exponential family\nThe exponential family contains the following distributions, among others:\n\nnormal\nexponential\nPoisson\nBernoulli\nbinomial (for fixed number of trials)\nchi-squared\nYou can use a histogram to visualise the distribution of your response variable, but it is typically most useful just to think about the nature of your response variable. For instance, binary variables will follow a Bernoulli distribution, proportional variables follow a binomial distribution, and most count variables will follow a Poisson distribution.\nIf you have a very unusual variable that doesn’t follow one of these exponential family distributions, however, then a GLM will not be an appropriate choice. In other words, a GLM is not necessarily a magic fix!\n\n11.2.2 Assumption 2: Correct link function\nA closely-related assumption to assumption 1 above, is that we have chosen the correct link function for our model.\nIf we have done so, then there should be a linear relationship between our transformed model and our response variable; in other words, if we have chosen the right link function, then we have correctly “linearised” our model.\n\n11.2.3 Assumption 3: Independence\nWe expect that the each observation or data point in our sample is independent of all the others. Specifically, we expect that our set of \\(y\\) response variables are independent of one another.\nFor this to be true, we have to make sure:\n\nthat we aren’t treating technical replicates as true/biological replicates;\nthat we don’t have observations/data points in our sample that are artificially similar to each other (compared to other data points);\nthat we don’t have any nuisance/confounding variables that create “clusters” or hierarchy in our dataset;\nthat we haven’t got repeated measures, i.e., multiple measurements/rows per individual in our sample\n\nThere is no diagnostic plot for assessing this assumption. To determine whether your data are independent, you need to understand your experimental design.\nYou might find this page useful if you’re looking for more information on what counts as truly independent data.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html#other-features-to-check",
    "href": "materials/glm-11-checking-assumptions.html#other-features-to-check",
    "title": "\n11  Checking assumptions\n",
    "section": "\n11.3 Other features to check",
    "text": "11.3 Other features to check\nThere are a handful of other features or qualities that can affect the quality of model fit, and therefore the quality of the inferences we draw from it.\nThese are not necessarily “formal” assumptions, but it’s good practice to check for them.\nLack of influential observations\nA data point is overly influential, i.e., has high leverage, if removing that point from the dataset would cause large changes in the model coefficients. Data points with high leverage are typically those that don’t follow the same general “trend” as the rest of the data.\nLack of collinearity\nCollinearity is when predictor variables are overly/strongly correlated with each other. This can make it very difficult to estimate the right beta coefficients and individual p-values for those predictors.\nDispersion\nThis is mentioned here for completeness, and as a bit of sizzle for next chapter when we talk about Poisson regression.\nWe won’t be worrying about dispersion in logistic regression specifically.\nIf you want more detail, you can skip ahead now to Section 12.7.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html#assessing-assumptions-quality",
    "href": "materials/glm-11-checking-assumptions.html#assessing-assumptions-quality",
    "title": "\n11  Checking assumptions\n",
    "section": "\n11.4 Assessing assumptions & quality",
    "text": "11.4 Assessing assumptions & quality\nIn linear modelling, we rely heavily on visuals to determine whether various assumptions were met.\nWhen checking assumptions and assessing quality of fit for GLMs, we don’t use the panel of diagnostic plots that we used for linear models any more. However, there are some visualisations that can help us, plus several metrics we can calculate from our data or model.\n\n\n\n\n\n\n\n\nIs it a formal assumption?\nHow can I assess it?\n\n\n\n\nResponse variable comes from a distribution in the exponential family\n&\nThe model uses the right link function\n\nYes\n\nKnowledge of the experimental design\nSome plots, e.g., posterior predictive check/uniform Q-Q, may help\n\n\n\nIndependent observations\nYes\n\nKnowledge of experimental design\nThere are no formal tests or plots that assess independence reliably\n\n\n\nInfluential observations\nNo\nCook’s distance/leverage plot\n\n\nLack of collinearity\nNo\nVariance inflation factor\n\n\nDispersion\nSort of\n\nCalculating the dispersion parameter\nCan also be visualised\n\n\n\n\nLet’s fit some useful diagnostic plots, using the diabetes and aphids example datasets.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nWe’re going to rely heavily on the performance package in R for assessing the assumptions and fit of our model.\nThe check_model function will be our primary workhorse. This function automatically detects the type of model you give it, and produces the appropriate panel of plots all by itself. (So, so cool. And yes, it works for linear models too!)\n\ncheck_model(glm_dia)\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\nFigure 11.1: Diagnostic plots for the glm_dia model\n\n\n\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_dia_py = model.fit()\n\n\n\n\n\n11.4.1 Influential observations\n\n\nR\nPython\n\n\n\nWe can check for outliers via a leverage plot (in performance), which you may remember from linear modelling.\nIdeally, data points would fall inside the green contour lines. Data points that don’t will be highlighted in red.\n\ncheck_model(glm_dia, check = 'outliers')\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\nFigure 11.2: Influential points check for glm_dia model\n\n\n\n\nAlternatively, we can use the check_outliers function:\n\ncheck_outliers(glm_dia, threshold = list('cook' = 0.5))\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\n\n\nWe can use the get_influence function to extract information like leverage, Cook’s distance etc. about all of our data points:\n\n# extract the Cook's distances\nglm_dia_py_resid = pd.DataFrame(glm_dia_py.\n                                get_influence().\n                                summary_frame()[\"cooks_d\"])\n\n# add row index \nglm_dia_py_resid['obs'] = glm_dia_py_resid.reset_index().index\n\nWe now have two columns:\n\nglm_dia_py_resid.head()\n\n    cooks_d  obs\n0  0.000709    0\n1  0.000069    1\n2  0.000641    2\n3  0.000080    3\n4  0.009373    4\n\n\nWe can use these to create the plot:\n\np = (ggplot(glm_dia_py_resid,\n         aes(x = \"obs\",\n             y = \"cooks_d\")) +\n     geom_segment(aes(x = \"obs\", y = \"cooks_d\", xend = \"obs\", yend = 0)) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 11.3: Influential points check for glm_dia_py model\n\n\n\n\n\n\n\n\n\n\nAlternative method using matplotlib\n\n\n\n\n\n\ninfluence = glm_dia_py.get_influence()\n\nThen, we can visualise and interrogate that information.\nWe can produce a Cook’s distance plot (this uses matplotlib):\n\ncooks = influence.cooks_distance[0]\n\nplt.stem(np.arange(len(cooks)), cooks, markerfmt=\",\")\nplt.axhline(0.5, color='red', linestyle='--', label='Threshold')\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance Plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\nFigure 11.4: Influential points check for glm_dia_py model\n\n\n\n\nand/or we can extract a list of data points with a Cook’s distance greater than some specified threshold:\n\ninfluential_points = np.where(cooks &gt; 0.5)[0] # Set appropriate threshold here\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: []\n\n\nThe list is empty, indicating no high leverage points that we need to worry about.\n\n\n\nWe can check which points may be influential, for example by setting a threshold of &gt; 0.5:\n\ninfluential_points = glm_dia_py_resid[glm_dia_py_resid[\"cooks_d\"] &gt; 0.5]\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: Empty DataFrame\nColumns: [cooks_d, obs]\nIndex: []\n\n\nThe DataFrame is empty, indicating no high leverage points that we need to worry about.\n\n\n\n\n\n\n\n\n\nDealing with “outliers”\n\n\n\nRemember:\n\nOutliers and influential points aren’t necessarily the same thing; all outliers need to be followed up, to check if they are influential in a problematic way\nYou can’t just drop data points because they are inconvenient! This is cherry-picking, and it can impact the quality and generalisability of your conclusions\n\n\n\n\n11.4.2 Lack of collinearity\nThe best way to assess whether we have collinearity is to look at something called the variance inflation factor (VIF).\nWhen calculating VIF for a model, a separate VIF value will be generated for each predictor. VIF &gt;5 is worth an eyebrow raise; VIF &gt;10 definitely needs some follow-up; and VIF &gt;20 suggests a really strong correlation that is definitely problematic.\n\n\nR\nPython\n\n\n\n\ncheck_collinearity(glm_dia)\n\nModel has interaction terms. VIFs might be inflated.\n  Try to center the variables used for the interaction, or check\n  multicollinearity among predictors of a model without interaction terms.\n\n\n# Check for Multicollinearity\n\nHigh Correlation\n\n              Term   VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n           glucose 37.99 [32.97, 43.80]     6.16      0.03     [0.02, 0.03]\n         diastolic 24.24 [21.06, 27.92]     4.92      0.04     [0.04, 0.05]\n glucose:diastolic 69.45 [60.21, 80.14]     8.33      0.01     [0.01, 0.02]\n\n\nWe can also visualise the VIFs, if we prefer, with the VIF plot in check_model:\n\ncheck_model(glm_dia, check = \"vif\")\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\nFigure 11.5: Collinearity check for glm_dia model\n\n\n\n\n\n\nThe statsmodels package contains a function for calculating VIF.\nIt uses the original dataset, rather than the model, to do this. This means you have to manually exclude the response variable, and then use the dmatrix function from patsy to .\nTo make sure all that\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrix\n\n# Drop the response variable\nX = diabetes_py.drop(columns='test_result')\n\n# Create design matrix based on model formula\nX = dmatrix(\"glucose * diastolic\", data=diabetes_py, return_type='dataframe')\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n             feature         VIF\n0          Intercept  600.539179\n1            glucose   36.879125\n2          diastolic   17.407953\n3  glucose:diastolic   63.987700\n\n\n\n\n\nThere is definitely some collinearity going on in our model - these VIF values are way over 10.\nThe most likely culprit for this is the interaction term - let’s see if we can improve things by dropping it:\n\n\nR\nPython\n\n\n\n\nglm_dia_add &lt;- glm(test_result ~ glucose + diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\n\ncheck_collinearity(glm_dia_add)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n      Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n   glucose 1.02 [1.00, 1.78]     1.01      0.98     [0.56, 1.00]\n diastolic 1.02 [1.00, 1.78]     1.01      0.98     [0.56, 1.00]\n\ncheck_model(glm_dia_add, check = \"vif\")\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\nFigure 11.6: Collinearity check for glm_dia_add model\n\n\n\n\n\n\nWe will try again, this time without manually adding the interaction:\n\n# 1. Drop the response variable\nX = diabetes_py.drop(columns='test_result')\n\n# 2. Add constant column for intercept\nX = sm.add_constant(X)\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n     feature        VIF\n0      const  42.747425\n1    glucose   1.052426\n2  diastolic   1.052426\n\n\n\n\n\nMuch better!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html#exercises",
    "href": "materials/glm-11-checking-assumptions.html#exercises",
    "title": "\n11  Checking assumptions\n",
    "section": "\n11.5 Exercises",
    "text": "11.5 Exercises\n\n11.5.1 Revisiting rats and levers (again)\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\n####Revisiting rats and levers (again)\nLevel: \nIn Exercise 8.7.1 and Exercise 9.7.3, you worked through the levers dataset, fitting an appropriate model and then assessing its significance.\nNow, using what you’ve learned in this chapter, assess whether this logistic regression is appropriate:\n\n\nR\nPython\n\n\n\n\nlevers &lt;- read_csv(\"data/levers.csv\")\n\nlevers &lt;- levers |&gt; \n  mutate(incorrect_presses = trials - correct_presses)\n\nglm_lev &lt;- glm(cbind(correct_presses, incorrect_presses) ~ stress_type * sex + rat_age,\n               family = binomial,\n               data = levers)\n\n\n\n\nlevers = pd.read_csv(\"data/levers.csv\")\n\nlevers['incorrect_presses'] = levers['trials'] - levers['correct_presses']\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev = model.fit()\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nConsider the response variable\nIs a logistic regression, with a logit link function, appropriate?\nFor the answer to be “yes”, then our response variable needs to be either a binary or a proportional variable (binomially distributed). We’re looking for those success/fail trials.\nSometimes, it can help to visualise the design:\n\n\nExperimental design for rat lever experiment\n\nWe can see that for each rat, the proportion score is made up of a series of trials, each of which can either be correct (+) or incorrect (-).\nConsider independence\nAgain, we need to think about the design.\nIt might initially seem as if we have multiple observations per animal - since each rat pressed a whole bunch of levers - but actually if we look at each row of our dataset:\n\n\nR\nPython\n\n\n\n\nhead(levers)\n\n# A tibble: 6 × 8\n  rat_id stress_type sex    rat_age trials correct_presses prop_correct\n   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;\n1      1 control     female      13     20               7         0.35\n2      2 control     female      15     20              11         0.55\n3      3 control     female      11     20               5         0.25\n4      4 control     female      15     20               5         0.25\n5      5 stressed    female      13     20               8         0.4 \n6      6 stressed    male        14     20               8         0.4 \n# ℹ 1 more variable: incorrect_presses &lt;dbl&gt;\n\n\n\n\n\nlevers.head()\n\n   rat_id stress_type     sex  ...  correct_presses  prop_correct  incorrect_presses\n0       1     control  female  ...                7          0.35                 13\n1       2     control  female  ...               11          0.55                  9\n2       3     control  female  ...                5          0.25                 15\n3       4     control  female  ...                5          0.25                 15\n4       5    stressed  female  ...                8          0.40                 12\n\n[5 rows x 8 columns]\n\n\n\n\n\nit becomes clear that each row of the dataset represents a rat, rather than each row representing a separate trial/button press.\nIn other words: by collecting the proportion of correct trials, we have averaged across the rat, and so it only appears once in our dataset.\nThis means that each of the rows of the dataset truly are independent.\nNow, this does make some assumptions about the nature of the rats’ relationships to each other. Some non-independence could be introduced if:\n\nSome of the rats are genetically more similar to each other, e.g., if multiple rats from the same litter were included\nRats were trained together before testing\nRats were exposed to stress in one big batch (i.e., putting them all in one cage with the smell of a predator) rather than individually\nInfluential observations\n\n\nR\nPython\n\n\n\n\ncheck_model(glm_lev, check = 'outliers')\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\ncheck_outliers(glm_lev)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\n\n\n\ninfluence = glm_lev.get_influence()\n\ncooks = influence.cooks_distance[0]\n\nplt.stem(np.arange(len(cooks)), cooks, markerfmt=\",\")\nplt.axhline(0.5, color='red', linestyle='--', label='Threshold')\nplt.xlabel('Observation')\nplt.ylabel(\"Cook's Distance\")\nplt.title(\"Cook's Distance Plot\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\ninfluential_points = np.where(cooks &gt; 0.5)[0] # Set appropriate threshold here\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: []\n\n\n\n\n\nLooks good!\nCollinearity\n\n\nR\nPython\n\n\n\n\ncheck_collinearity(glm_lev)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n            Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n     stress_type 1.81 [1.40,   2.64]     1.34      0.55     [0.38, 0.72]\n             sex 2.23 [1.66,   3.28]     1.49      0.45     [0.30, 0.60]\n         rat_age 1.02 [1.00, 287.92]     1.01      0.98     [0.00, 1.00]\n stress_type:sex 2.70 [1.97,   4.01]     1.64      0.37     [0.25, 0.51]\n\ncheck_model(glm_lev, check = \"vif\")\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\n\n\n\nRemember to drop the response variables (and also rat ID in this case).\n\n# Drop the response variable\nX = levers.drop(columns=['rat_id','correct_presses','prop_correct','incorrect_presses'])\n\n# Create design matrix\nX = dmatrix(\"stress_type * sex + rat_age\", data=levers, return_type='dataframe')\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n                               feature        VIF\n0                            Intercept  48.157694\n1              stress_type[T.stressed]   1.768004\n2                          sex[T.male]   2.190167\n3  stress_type[T.stressed]:sex[T.male]   2.799690\n4                              rat_age   1.016442\n\n\n\n\n\nAgain, all good - none of our predictors have a concerning VIF.\nCollectively, it would seem that our model does indeed meet the assumptions, and there are no glaring obstacles to us proceeding with a significance test and an interpretation.\n\n\n\n\n\n\n\n\n\n\n11.5.2 Seed germination\n\n\n\n\n\n\nExercise 2 - Seed germination\n\n\n\n\n\n\nLevel: \nThis exercise uses a new dataset, seeds, which is all about a seed germination experiment.\nEach row of the dataset represents a seed tray, in which 25 seeds were planted. The trays were treated with one of two light conditions (sun, shade) and one of three watering frequencies (low, medium, high).\nThe researchers want to know whether either or both of these predictors have an affect on the proportion of seeds that successfully germinate.\nIn this exercise, you should:\n\nVisualise the data\nFit a suitable model\nTest the assumptions and quality of model fit\nDecide whether to draw a biological conclusion from the data\n\n\n\nR\nPython\n\n\n\n\nseeds &lt;- read_csv(\"data/seeds.csv\")\n\n\n\n\nseeds = pd.read_csv(\"data/seeds.csv\")\n\n\n\n\nThere is no formal worked answer provided. However, all of the code you will need can be adapted from the diabetes and aphids examples worked through in the chapter.\nYou are also strongly encouraged to work with, or share your answer with, a neighbour.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nConsider a possible interaction effect\nLook closely at the dataset itself, and all its columns\nYou should find at least two issues and/or failures of assumptions!\n\nIt may help to visualise the experimental design (the number of trays has been simplified so that the figure fits on the page):\n\n\nSeed germination experiment",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-11-checking-assumptions.html#summary",
    "href": "materials/glm-11-checking-assumptions.html#summary",
    "title": "\n11  Checking assumptions\n",
    "section": "\n11.6 Summary",
    "text": "11.6 Summary\nWhile generalised linear models make fewer assumptions than standard linear models, we do still expect certain things to be true about the model and our variables for GLMs to be valid.\n\n\n\n\n\n\nKey points\n\n\n\n\nFor a generalised linear model, we assume that we have chosen the correct link function, that our response variable follows a distribution from the exponential family, and that our data are independent\nWe also want to check that there are no overly influential points, no collinearity, and that the dispersion parameter is close to 1\nTo assess some of these assumptions/qualities, we have to rely on our understanding of our dataset\nFor others, we can calculate metrics like Cook’s distance and variance inflation factor, or produce diagnostic plots",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html",
    "href": "materials/glm-12-analysing-count-data.html",
    "title": "\n12  Analysing count data\n",
    "section": "",
    "text": "12.1 Context\nWe have now covered analysing binary responses and learned how to assess the quality and appropriateness of the resulting models. In the next sections we extend this further, by looking at a different type of response variable: count data. The humble count may look innocent, but often hides a whole lot of nuances that are hard to spot. So, pay attention!",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#libraries-and-functions",
    "href": "materials/glm-12-analysing-count-data.html#libraries-and-functions",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.2 Libraries and functions",
    "text": "12.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(performance)\nlibrary(tidyverse)\n\n\n\n\n# A maths library\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\n\n\n\n\n\n\n\nThe worked example in this chapter will use the islands dataset.\nThis is a dataset comprising 35 observations of two variables. For each small island in the dataset, the researcher recorded the number of unique species; they want to know if this can be predicted from the area (km2) of the island.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#load-and-visualise-the-data",
    "href": "materials/glm-12-analysing-count-data.html#load-and-visualise-the-data",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.3 Load and visualise the data",
    "text": "12.3 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nislands &lt;- read_csv(\"data/islands.csv\")\n\nhead(islands)\n\n# A tibble: 6 × 2\n  species  area\n    &lt;dbl&gt; &lt;dbl&gt;\n1     114  12.1\n2     130  13.4\n3     113  13.7\n4     109  14.5\n5     118  16.8\n6     136  19.0\n\n\n\n\n\nislands = pd.read_csv(\"data/islands.csv\")\n\nislands.head()\n\n   species       area\n0      114  12.076133\n1      130  13.405439\n2      113  13.723525\n3      109  14.540359\n4      118  16.792122\n\n\n\n\n\nWe can plot the data:\n\n\nR\nPython\n\n\n\n\nggplot(islands, aes(x = area, y = species)) +\n  geom_point()\n\n\n\n\n\n\nFigure 12.1: Scatterplot of area and species\n\n\n\n\n\n\n\np = (ggplot(islands, aes(x = \"area\", y = \"species\")) +\n   geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 12.2: Scatterplot of area and species\n\n\n\n\n\n\n\nEach dot on the scatterplot represents an island in the dataset.\nIt looks as though area definitely has some relationship with the number of species that we observe.\nNext step is to try to model these data.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#constructing-a-model",
    "href": "materials/glm-12-analysing-count-data.html#constructing-a-model",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.4 Constructing a model",
    "text": "12.4 Constructing a model\nThe species variable is the outcome or response (since we’re interested in whether it’s predicted by area).\nIt qualifies as a count variable. It’s bounded at 0 and \\(\\infty\\), and can only jump up in integers - in other words, you can’t have less than 0 species, or 6.3 species.\nThis means the best place to start is a Poisson regression. We fit these in a very similar way to a logistic regression, just with a different option specified for the family argument.\n\n\nR\nPython\n\n\n\n\nglm_isl &lt;- glm(species ~ area,\n               data = islands, family = \"poisson\")\n\nand we look at the model summary:\n\nsummary(glm_isl)\n\n\nCall:\nglm(formula = species ~ area, family = \"poisson\", data = islands)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 4.241129   0.041322  102.64   &lt;2e-16 ***\narea        0.035613   0.001247   28.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 856.899  on 34  degrees of freedom\nResidual deviance:  30.437  on 33  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the coefficients (the two numbers underneath Estimate in the table above):\n\ncoefficients(glm_isl)\n\n(Intercept)        area \n 4.24112904  0.03561346 \n\n\n\n\n\nmodel = smf.glm(formula = \"species ~ area\",\n                family = sm.families.Poisson(),\n                data = islands)\n\nglm_isl = model.fit()\n\nLet’s look at the model output:\n\nprint(glm_isl.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                species   No. Observations:                   35\nModel:                            GLM   Df Residuals:                       33\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -139.33\nDate:                Thu, 24 Jul 2025   Deviance:                       30.437\nTime:                        08:02:54   Pearson chi2:                     30.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.2411      0.041    102.636      0.000       4.160       4.322\narea           0.0356      0.001     28.551      0.000       0.033       0.038\n==============================================================================\n\n\nThe output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the coefficients (the two numbers underneath coef in the table above):\n\nprint(glm_isl.params)\n\nIntercept    4.241129\narea         0.035613\ndtype: float64\n\n\n\n\n\n\n12.4.1 The model equation\nNow that we have the beta coefficients, we can place them inside an equation.\nThe left-hand side of this equation is the expected number of species, \\(E(species)\\).\nOn the right-hand side, we take the linear equation \\(\\beta_0 + \\beta_1 * x_1\\) and embed it inside the inverse link function, which in this case is just the exponential function.\nIt looks like this:\n\\[ E(species) = \\exp(4.24 + 0.036 \\times area) \\]\nInterpreting this requires a bit of thought (not much, but a bit).\nThe intercept coefficient, 4.24, is related to the number of species we would expect on an island of zero area. But in order to turn this number into something meaningful we have to exponentiate it.\nSince \\(\\exp(4.24) \\approx 69\\), we can say that the baseline number of species the model expects on any island is 69.\nThe coefficient of area is the fun bit. For starters we can see that it is a positive number which does mean that increasing area leads to increasing numbers of species. Good so far.\nBut what does the value 0.036 actually mean? Well, if we exponentiate it as well, we get \\(\\exp(0.036) \\approx 1.04\\). This means that for every increase in area of 1 km2 (the original units of the area variable), the number of species on the island is multiplied by 1.04.\nSo, an island of area km2 will have \\(1.04 \\times 69 \\approx 72\\) species.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#plotting-the-poisson-regression",
    "href": "materials/glm-12-analysing-count-data.html#plotting-the-poisson-regression",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.5 Plotting the Poisson regression",
    "text": "12.5 Plotting the Poisson regression\n\n\nR\nPython\n\n\n\n\nggplot(islands, aes(area, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = \"poisson\")) +\n  xlim(10,50)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nFigure 12.3: Poisson regression on species ~ area\n\n\n\n\n\n\nFirst, we produce a set of predictions, which we can then plot.\n\nmodel = pd.DataFrame({'area': list(range(10, 50))})\n\nmodel[\"pred\"] = glm_isl.predict(model)\n\nmodel.head()\n\n   area        pred\n0    10   99.212463\n1    11  102.809432\n2    12  106.536811\n3    13  110.399326\n4    14  114.401877\n\n\n\np = (ggplot(islands, aes(x = \"area\", y = \"species\")) +\n   geom_point() +\n   geom_line(model, aes(x = \"area\", y = \"pred\"),\n                        colour = \"blue\", size = 1))\n\np.show()\n\n\n\n\n\n\nFigure 12.4: Poisson regression on species ~ area",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#assumptions-model-fit",
    "href": "materials/glm-12-analysing-count-data.html#assumptions-model-fit",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.6 Assumptions & model fit",
    "text": "12.6 Assumptions & model fit\nAs a reminder from last chapter, we want to consider the following things to assess whether we’ve fit the right model:\n\nThe distribution of the response variable\nThe link function\nIndependence\nInfluential points\n\n(We don’t need to worry about collinearity here - there’s only one predictor!)\nWe’re also going to talk about dispersion, which really comes into play when talking about count data - we’ll get to that after we’ve run through the stuff that’s familiar.\n\n\nR\nPython\n\n\n\nWith the generic check_model function, we can visually assess a few things quite quickly.\n\ncheck_model(glm_isl)\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\nFigure 12.5: Diagnostic plots for glm_isl\n\n\n\n\nThe posterior predictive check looks alright. The blue simulations seem to be following a pretty similar distribution to the actual data in green.\nThe leverage/Cook’s distance plot also isn’t identifying any data points that we need to be concerned about. We can follow up on that:\n\ncheck_outliers(glm_isl, threshold = list('cook'=0.5))\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\n\n\nWe can check for influential points:\n\ninfluence = glm_isl.get_influence()\n\ncooks = influence.cooks_distance[0]\ninfluential_points = np.where(cooks &gt; 0.5)[0]\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: []\n\n\n\n\n\nInfluential points: There’s no evidence of any points with high Cook’s distances, so life is rosy on that front.\nIndependence: From the description of the dataset, it sounds plausible that each island is independent. Of course, if we later found out that some of the islands were clustered together into a bunch of archipelagos, that would definitely be cause for concern.\nDistribution & link function: Again, from the description of the species variable, we can be confident that this really is a count variable. Whether or not Poisson regression with the log link function is correct, however, will depend on what happens when we look at the dispersion parameter.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#sec-mat_dispersion",
    "href": "materials/glm-12-analysing-count-data.html#sec-mat_dispersion",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.7 Dispersion",
    "text": "12.7 Dispersion\n\n12.7.1 What is dispersion?\nDispersion, in statistics, is a general term to describe the variability, scatter, or spread of a distribution. Variance is actually a type of dispersion.\nIn a normal distribution, the mean (average/central tendency) and the variance (dispersion) are independent of each other; we need both numbers, or parameters, to understand the shape of the distribution.\nOther distributions, however, require different parameters to describe them in full. For a Poisson distribution - which we’ll learn more about when we talk about count data - we need just one parameter (\\(\\lambda\\)) to describe the distribution, because the mean and variance are assumed to be the same.\nIn the context of a model, you can think about the dispersion as the degree to which the data are spread out around the model curve. A dispersion parameter of 1 means the data are spread out exactly as we expect; &lt;1 is called underdispersion; and &gt;1 is called overdispersion.\n\n12.7.2 A “hidden assumption”\nWhen performing Poisson regression, we make an extra assumption: that the dispersion parameter to 1. This means we don’t have to waste time and statistical power in estimating the dispersion.\nHowever, if our data are underdispersed or overdispersed, then we might be violating this assumption we’ve made.\nUnderdispersion is quite rare. It’s far more likely that you’ll encounter overdispersion.\nIn these situations, you may wish to fit a different GLM to the data. Negative binomial regression, for instance, is a common alternative for count data.\n\n12.7.3 Assessing dispersion\nThe easiest way to check dispersion in a model is to calculate the ratio of the residual deviance to the residual degrees of freedom.\nIf we take a look at the model output, we can see the two quantities we care about - residual deviance and residual degrees of freedom:\n\n\nR\nPython\n\n\n\n\nsummary(glm_isl)\n\n\nCall:\nglm(formula = species ~ area, family = \"poisson\", data = islands)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 4.241129   0.041322  102.64   &lt;2e-16 ***\narea        0.035613   0.001247   28.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 856.899  on 34  degrees of freedom\nResidual deviance:  30.437  on 33  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n\n\nprint(glm_isl.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                species   No. Observations:                   35\nModel:                            GLM   Df Residuals:                       33\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -139.33\nDate:                Thu, 24 Jul 2025   Deviance:                       30.437\nTime:                        08:02:55   Pearson chi2:                     30.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.2411      0.041    102.636      0.000       4.160       4.322\narea           0.0356      0.001     28.551      0.000       0.033       0.038\n==============================================================================\n\n\n\n\n\nThe residual deviance is 30.44, on 33 residual degrees of freedom. All we need to do is divide one by the other to get our dispersion parameter.\n\n\nR\nPython\n\n\n\n\nglm_isl$deviance/glm_isl$df.residual\n\n[1] 0.922334\n\n\n\n\n\nprint(glm_isl.deviance/glm_isl.df_resid)\n\n0.9223340414458482\n\n\n\n\n\nThe dispersion parameter here is 0.9223. That’s pretty good - not far off 1 at all.\nBut how can we check whether it is significantly different from 1?\n\n\nR\nPython\n\n\n\nOnce again, the performance package comes in very helpful here.\nThe check_overdispersion function will give us both the dispersion parameter and a p-value (which is based on a chi-square test - like the goodness-of-fit ones you were running last chapter).\n\ncheck_overdispersion(glm_isl)\n\n# Overdispersion test\n\n       dispersion ratio =  0.919\n  Pearson's Chi-Squared = 30.339\n                p-value =    0.6\n\n\nNo overdispersion detected.\n\n\nHere, it confirms our suspicions; the dispersion parameter is both descriptively close to 1, and also not significantly different from 1.\nYou’ll notice that this function does give a slightly different value for the dispersion parameter, compared to when we calculated it manually above.\nThis is because it actually uses the sum of squared Pearson residuals instead of the residual deviance (a distinction which really isn’t worth worrying about). Broadly, the conclusion should be the same, so the difference doesn’t matter very much.\n\n\nWell, you’ve actually already got the knowledge you need to do this, from the previous chapter on significance testing.\nSpecifically, the chi-squared goodness-of-fit test can be used to check whether the dispersion is within sensible limits.\n\npvalue = chi2.sf(glm_isl.deviance, glm_isl.df_resid)\n\nprint(pvalue)\n\n0.5953470127463268\n\n\nIf our chi-squared goodness-of-fit test returns a large (insignificant) p-value, as it does here, that tells us that we don’t need to worry about the dispersion.\nIf our chi-squared goodness-of-fit test returned a small, significant p-value, this would tell us our model doesn’t fit the data well. And, since dispersion is all about the spread of points around the model, it makes sense that these two things are so closely related!\n\n\n\nGreat - we can proceed with the Poisson regression, since we don’t seem to have overdispersed or underdispersed data.\nNext chapter we’ll look at what we would have done next, if we had run into problems with dispersion.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#assessing-significance",
    "href": "materials/glm-12-analysing-count-data.html#assessing-significance",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.8 Assessing significance",
    "text": "12.8 Assessing significance\nBy testing the dispersion with a chi-square test, we have already essentially checked the goodness-of-fit of this model - it’s good.\nThis leaves us with two things to check:\n\nIs the overall model better than the null model?\nAre any of the individual predictors significant?\n\n\n12.8.1 Comparing against the null\nWe need to fit the null model, and then we can extract the null deviance and degrees of freedom to compare against our model.\n\n\nR\nPython\n\n\n\n\nglm_isl_null &lt;- glm(species ~ 1,\n                    data = islands, family = \"poisson\")\n\nanova(glm_isl, glm_isl_null)\n\nAnalysis of Deviance Table\n\nModel 1: species ~ area\nModel 2: species ~ 1\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        33      30.44                          \n2        34     856.90 -1  -826.46 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince there’s only one predictor, we actually could achieve the same effect here by just computing an analysis of deviance table:\n\nanova(glm_isl, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: species\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                    34     856.90              \narea  1   826.46        33      30.44 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nFirst, we fit a null model:\n\nmodel = smf.glm(formula = \"species ~ 1\",\n                family = sm.families.Poisson(),\n                data = islands)\n                \nglm_isl_null = model.fit()\n\nglm_isl_null.df_resid\n\nnp.int64(34)\n\n\nAnd now we can compare the two:\n\n# Calculate the likelihood ratio (i.e. the chi-square value)\nlrstat = -2*(glm_isl_null.llf - glm_isl.llf)\n\n# Calculate the associated p-value\npvalue = chi2.sf(lrstat, glm_isl_null.df_resid - glm_isl.df_resid)\n\nprint(lrstat, pvalue)\n\n826.4623291533155 9.523357725017566e-182\n\n\n\n\n\nThis gives a reported p-value extremely close to zero, which is rather small.\nTherefore, this model is significant over the null, and species does appear to be predicted by area.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#exercises",
    "href": "materials/glm-12-analysing-count-data.html#exercises",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.9 Exercises",
    "text": "12.9 Exercises\n\n12.9.1 Seat belts\n\n\n\n\n\n\nExercise 1 - Seat belts\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/seatbelts.csv.\nThe data tracks the number of drivers killed in road traffic accidents, before and after the seat belt law was introduced. The information on whether the law was in place is encoded in the law column as 0 (law not in place) or 1 (law in place).\nThe year variable is our predictor of interest.\nIn this exercise, you should:\n\nVisualise the data\nCreate a poisson regression model and extract its equation\nPlot the regression model on top of the data\nAssess if the model is a decent predictor for the number of fatalities (& check assumptions)\n\n\n\nR\nPython\n\n\n\n\nseatbelts &lt;- read_csv(\"data/seatbelts.csv\")\n\nhead(seatbelts)\n\n# A tibble: 6 × 10\n  casualties drivers front  rear   kms petrol_price van_killed   law  year month\n       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1        107    1687   867   269  9059        0.103         12     0  1969 Jan  \n2         97    1508   825   265  7685        0.102          6     0  1969 Feb  \n3        102    1507   806   319  9963        0.102         12     0  1969 Mar  \n4         87    1385   814   407 10955        0.101          8     0  1969 Apr  \n5        119    1632   991   454 11823        0.101         10     0  1969 May  \n6        106    1511   945   427 12391        0.101         13     0  1969 Jun  \n\n\n\n\n\nseatbelts = pd.read_csv(\"data/seatbelts.csv\")\n\nseatbelts.head()\n\n   casualties  drivers  front  rear  ...  van_killed  law  year  month\n0         107     1687    867   269  ...          12    0  1969    Jan\n1          97     1508    825   265  ...           6    0  1969    Feb\n2         102     1507    806   319  ...          12    0  1969    Mar\n3          87     1385    814   407  ...           8    0  1969    Apr\n4         119     1632    991   454  ...          10    0  1969    May\n\n[5 rows x 10 columns]\n\n\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nVisualise the data\nFirst we have a look at the data comparing no law versus law:\n\n\nR\nPython\n\n\n\nWe have to convert the law column to a factor, otherwise R will see it as numerical.\n\nseatbelts |&gt; \n  ggplot(aes(as_factor(law), casualties)) +\n   geom_boxplot() +\n   geom_jitter(width = 0.2)\n\n\n\n\n\n\nFigure 12.6: Boxplot of driver casualties before and after seatbelt introduction\n\n\n\n\nThe data are recorded by month and year, so we can also display the number of drivers killed by year:\n\nseatbelts |&gt; \n  ggplot(aes(year, casualties)) +\n  geom_point()\n\n\n\n\n\n\nFigure 12.7: Scatterplot of driver casualties across years\n\n\n\n\n\n\nWe have to convert the law column to a factor, otherwise R will see it as numerical.\n\np = (ggplot(seatbelts, aes(x = seatbelts.law.astype(object),\n                       y = \"casualties\")) +\n   geom_boxplot() +\n   geom_jitter(width = 0.2))\n\np.show()\n\n\n\n\n\n\nFigure 12.8: Boxplot of driver casualties before and after seatbelt introduction\n\n\n\n\nThe data are recorded by month and year, so we can also display the number of casualties by year:\n\np = (ggplot(seatbelts,\n         aes(x = \"year\",\n             y = \"casualties\")) +\n     geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 12.9: Scatterplot of driver casualties across years\n\n\n\n\n\n\n\nThe data look a bit weird. There’s a bit of a wavy pattern across years. And although it looks like fatalities are lower after the law was implemented, there are many more observations when the law was not in place, which is going to make the data harder to interpret.\nConstructing a model\n\n\nR\nPython\n\n\n\n\nglm_stb &lt;- glm(casualties ~ year,\n               data = seatbelts, family = \"poisson\")\n\n\n\n\nmodel = smf.glm(formula = \"casualties ~ year\",\n                family = sm.families.Poisson(),\n                data = seatbelts)\n\nglm_stb = model.fit()\n\n\n\n\nModel equation\n\n\nR\nPython\n\n\n\n\ncoefficients(glm_stb)\n\n(Intercept)        year \n  37.168958   -0.016373 \n\n\n\n\n\nprint(glm_stb.params)\n\nIntercept    37.168958\nyear         -0.016373\ndtype: float64\n\n\n\n\n\nThe coefficients of the Poisson model equation should be placed in the following formula, in order to estimate the expected number of species as a function of island size:\n\\[ E(casualties) = \\exp(37.17 - -0.016 \\times year) \\]\nVisualise model\nWe can just adapt the code used in the worked example earlier in the chapter:\n\n\nR\nPython\n\n\n\n\nggplot(seatbelts, aes(year, casualties)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = poisson)) +\n  xlim(1970, 1985)\n\n\n\n\n\n\nFigure 12.10: Poisson regression of driver casualties across years\n\n\n\n\n\n\n\nmodel = pd.DataFrame({'year': list(range(1968, 1985))})\n\nmodel[\"pred\"] = glm_stb.predict(model)\n\nmodel.head()\n\n   year        pred\n0  1968  140.737690\n1  1969  138.452153\n2  1970  136.203733\n3  1971  133.991827\n4  1972  131.815842\n\n\n\np = (ggplot(seatbelts, aes(x = \"year\",\n                       y = \"casualties\")) +\n   geom_point() +\n   geom_line(model, aes(x = \"year\", y = \"pred\"),\n                        colour = \"blue\", size = 1))\n\np.show()\n\n\n\n\n\n\nFigure 12.11: Poisson regression of driver casualties across years\n\n\n\n\n\n\n\nAssessing model quality & significance\nAre the assumptions met?\n\n\nResponse distribution & link function\nYes, based on the info given\n\n\nIndependence\nYes, based on the info given\n\n\nInfluential points\nCheck with Cook’s distance\n\n\nDispersion\nCheck dispersion parameter\n\n\n\n\nR\nPython\n\n\n\nWe can assess outliers visually or by printing a list of high leverage points:\n\ncheck_outliers(glm_stb, threshold = list('cook'=0.5))\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\ncheck_model(glm_stb, check = 'outliers')\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\n\nIt seems we’re okay on the outlier front.\nFor dispersion, again we have some options of how to test it:\n\ncheck_overdispersion(glm_stb)\n\n# Overdispersion test\n\n       dispersion ratio =   4.535\n  Pearson's Chi-Squared = 861.667\n                p-value = &lt; 0.001\n\n\nOverdispersion detected.\n\ncheck_model(glm_stb, check = 'overdispersion')\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\n\nDispersion is definitely a problem here.\nFor completeness, we can also peek at the posterior predictive check:\n\ncheck_model(glm_stb, check = 'pp_check')\n\nCannot simulate residuals for models of class `glm`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\n\nIt doesn’t seem to be doing the best job. Around 0 and in the 180+ range of the x-axis, it’s under-predicting the counts; and it might be over-predicting a bit in the middle (120-140 ish).\n\n\nWe can assess outliers by printing a list of high leverage points:\n\ninfluence = glm_isl.get_influence()\n\ncooks = influence.cooks_distance[0]\ninfluential_points = np.where(cooks &gt; 0.5)[0]\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: []\n\n\nIt seems we’re okay on the outlier front.\nFor dispersion, let’s calculate the parameter and p-value:\n\npvalue = chi2.sf(glm_stb.deviance, glm_stb.df_resid)\n\nprint(glm_stb.deviance/glm_stb.df_resid, pvalue)\n\n4.475851158209689 3.1298697149299994e-84\n\n\nDispersion is definitely a problem here.\n\n\n\n\n\n\n\n\n\n\nResponse distribution & link function\nYes, based on the info given\n\n\nIndependence\nYes, based on the info given\n\n\nInfluential points\nYes, no data points identified\n\n\nDispersion\nClear overdispersion\n\n\n\nGiven that we have a problem with overdispersion, we probably wouldn’t proceed with significance testing here under normal circumstances. Instead, we’d want to fit a different GLM that can handle this overdispersion.\nFor completeness, however, since these are course materials rather than normal circumstances - let’s check: is the model significant over the null?\n\n\nR\nPython\n\n\n\n\nglm_null &lt;- glm(casualties ~ 1, \n                family = \"poisson\", \n                data = seatbelts)\n\nanova(glm_stb, glm_null)\n\nAnalysis of Deviance Table\n\nModel 1: casualties ~ year\nModel 2: casualties ~ 1\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       190     850.41                          \n2       191     984.50 -1  -134.08 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmodel = smf.glm(formula = \"casualties ~ 1\",\n                family = sm.families.Poisson(),\n                data = seatbelts)\n\nglm_stb_null = model.fit()\n\n\nlrstat = -2*(glm_stb_null.llf - glm_stb.llf)\npvalue = chi2.sf(lrstat, glm_stb_null.df_resid - glm_stb.df_resid)\n\nprint(lrstat, pvalue)\n\n134.0834016068129 5.23879166948147e-31\n\n\n\n\n\nWhat does the significant p-value here tell us?\nWell, given the difference in degrees of freedom (1), the change in residual deviance between our model and the null model is unexpectedly large. So the model is doing something over and above what the null is doing.\nConclusions\nDoes this significant comparison against the null allow us to draw any conclusions about whether the seatbelt law affects the number of casualties?\nNo. Don’t let it tempt you.\nThe model we’ve constructed here is not a good fit and doesn’t meet all of the assumptions. We need to fit a different type of model that can cope with the overdispersion, which we’ll look into next chapter.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-12-analysing-count-data.html#summary",
    "href": "materials/glm-12-analysing-count-data.html#summary",
    "title": "\n12  Analysing count data\n",
    "section": "\n12.10 Summary",
    "text": "12.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nCount data are bounded between 0 and \\(\\infty\\), with integer increases\nThis type of data can be modelled with a Poisson regression, using a log link function\nPoisson regression makes all of the same assumptions as logistic regression, plus an assumption about dispersion",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html",
    "href": "materials/glm-13-overdispersed-count-data.html",
    "title": "\n13  Overdispersed count data\n",
    "section": "",
    "text": "13.1 Context\nIn the previous chapter we looked at how to analyse count data. We used a Poisson regression to do this, which makes assumptions about the dispersion parameter. If everything is as expected (the mean and variance are they same) it’s 1. But, data are rarely that compliant so we need to be able to deal with situations where this is not the case. Here, we look at the situation where we have overdispersion.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#libraries-and-functions",
    "href": "materials/glm-13-overdispersed-count-data.html#libraries-and-functions",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.2 Libraries and functions",
    "text": "13.2 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n13.2.1 Libraries\n\nlibrary(MASS)\nlibrary(performance)\nlibrary(tidyverse)\nlibrary(broom)\n\n\n\n\n\n13.2.2 Libraries\n\n# A maths library\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\nimport numpy as np\nfrom statsmodels.discrete.discrete_model import NegativeBinomial",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#removing-dispersion-assumptions",
    "href": "materials/glm-13-overdispersed-count-data.html#removing-dispersion-assumptions",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.3 Removing dispersion assumptions",
    "text": "13.3 Removing dispersion assumptions\nWhen we analysed the count data in the previous chapter, we used a Poisson regression. This makes the assumption that the dispersion parameter \\(\\approx 1\\) (for a refresher on dispersion, see Section 12.7).\nHere we look at a situation where that assumption is violated and the dispersion parameter is much larger than 1. For this we turn to negative binomial models. Instead of making an assumption about the dispersion parameter, they estimate it as part of the model fitting.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#parasites-dataset",
    "href": "materials/glm-13-overdispersed-count-data.html#parasites-dataset",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.4 Parasites dataset",
    "text": "13.4 Parasites dataset\nWe’ll explore this with the parasites dataset.\nThese data are about parasites found on the gills of freshwater fish.\nThe ecologists who conducted the research observed a total of 64 fish, which varied by:\n\nwhich lake they were found in\nthe fish_length (cm)\n\nThey were interested in how these factors influenced the parasite_count of small parasites found on each fish.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#load-and-visualise-the-data",
    "href": "materials/glm-13-overdispersed-count-data.html#load-and-visualise-the-data",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.5 Load and visualise the data",
    "text": "13.5 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nparasites &lt;- read_csv(\"data/parasites.csv\")\n\nhead(parasites)\n\n# A tibble: 6 × 3\n  parasite_count lake  fish_length\n           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1             46 C            21.1\n2            138 C            26.4\n3             23 C            18.9\n4             35 B            27.2\n5            118 C            29  \n6             43 B            24.2\n\n\n\nggplot(parasites, aes(x = fish_length, y = parasite_count, colour = lake)) +\n  geom_point()\n\n\n\n\n\n\nFigure 13.1: Scatterplot of parasite count against fishlength\n\n\n\n\n\nggplot(parasites, aes(x = lake, y = parasite_count, colour = fish_length)) +\n  geom_violin() +\n  geom_jitter(width = 0.2)\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\nFigure 13.2: Violin plot of parasite count against fishlength\n\n\n\n\n\n\n\nparasites = pd.read_csv(\"data/parasites.csv\")\n\nparasites.head()\n\n   parasite_count lake  fish_length\n0              46    C         21.1\n1             138    C         26.4\n2              23    C         18.9\n3              35    B         27.2\n4             118    C         29.0\n\n\n\np = (ggplot(parasites, aes(x = \"fish_length\", y = \"parasite_count\",\n                           colour = \"lake\")) +\n       geom_point())\n\np.show()\n\n\n\n\n\n\nFigure 13.3: Scatterplot of parasite count against fishlength\n\n\n\n\n\np = (ggplot(parasites, aes(x = \"lake\", y = \"parasite_count\",\n                       colour = \"fish_length\")) +\n   geom_violin() +\n   geom_jitter(width = 0.2))\n\np.show()\n\n\n\n\n\n\nFigure 13.4: Violin plot of parasite count against fishlength\n\n\n\n\n\n\n\nWe get a reasonably clear picture here; it seems that lake C might have a higher number of parasites overall, and that across all three lakes, bigger fish have more parasites too.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#constructing-a-poisson-model",
    "href": "materials/glm-13-overdispersed-count-data.html#constructing-a-poisson-model",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.6 Constructing a Poisson model",
    "text": "13.6 Constructing a Poisson model\nGiven that the response variable, parasite_count, is a count variable, let’s try to construct a Poisson regression.\nWe’ll construct the full model, with an interaction.\n\n\nR\nPython\n\n\n\n\nglm_para &lt;- glm(parasite_count ~ lake * fish_length,\n                data = parasites, family = \"poisson\")\n\nand we look at the model summary:\n\nsummary(glm_para)\n\n\nCall:\nglm(formula = parasite_count ~ lake * fish_length, family = \"poisson\", \n    data = parasites)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.656e+00  1.763e-01   9.391   &lt;2e-16 ***\nlakeB             -7.854e-01  2.804e-01  -2.801   0.0051 ** \nlakeC              3.527e-01  2.261e-01   1.560   0.1188    \nfish_length        9.127e-02  6.643e-03  13.738   &lt;2e-16 ***\nlakeB:fish_length  1.523e-02  1.031e-02   1.477   0.1398    \nlakeC:fish_length -5.079e-05  8.389e-03  -0.006   0.9952    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2073.8  on 63  degrees of freedom\nResidual deviance: 1056.5  on 58  degrees of freedom\nAIC: 1429.2\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nmodel = smf.glm(formula = \"parasite_count ~ lake * fish_length\",\n                family = sm.families.Poisson(),\n                data = parasites)\n\nglm_para = model.fit()\n\nLet’s look at the model output:\n\nprint(glm_para.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:         parasite_count   No. Observations:                   64\nModel:                            GLM   Df Residuals:                       58\nModel Family:                 Poisson   Df Model:                            5\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -708.61\nDate:                Thu, 24 Jul 2025   Deviance:                       1056.5\nTime:                        08:08:32   Pearson chi2:                 1.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n=========================================================================================\n                            coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------\nIntercept                 1.6556      0.176      9.391      0.000       1.310       2.001\nlake[T.B]                -0.7854      0.280     -2.801      0.005      -1.335      -0.236\nlake[T.C]                 0.3527      0.226      1.560      0.119      -0.090       0.796\nfish_length               0.0913      0.007     13.738      0.000       0.078       0.104\nlake[T.B]:fish_length     0.0152      0.010      1.477      0.140      -0.005       0.035\nlake[T.C]:fish_length -5.079e-05      0.008     -0.006      0.995      -0.016       0.016\n=========================================================================================\n\n\n\n\n\n\n13.6.1 Checking dispersion\nBefore we launch into any significance testing, we’re going to check one of the assumptions: that the dispersion parameter = 1 (you might’ve guessed where this is going…)\n\n\nR\nPython\n\n\n\n\ncheck_overdispersion(glm_para)\n\n# Overdispersion test\n\n       dispersion ratio =   18.378\n  Pearson's Chi-Squared = 1065.905\n                p-value =  &lt; 0.001\n\n\nOverdispersion detected.\n\n\n\n\n\nprint(glm_para.deviance/glm_para.df_resid)\n\n18.215417572845308\n\npvalue = chi2.sf(glm_para.deviance, glm_para.df_resid)\nprint(pvalue)\n\n2.3125874815114225e-183\n\n\n\n\n\nOh no - there is definitely overdispersion here.\nWe won’t bother looking at the analysis of deviance table or asking whether the model is better than the null model. The Poisson regression we’ve fitted here is not right, and we need to fit a different type of model.\nThe main alternative to a Poisson model, used for overdispersed count data, is something called a negative binomial model.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#negative-binomial-model",
    "href": "materials/glm-13-overdispersed-count-data.html#negative-binomial-model",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.7 Negative binomial model",
    "text": "13.7 Negative binomial model\n\n\nR\nPython\n\n\n\nTo specify a negative binomial model, we use the glm.nb function from the MASS package.\nThe syntax is the same as glm, except we don’t need to specify a family argument.\n\nnb_para &lt;- glm.nb(parasite_count ~ lake * fish_length, parasites)\n\nsummary(nb_para)\n\n\nCall:\nglm.nb(formula = parasite_count ~ lake * fish_length, data = parasites, \n    init.theta = 3.37859216, link = log)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)        1.943370   0.739434   2.628  0.00858 **\nlakeB             -0.784885   1.094625  -0.717  0.47335   \nlakeC              0.212060   1.006021   0.211  0.83305   \nfish_length        0.079939   0.029485   2.711  0.00671 **\nlakeB:fish_length  0.015428   0.043380   0.356  0.72210   \nlakeC:fish_length  0.005719   0.039542   0.145  0.88501   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.3786) family taken to be 1)\n\n    Null deviance: 118.202  on 63  degrees of freedom\nResidual deviance:  67.288  on 58  degrees of freedom\nAIC: 615.99\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.379 \n          Std. Err.:  0.623 \n\n 2 x log-likelihood:  -601.991 \n\n\nThis output is very similar to the other GLM outputs that we’ve seen but with some additional information at the bottom regarding the dispersion parameter that the negative binomial model has used, which in R is called theta (\\(\\theta\\)), 3.379.\nThis number is estimated from the data. This is what makes negative binomial regression different from Poisson regression.\n\n\nWe can continue to use statsmodels, specifically the NegativeBinomial function.\nThe syntax for getting this to work is a little different, and takes several steps.\nFirst, we identify our response variable:\n\nY = parasites['parasite_count']\n\nNow, we need to set up our set of predictors. The function we’re using requires us to provide a matrix where each row is a unique observation (fish, in this case) and each column is a predictor.\nBecause we want to include the lake:fish_length interaction, we actually need to manually generate those columns ourselves, like this:\n\n# 1. Create dummy variables for 'lake' (switch to True/False values)\nlake_dummies = pd.get_dummies(parasites['lake'], drop_first=True)\n\n# 2. Create interaction terms manually: lake_dummies * fish_length\ninteractions = lake_dummies.multiply(parasites['fish_length'], axis=0)\ninteractions.columns = [f'{col}:fish_length' for col in interactions.columns]\n\n# 3. Combine all predictors: fish_length, lake dummies, interactions\nX = pd.concat([parasites['fish_length'], lake_dummies, interactions], axis=1)\n\n# 4. Add a constant/intercept (required), and make sure that we're using floats\nX = sm.add_constant(X).astype(float)\n\nNow, we can fit our model and look at the summary:\n\nfrom statsmodels.discrete.discrete_model import NegativeBinomial\n\n# Specify the model (Y, X)\nmodel = NegativeBinomial(Y, X)\n\nnb_para = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 4.703057\n         Iterations: 32\n         Function evaluations: 39\n         Gradient evaluations: 39\n\nprint(nb_para.summary())\n\n                     NegativeBinomial Regression Results                      \n==============================================================================\nDep. Variable:         parasite_count   No. Observations:                   64\nModel:               NegativeBinomial   Df Residuals:                       58\nMethod:                           MLE   Df Model:                            5\nDate:                Thu, 24 Jul 2025   Pseudo R-squ.:                 0.05947\nTime:                        08:08:33   Log-Likelihood:                -301.00\nconverged:                       True   LL-Null:                       -320.03\nCovariance Type:            nonrobust   LLR p-value:                 3.658e-07\n=================================================================================\n                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nconst             1.9434      0.689      2.819      0.005       0.592       3.294\nfish_length       0.0799      0.027      2.915      0.004       0.026       0.134\nB                -0.7849      1.026     -0.765      0.444      -2.796       1.226\nC                 0.2121      0.961      0.221      0.825      -1.671       2.095\nB:fish_length     0.0154      0.041      0.380      0.704      -0.064       0.095\nC:fish_length     0.0057      0.038      0.152      0.879      -0.068       0.080\nalpha             0.2960      0.055      5.424      0.000       0.189       0.403\n=================================================================================\n\n\nIn particular, we might like to know what the dispersion parameter is.\nBy default, NegativeBinomial from statsmodels provides something called alpha (\\(\\alpha\\)), which is not the same as the significance threshold. To convert this into the dispersion parameter you’re using to seeing (sometimes referred to as theta, \\(\\theta\\)), you need \\(\\frac{1}{\\alpha}\\).\n\nprint(1/nb_para.params['alpha'])\n\n3.378591992548569\n\n\n\n\n\n\n13.7.1 Extract equation\nIf we wanted to express our model as a formal equation, we need to extract the coefficients:\n\n\nR\nPython\n\n\n\n\ncoefficients(nb_para)\n\n      (Intercept)             lakeB             lakeC       fish_length \n      1.943370195      -0.784885086       0.212060149       0.079939234 \nlakeB:fish_length lakeC:fish_length \n      0.015428470       0.005718571 \n\n\n\n\n\nprint(nb_para.params)\n\nconst            1.943370\nfish_length      0.079939\nB               -0.784885\nC                0.212060\nB:fish_length    0.015428\nC:fish_length    0.005719\nalpha            0.295981\ndtype: float64\n\n\n\n\n\nThese are the coefficients of the negative binomial model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of the other variables:\n\\[\n\\begin{split}\nE(species) = \\exp(1.94 + \\begin{pmatrix} 0 \\\\ -0.78 \\\\ 0.212 \\end{pmatrix} \\begin{pmatrix} A \\\\ B \\\\ C \\end{pmatrix} + 0.08 \\times fishlength + \\begin{pmatrix} 0 \\\\ 0.0154 \\\\ 0.0057 \\end{pmatrix} \\begin{pmatrix} A \\\\ B \\\\ C \\end{pmatrix} \\times fishlength)\n\\end{split}\n\\]\n\n13.7.2 Comparing Poisson and negative binomial\nA Poisson regression and negative binomial regression are very similar to each other. They use the same link function - the log link function. There is however a key difference.\n\n\n\n\n\n\nDifference Poisson and negative binomial\n\n\n\nIn a Poisson regression the dispersion parameter \\(\\lambda\\) = 1. In a negative binomial regression it is estimated from the data.\n\n\nThis means they will both have equations in the same form, as we just showed above. It also means that they can produce some quite similar-looking models.\nLet’s visualise and compare them.\nFirst, let’s plot the Poisson model:\n\n\nR\nPython\n\n\n\n\nggplot(parasites, aes(x = fish_length, y = parasite_count, colour = lake)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = \"poisson\"))\n\n\n\n\n\n\nFigure 13.5: Poisson model for parasite count against fishlength, by lake\n\n\n\n\n\n\nTo do this, we need to produce a set of predictions, which we can then plot.\n\n# Get unique lake values from your dataset\nlake_levels = parasites['lake'].unique()\n\n# Create prediction grid for each lake\nnew_data = pd.concat([\n    pd.DataFrame({\n        'fish_length': np.linspace(10, 40, 100),\n        'lake': lake\n    }) for lake in lake_levels\n])\n\n# Predict\nnew_data['pred'] = glm_para.predict(new_data)\n\nnew_data.head()\n\n   fish_length lake       pred\n0    10.000000    C  18.549643\n1    10.303030    C  19.069529\n2    10.606061    C  19.603986\n3    10.909091    C  20.153422\n4    11.212121    C  20.718257\n\n\n\np = (ggplot(parasites, aes(x = \"fish_length\",\n                           y = \"parasite_count\",\n                           colour = \"lake\")) +\n   geom_point() +\n   geom_line(new_data, aes(x = \"fish_length\", y = \"pred\",\n                           colour = \"lake\"), size = 1))\n\np.show()\n\n\n\n\n\n\nFigure 13.6: Poisson model for parasite count against fishlength, by lake\n\n\n\n\n\n\n\nNow, let’s plot the negative binomial model:\n\n\nR\nPython\n\n\n\nAll we have to do is switch the method for geom_smooth over to glm.nb:\n\nggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE)\n\n\n\n\n\n\nFigure 13.7: Negative binomial model for parasite count against fishlength, by lake\n\n\n\n\n\n\n\n# Get unique lake values from your dataset\nlake_levels = parasites['lake'].unique()\n\n# Create prediction grid for each lake\nnew_data_nb = pd.concat([\n    pd.DataFrame({\n        'fish_length': np.linspace(10, 40, 100),\n        'lake': lake\n    }) for lake in lake_levels\n])\n\n# Repeat the transformations we used when fitting the model\nlake_dummies = pd.get_dummies(new_data['lake'], drop_first=True)\ninteractions = lake_dummies.multiply(new_data_nb['fish_length'], axis=0)\ninteractions.columns = [f'{col}:fish_length' for col in interactions.columns]\nX_new = pd.concat([new_data_nb['fish_length'], lake_dummies, interactions], axis=1)\nX_new = sm.add_constant(X_new).astype(float)\n\n# Now we can safely predict:\nnew_data_nb['pred'] = nb_para.predict(X_new)\n\nnew_data_nb.head()\n\n   fish_length lake       pred\n0    10.000000    C  20.328185\n1    10.303030    C  20.862750\n2    10.606061    C  21.411372\n3    10.909091    C  21.974421\n4    11.212121    C  22.552276\n\n\n\np = (ggplot(parasites, aes(x = \"fish_length\",\n                           y = \"parasite_count\",\n                           colour = \"lake\")) +\n   geom_point() +\n   geom_line(new_data_nb, aes(x = \"fish_length\", y = \"pred\",\n                              colour = \"lake\"), size = 1))\n\np.show()\n\n\n\n\n\n\nFigure 13.8: Negative binomial model for parasite count against fishlength, by lake\n\n\n\n\n\n\n\nIt’s subtle - the two sets of curves look quite similar - but they’re not quite the same.\nLooking at them side-by-side may help:\n\n\n\n\n\n\n\nFigure 13.9: Comparison between Poisson and Negative binomial models\n\n\n\n\n\n13.7.3 Checking assumptions & model quality\nThese steps are performed in precisely the same way we would do for a Poisson regression.\nRemember, the things we checked for in a Poisson regression were:\n\nResponse distribution\nCorrect link function\nIndependence\nInfluential points\nCollinearity\nDispersion\n\nFor free, I’ll just tell you now that the first three assumptions are met.\nPlus, we don’t have to worry about dispersion with a negative binomial model.\nSo, all we really need to pay attention to is whether there are any influential points to worry about, or any collinearity.\n\n\nR\nPython\n\n\n\nOnce again, the performance package comes in handy for assessing all of these.\n\ncheck_model(nb_para, check = c('pp_check',\n                               'vif',\n                               'outliers'))\n\nCannot simulate residuals for models of class `negbin`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\ncheck_outliers(nb_para, threshold = list('cook'=0.5))\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\ncheck_collinearity(nb_para)\n\nModel has interaction terms. VIFs might be inflated.\n  Try to center the variables used for the interaction, or check\n  multicollinearity among predictors of a model without interaction terms.\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n        Term  VIF        VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n fish_length 3.13 [  2.26,    4.61]     1.77      0.32     [0.22, 0.44]\n\nHigh Correlation\n\n             Term     VIF        VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n             lake 1321.85 [863.04, 2024.86]    36.36  7.57e-04     [0.00, 0.00]\n lake:fish_length 1430.57 [934.01, 2191.42]    37.82  6.99e-04     [0.00, 0.00]\n\n\n\n\n\n\n\nFigure 13.10: Diagnostic plots for nb_para\n\n\n\n\n\n\nWe check these things similarly to how we’ve done it previously.\nFor influential points, we’re actually going to refit our model via smf.glm (which will give us access to get_influence). However, we’ll use the alpha value that was estimated from the model we fitted with NegativeBinomial, rather than making any assumptions about its value.\n\nalpha_est = nb_para.params['alpha']\n\nmodel = smf.glm(formula='parasite_count ~ lake * fish_length',\n                data=parasites,\n                family=sm.families.NegativeBinomial(alpha = alpha_est))\n\nglm_nb_para = model.fit()\n\nNow we can check influential points:\n\ninfluence = glm_nb_para.get_influence()\n\ncooks = influence.cooks_distance[0]\n\ninfluential_points = np.where(cooks &gt; 0.5)[0] # Set appropriate threshold here\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: [19 26]\n\n\nThere seem to be a couple of points with a higher Cook’s distance. If you investigate a bit further, they both still have a Cook’s distance of &lt;1, and neither of them cause dramatic changes to the model fit if removed.\nThe method that we’re using here to assess influence is quite sensitive, so a threshold of 1 might be more appropriate.\nCollinearity/variance inflation factors:\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrix\n\n# Drop the response variable\nX = parasites.drop(columns='parasite_count')\n\n# Create design matrix based on model formula\nX = dmatrix(\"lake * fish_length\", data=parasites, return_type='dataframe')\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n                 feature         VIF\n0              Intercept  110.301908\n1              lake[T.B]   46.505809\n2              lake[T.C]   47.455623\n3            fish_length    3.122694\n4  lake[T.B]:fish_length   47.367520\n5  lake[T.C]:fish_length   49.942831\n\n\n\n\n\nThe posterior predictive check looks alright, and there’s no high leverage points to worry about, but we seem to have some potential issues around our main effect of lake along with the lake:fish_length interaction.\nThis is often a sign that that the interaction term is unnecessary. Let’s test that theory with some significance testing and model comparison.\n\n13.7.4 Significance & model comparison\nLet’s see whether any of our predictors are significant (main effects and/or interaction effect).\n\n\nR\nPython\n\n\n\nWe can use the trusty anova and step functions:\n\nanova(nb_para, test = \"Chisq\")\n\nWarning in anova.negbin(nb_para, test = \"Chisq\"): tests made without\nre-estimating 'theta'\n\n\nAnalysis of Deviance Table\n\nModel: Negative Binomial(3.3786), link: log\n\nResponse: parasite_count\n\nTerms added sequentially (first to last)\n\n                 Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                                63    118.202              \nlake              2  19.1628        61     99.040 6.900e-05 ***\nfish_length       1  31.6050        60     67.435 1.889e-08 ***\nlake:fish_length  2   0.1471        58     67.288    0.9291    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nstep(nb_para)\n\nStart:  AIC=613.99\nparasite_count ~ lake * fish_length\n\n                   Df Deviance    AIC\n- lake:fish_length  2   67.435 610.14\n&lt;none&gt;                  67.288 613.99\n\nStep:  AIC=610.14\nparasite_count ~ lake + fish_length\n\n              Df Deviance    AIC\n&lt;none&gt;             67.286 610.14\n- lake         2   84.179 623.03\n- fish_length  1   98.819 639.67\n\n\n\nCall:  glm.nb(formula = parasite_count ~ lake + fish_length, data = parasites, \n    init.theta = 3.370433436, link = log)\n\nCoefficients:\n(Intercept)        lakeB        lakeC  fish_length  \n    1.78006     -0.40015      0.35282      0.08654  \n\nDegrees of Freedom: 63 Total (i.e. Null);  60 Residual\nNull Deviance:      117.9 \nResidual Deviance: 67.29    AIC: 612.1\n\n\nAlternatively, we could fit an additive model and compare the two directly:\n\nnb_para_add &lt;- glm.nb(parasite_count ~ lake + fish_length, parasites)\n\nanova(nb_para, nb_para_add)\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: parasite_count\n               Model    theta Resid. df    2 x log-lik.   Test    df  LR stat.\n1 lake + fish_length 3.370433        60       -602.1381                       \n2 lake * fish_length 3.378592        58       -601.9912 1 vs 2     2 0.1469037\n    Pr(Chi)\n1          \n2 0.9291809\n\n\nThis tells us that the interaction term is not significant.\n\n\nLet’s start by fitting a new additive model without our interaction.\nAgain, we start by specifying our response variable.\n\nY = parasites['parasite_count']\n\nThis next bit is much simpler than before, without the need to generate the interaction:\n\n# Create dummy variables for 'lake' (switch to True/False values)\nlake_dummies = pd.get_dummies(parasites['lake'], drop_first=True)\n\n# Combine the predictors: fish_length, lake dummies\nX = pd.concat([parasites['fish_length'], lake_dummies], axis=1)\n\n# Add a constant/intercept (required), and make sure that we're using floats\nX = sm.add_constant(X).astype(float)\n\nNow, we can fit our model and look at the summary.\n\n# Specify the model (Y, X)\nmodel = NegativeBinomial(Y, X)\n\nnb_para_add = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 4.704204\n         Iterations: 17\n         Function evaluations: 22\n         Gradient evaluations: 22\n\n\nNow, we want to compare the two models (nb_para and nb_para_add).\n\nlrstat = -2*(nb_para_add.llf - nb_para.llf)\n\npvalue = chi2.sf(lrstat, nb_para_add.df_resid - nb_para.df_resid)\n\nprint(lrstat, pvalue)\n\n0.14690373762027775 0.9291808672960002\n\n\nThis tells us that the interaction term is not significant.\n\n\n\nDropping the interaction will also solve our VIF problem:\n\n\nR\nPython\n\n\n\n\ncheck_collinearity(nb_para_add)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n        Term  VIF       VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n        lake 1.01 [1.00, 1.29e+14]     1.00      0.99     [0.00, 1.00]\n fish_length 1.01 [1.00, 1.29e+14]     1.00      0.99     [0.00, 1.00]\n\n\n\n\n\n# Drop the response variable\nX = parasites.drop(columns='parasite_count')\n\n# Create design matrix based on model formula\nX = dmatrix(\"lake + fish_length\", data=parasites, return_type='dataframe')\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n       feature        VIF\n0    Intercept  37.353114\n1    lake[T.B]   1.254778\n2    lake[T.C]   1.261793\n3  fish_length   1.006317\n\n\n\n\n\nExcellent. Seems we have found a well-fitting model for our parasites data!",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#exercises",
    "href": "materials/glm-13-overdispersed-count-data.html#exercises",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.8 Exercises",
    "text": "13.8 Exercises\n\n13.8.1 Bacteria colonies\n\n\n\n\n\n\nExercise 1 - Bacteria colonies\n\n\n\n\n\n\nLevel: \nIn this exercise, we’ll use the bacteria dataset.\nEach row of the dataset represents a petri dish on which bacterial colonies were grown. Each dish was given one of three antibacterial treatments, and then after enough time had passed (between 12 and 24 hours), the number of colonies on the dish were counted. Due to variation in the lab, not all dishes were assessed at the same time.\n\n\nR\nPython\n\n\n\n\nbacteria &lt;- read_csv(\"data/bacteria.csv\")\n\nhead(bacteria)\n\n# A tibble: 6 × 3\n  colony_count treatment incubation_time\n         &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;\n1           37 control                13\n2           34 control                22\n3          100 control                23\n4           47 control                15\n5           61 control                14\n6            4 control                12\n\n\n\n\n\nbacteria = pd.read_csv(\"data/bacteria.csv\")\n\nbacteria.head()\n\n   colony_count treatment  incubation_time\n0            37   control               13\n1            34   control               22\n2           100   control               23\n3            47   control               15\n4            61   control               14\n\n\n\n\n\nThe dataset contains three variables in total:\n\nThe response variable colony_count\n\nThe treatment (control/high/low)\nThe incubation_time (hrs)\n\nYou should:\n\nFit an additive (no interaction) negative binomial model\nVisualise the model*\nEvaluate whether the assumptions are met & the model fits well\nTest the significance of the model versus the null**\n\n*R users, this will involve a bit of thinking; consider using the broom::augment function to help you.\n**Python users, this will involve adapting previous code in a new way.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nFit an additive model\n\n\nR\nPython\n\n\n\n\nnb_petri &lt;- glm.nb(colony_count ~ treatment + incubation_time, bacteria)\n\nsummary(nb_petri)\n\n\nCall:\nglm.nb(formula = colony_count ~ treatment + incubation_time, \n    data = bacteria, init.theta = 2.196271257, link = log)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.55945    0.39276   6.517 7.19e-11 ***\ntreatmenthigh   -1.16289    0.18516  -6.280 3.38e-10 ***\ntreatmentlow     0.31331    0.18426   1.700 0.089058 .  \nincubation_time  0.07973    0.02232   3.573 0.000353 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.1963) family taken to be 1)\n\n    Null deviance: 174.616  on 89  degrees of freedom\nResidual deviance:  99.189  on 86  degrees of freedom\nAIC: 836.62\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.196 \n          Std. Err.:  0.342 \n\n 2 x log-likelihood:  -826.621 \n\n\n\n\nIdentify response variable:\n\nY = bacteria['colony_count']\n\nSet up predictors:\n\n# Create dummy variable for categorical treatment predictor\ntreatment_dummies = pd.get_dummies(bacteria['treatment'], drop_first=True)\n\n# Combine predictors\nX = pd.concat([bacteria['incubation_time'], treatment_dummies], axis=1)\n\n# Add a constant/intercept (required), cast to float\nX = sm.add_constant(X).astype(float)\n\nFit model:\n\nmodel = NegativeBinomial(Y, X)\n\nnb_petri = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 4.592339\n         Iterations: 16\n         Function evaluations: 18\n         Gradient evaluations: 18\n\nprint(nb_petri.summary())\n\n                     NegativeBinomial Regression Results                      \n==============================================================================\nDep. Variable:           colony_count   No. Observations:                   90\nModel:               NegativeBinomial   Df Residuals:                       86\nMethod:                           MLE   Df Model:                            3\nDate:                Thu, 24 Jul 2025   Pseudo R-squ.:                 0.06337\nTime:                        08:08:35   Log-Likelihood:                -413.31\nconverged:                       True   LL-Null:                       -441.27\nCovariance Type:            nonrobust   LLR p-value:                 4.364e-12\n===================================================================================\n                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nconst               2.5595      0.381      6.714      0.000       1.812       3.307\nincubation_time     0.0797      0.022      3.611      0.000       0.036       0.123\nhigh               -1.1629      0.190     -6.121      0.000      -1.535      -0.791\nlow                 0.3133      0.185      1.691      0.091      -0.050       0.676\nalpha               0.4553      0.071      6.414      0.000       0.316       0.594\n===================================================================================\n\n\n\n\n\nVisualise the model\n\n\nR\nPython\n\n\n\nBecause we’ve fitted a negative binomial model without an interaction term, we can’t just use geom_smooth (it will automatically include the interaction, which is incorrect).\nSo, we’re going to use the broom::augment function to extract some fitted values. We need to exponentiate these values to get the actual predicted counts, which we can then model with geom_line, like this:\n\nlibrary(broom)\n\n# created augmented model object\npetri_aug &lt;- augment(nb_petri)\n\nWarning: The `augment()` method for objects of class `negbin` is not maintained by the broom team, and is only supported through the `glm` tidier method. Please be cautious in interpreting and reporting broom output.\n\nThis warning is displayed once per session.\n\n# add a new predicted column (exp(fitted))\npetri_aug$.predicted &lt;- exp(petri_aug$.fitted) \n\nggplot(petri_aug, aes(x = incubation_time, y = colony_count, colour = treatment)) +\n  geom_point() +\n  geom_line(mapping = aes(y = .predicted), linewidth = 1)\n\n\n\n\n\n\n\nHere’s a more efficient way of doing exactly the same thing - we put the augmented object in directly as our data, and do the exponentiation inside the geom_line aes function:\n\nggplot(augment(nb_petri), aes(x = incubation_time, y = colony_count, colour = treatment)) +\n  geom_point() +\n  geom_line(mapping = aes(y = exp(.fitted)), linewidth = 1)\n\nNotice how this is subtly different from the model we would’ve visualised, if we’d just used geom_smooth:\n\nggplot(petri_aug, aes(x = incubation_time, y = colony_count, colour = treatment)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE) +\n  labs(title = \"This is not the model we wanted!\")\n\n\n\n\n\n\n\n\n\nWe can copy and adapt the code we used for the parasites example in the main body of the chapter - just remember to change all the names!\n\n# Get unique treatment values\ntreatment_levels = bacteria['treatment'].unique()\n\n# Create prediction grid for each lake\nnew_data_nb = pd.concat([\n    pd.DataFrame({\n        'incubation_time': np.linspace(12, 24, 100), # set sensible start & end!\n        'treatment': treatment\n    }) for treatment in treatment_levels\n])\n\n# Repeat the transformations we used when fitting the model\ntreatment_dummies = pd.get_dummies(new_data_nb['treatment'], drop_first=True)\nX_new = pd.concat([new_data_nb['incubation_time'], treatment_dummies], axis=1)\nX_new = sm.add_constant(X_new).astype(float)\n\n# Now we can safely predict:\nnew_data_nb['pred'] = nb_petri.predict(X_new)\n\nnew_data_nb.head()\n\n   incubation_time treatment       pred\n0        12.000000   control  33.657760\n1        12.121212   control  33.984624\n2        12.242424   control  34.314663\n3        12.363636   control  34.647907\n4        12.484848   control  34.984387\n\n\n\np = (ggplot(bacteria, aes(x = \"incubation_time\",\n                          y = \"colony_count\",\n                          colour = \"treatment\")) +\n   geom_point() +\n   geom_line(new_data_nb, aes(x = \"incubation_time\", y = \"pred\",\n                              colour = \"treatment\"), size = 1))\n                              \np.show()\n\n\n\n\n\n\n\n\n\n\nEvaluate assumptions & fit\nWe don’t need to worry about dispersion - we’re explicitly modelling that, rather than making any assumptions.\nWe can be reasonably sure we’ve chosen the right response variable distribution and link function; these are clearly count data.\nWith the info we’ve got, we don’t have any reason to be worried about independence. If, however, we found out more about the design - for example, that the petri dishes had been processed in batches, perhaps by different researchers - that might set off alarm bells.\n\n\nR\nPython\n\n\n\n\ncheck_model(nb_petri, check = c('pp_check', 'outliers', 'vif'))\n\nCannot simulate residuals for models of class `negbin`. Please try\n  `check_model(..., residual_type = \"normal\")` instead.\n\n\n\n\n\n\n\ncheck_collinearity(nb_petri)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n            Term  VIF   VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n       treatment 1.08 [1.01, 2.25]     1.04      0.92     [0.44, 0.99]\n incubation_time 1.08 [1.01, 2.25]     1.04      0.92     [0.44, 0.99]\n\ncheck_outliers(nb_petri, threshold = list('cook'=0.5))\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\n\nNo obvious issues to report!\n\n\nRefit the model with smf.glm, so we can access get_influence. Don’t forget to include the estimated alpha.\n\nalpha_est = nb_petri.params['alpha']\n\nmodel = smf.glm(formula='colony_count ~ treatment + incubation_time',\n                data=bacteria,\n                family=sm.families.NegativeBinomial(alpha = alpha_est))\n\nglm_nb_petri = model.fit()\n\nNow we can check influential points:\n\ninfluence = glm_nb_petri.get_influence()\n\ncooks = influence.cooks_distance[0]\n\ninfluential_points = np.where(cooks &gt; 0.5)[0] # Set appropriate threshold here\n\nprint(\"Influential points:\", influential_points)\n\nInfluential points: [88]\n\n\nWe have one possible influential point; if this were a real analysis, we would want to follow up on this now, e.g., removing it and refitting, and seeing if our model shifts dramatically.\n(Spoiler alert - it doesn’t!)\nCollinearity/variance inflation factors:\n\n# Drop the response variable\nX = bacteria.drop(columns='colony_count')\n\n# Create design matrix based on model formula\nX = dmatrix(\"treatment + incubation_time\", data=bacteria, return_type='dataframe')\n\n# Calculate VIF for each feature\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\nvif_data['VIF'] = [variance_inflation_factor(X.values, i)\n                   for i in range(X.shape[1])]\n\nprint(vif_data)\n\n             feature        VIF\n0          Intercept  28.462829\n1  treatment[T.high]   1.379350\n2   treatment[T.low]   1.435343\n3    incubation_time   1.079513\n\n\nNo issues with the VIF values!\n\n\n\nCompare model to null\n\n\nR\nPython\n\n\n\n\nnb_petri_null &lt;- glm.nb(colony_count ~ 1, bacteria)\n\nanova(nb_petri, nb_petri_null)\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: colony_count\n                        Model    theta Resid. df    2 x log-lik.   Test    df\n1                           1 1.220838        89       -882.5434             \n2 treatment + incubation_time 2.196271        86       -826.6210 1 vs 2     3\n  LR stat.      Pr(Chi)\n1                      \n2 55.92243 4.364176e-12\n\n\n\n\nWe’ve not constructed a null model yet earlier in this chapter, so don’t worry if you didn’t figure this bit out on your own.\nWe still need to set up a Y response and an X predictor matrix. However, since we only want to regress colony_count ~ 1, our X matrix just needs to be a column of 1s, with the same length as Y.\nWe can set that up like this:\n\nY = bacteria['colony_count']\nX = np.ones((len(Y), 1))\n\nFrom here, the model fitting proceeds exactly as before.\n\nmodel = NegativeBinomial(Y, X)\n\nnb_petri_null = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 4.903019\n         Iterations: 6\n         Function evaluations: 7\n         Gradient evaluations: 7\n\nprint(nb_petri_null.summary())\n\n                     NegativeBinomial Regression Results                      \n==============================================================================\nDep. Variable:           colony_count   No. Observations:                   90\nModel:               NegativeBinomial   Df Residuals:                       89\nMethod:                           MLE   Df Model:                            0\nDate:                Thu, 24 Jul 2025   Pseudo R-squ.:               3.734e-12\nTime:                        08:08:36   Log-Likelihood:                -441.27\nconverged:                       True   LL-Null:                       -441.27\nCovariance Type:            nonrobust   LLR p-value:                       nan\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          3.9035      0.097     40.423      0.000       3.714       4.093\nalpha          0.8191      0.116      7.049      0.000       0.591       1.047\n==============================================================================\n\n\nWe can see that the model, indeed, only contains a constant (intercept) with no predictors. That’s what we wanted.\nNow, we want to compare the two models (nb_petri and nb_petri_null). This reuses the likelihood ratio test code we’ve been using so far in the course:\n\nlrstat = -2*(nb_petri_null.llf - nb_petri.llf)\n\npvalue = chi2.sf(lrstat, nb_petri_null.df_resid - nb_petri.df_resid)\n\nprint(lrstat, pvalue)\n\n55.92242554561949 4.364156630706304e-12\n\n\n\n\n\nIn conclusion: our additive model is significant over the null, and there are no glaring concerns with assumptions that are not met.\nOur visualisation of the model seems sensible too. The number of colonies increases with incubation time - this seems very plausible - and the high antibacterial condition definitely is stunted compared to the control, as we might expect.\nWhy does the low antibacterial dose condition seem to have more growth than the control? Well, this could just be a fluke. If we repeated the experiment the effect might vanish.\nOr, it might be an indication of a stress-induced growth response in those dishes (i.e., a small amount of antibacterial treatment reveals some sort of resistance).\n\n\n\n\n\n\n\n\n\n\n13.8.2 Galapagos models\n\n\n\n\n\n\nExercise 2\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/galapagos.csv.\nIn this dataset, each row corresponds to one of the 30 Galapagos islands. The relationship between the number of plant species (species) and several geographic variables is of interest:\n\n\nendemics – the number of endemic species\n\narea – the area of the island km2\n\n\nelevation – the highest elevation of the island (m)\n\nnearest – the distance from the nearest island (km)\n\nHowever, fitting both a Poisson and a negative binomial regression produce quite bad results, with strange fit.\nIn this exercise, you should:\n\nFit and visualise a Poisson model\nFit and visualise a negative binomial model\nExplore ways to improve the fit of the model\n\n(Hint: just because we’re fitting a GLM, doesn’t mean we can’t still transform our data!)\n\n\n\n\n\n\nA note for Python users\n\n\n\n\n\nDue to the small (near-zero) beta coefficients for these data, the NegativeBinomial method we’ve been using struggles to converge on a successful model. Therefore, there is no Python code provided for this example.\nHowever, you might find it useful to read through the answer regardless - this is a very interesting dataset!\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\ngalapagos &lt;- read_csv(\"data/galapagos.csv\")\n\nFitting a Poisson model\n\nglm_gal &lt;- glm(species ~ area + endemics + elevation + nearest,\n               data = galapagos, family = \"poisson\")\n\n\np1 &lt;- ggplot(galapagos, aes(endemics, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = \"poisson\")) +\n  labs(title = \"Poisson\")\n\np1\n\n\n\n\n\n\n\nOur model is doing okay over on the right hand side, but it’s not really doing a good job in the bottom left corner - which is where most of the data points are clustered!\nFitting a negative binomial model\n\nnb_gal &lt;- glm.nb(species ~ area + endemics + elevation + nearest,\n               data = galapagos)\n\n\np2 &lt;- ggplot(galapagos, aes(endemics, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE) +\n  labs(title = \"Negative binomial\")\n\np2\n\n\n\n\n\n\n\nThis model does a better job in the lower left, but now is getting things a bit wrong over on the far right.\nImproving the fit\nIs there anything we can do to improve on these two models?\nThere appears to be a power relationship between species and endemics. So, one thing we could do is log-transform the response (species) and predictor (endemics) variables.\nIf we log-transform them both and put them on a scatterplot, they look like this - linear?!\n\nggplot(data = galapagos,\n       aes(x = log(endemics),\n           y = log(species))) +\n  geom_point()\n\n\n\n\n\n\n\nWe could add a regression line to this.\nOr, we could just log-transform endemics and fit a negative binomial model to that, without also log-transforming species. Let’s do all of this and plot them together with the original Poisson and Negative binomial models.\n\np3 &lt;- ggplot(galapagos, aes(log(endemics), log(species))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE,\n              method.args = list(family = poisson)) +\n  labs(title = \"Linear model of log-log\")\n\np4 &lt;- ggplot(galapagos, aes(log(endemics), species)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE) +\n  ylim(0, 500) +\n  labs(title = \"Negative binomial of log(endemics)\")\n\n\nlibrary(patchwork)\n\np1 + p2 + p3 + p4 +\n  plot_annotation(tag_levels = \"A\")\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: In lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n    ...) :\n extra argument 'family' will be disregarded\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\nFrom this it is clear that the negative binomial model fitted to species ~ log(endemics) in panel D produces a much better fit than the original fit in panel B.\nEqually, looking at the relationship between log(species) ~ log(endemics) in panel C it illustrates that this is pretty well-modelled using a linear line.\nThere is a slight issue, though.\nIf you look carefully then you see that in both panels C and D there is a stray value left of zero, almost escaping past the y-axis. There is also a warning message, saying Removed 1 row containing non-finite outside the scale range.\nWith a closer look at the dataset, we can see that there is one island where the number of endemics is equal to 0:\n\ngalapagos |&gt; \n  arrange(endemics) |&gt;\n  head(1)\n\n# A tibble: 1 × 5\n  species endemics  area elevation nearest\n    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1      24        0  0.08        93       6\n\n\nIf we take log(0) we get minus infinity, which isn’t biologically possible. This is where the problem lies.\nWe could adjust for this by adding a “pseudo-count”, or adding 1 to all of the counts. If that is acceptable or not is a matter of debate and we’ll leave it to you to ponder over this.\nWhatever you do, the most important thing is to make sure that you are transparent and clear on what you are doing and what the justification for it it.",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-13-overdispersed-count-data.html#summary",
    "href": "materials/glm-13-overdispersed-count-data.html#summary",
    "title": "\n13  Overdispersed count data\n",
    "section": "\n13.9 Summary",
    "text": "13.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nNegative binomial regression relaxes the assumption made by Poisson regressions that the variance is equal to the mean (dispersion = 1)\nIn a negative binomial regression the dispersion parameter \\(\\theta\\) is estimated from the data\nHowever, they both use the log link function and often produce similar-looking models",
    "crumbs": [
      "Count responses",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Overdispersed count data</span>"
    ]
  }
]