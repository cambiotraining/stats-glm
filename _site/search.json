[
  {
    "objectID": "index.html#core-aims",
    "href": "index.html#core-aims",
    "title": "Course overview",
    "section": "Core aims",
    "text": "Core aims\nTo introduce sufficient understanding and coding experience for analysing data with non-continuous response variables.\n\n\n\n\n\n\nCourse aims\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nConstruct\n\na logistic model for binary response variables\na logistic model for proportion response variables\na Poisson model for count response variables\na Negative Binomial model for count response variables (to be added later)\n\nPlot the data and the fitted curve in each case for both continuous and categorical predictors\nAssess the significance of fit\nAssess assumption of the model",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "R and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR \nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "materials.html#data",
    "href": "materials.html#data",
    "title": "3  Data",
    "section": "3.1 Data",
    "text": "3.1 Data\n Download",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "4  References",
    "section": "",
    "text": "Glen, Stephanie. 2021. “Link Function.” Statistics How\nTo: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant,\nPeter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow\nBetween Two Species of Darwin’s Finches.” Nature Ecology\n& Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#data",
    "href": "materials/glm-intro-lm.html#data",
    "title": "\n5  Linear models\n",
    "section": "\n5.1 Data",
    "text": "5.1 Data\nFor this example, we’ll be using the several data sets about Darwin’s finches. They are part of a long-term genetic and phenotypic study on the evolution of several species of finches. The exact details are less important for now, but there are data on multiple species where several phenotypic characteristics were measured (see Figure 5.1).\n\n\n\n\n\nFigure 5.1: Finches phenotypes (courtesy of HHMI BioInteractive)",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#exploring-data",
    "href": "materials/glm-intro-lm.html#exploring-data",
    "title": "\n5  Linear models\n",
    "section": "\n5.2 Exploring data",
    "text": "5.2 Exploring data\nIt’s always a good idea to explore your data visually. Here we are focussing on the (potential) relationship between beak length (blength) and beak depth (bdepth).\nOur data contains measurements from two years (year) and two species (species). If we plot beak depth against beak length, colour our data by species and look across the two time points (1975 and 2012), we get the following graph:\n\n\n\n\n\n\n\nFigure 5.2: Beak depth and length for G. fortis and G. scandens\n\n\n\n\nIt seems that there is a potential linear relationship between beak depth and beak length. There are some differences between the two species and two time points with, what seems, more spread in the data in 2012. The data for both species also seem to be less separated than in 1975.\nFor the current purpose, we’ll focus on one group of data: those of Geospiza fortis in 1975.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#linear-model",
    "href": "materials/glm-intro-lm.html#linear-model",
    "title": "\n5  Linear models\n",
    "section": "\n5.3 Linear model",
    "text": "5.3 Linear model\nLet’s look at the G. fortis data more closely, assuming that the have a linear relationship. We can visualise that as follows:\n\n\n\n\n\n\n\nFigure 5.3: Beak depth vs beak length G. fortis (1975)\n\n\n\n\nIf you recall from the Core statistics linear regression session, what we’re doing here is assuming that there is a linear relationship between the response variable (in this case bdepth) and predictor variable (here, blength).\nWe can get more information on this linear relationship by defining a linear model, which has the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhere \\(Y\\) is the response variable (the thing we’re interested in), \\(X\\) the predictor variable and \\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients. More explicitly for our data, we get:\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nBut how do we find this model? The computer uses a method called least-squares regression. There are several steps involved in this.\n\n5.3.1 Line of best fit\nThe computer tries to find the line of best fit. This is a linear line that best describes your data. We could draw a linear line through our cloud of data points in many ways, but the least-squares method converges to a single solution, where the sum of squared residual deviations is at its smallest.\nTo understand this a bit better, it’s helpful to realise that each data point consists of a fitted value (the beak depth predicted by the model at a given beak length), combined with the error. The error is the difference between the fitted value and the data point.\nLet’s look at this for one of the observations, for example finch 473:\n\n\n\n\n\n\n\nFigure 5.4: Beak depth vs beak length (finch 473, 1975)\n\n\n\n\nObtaining the fitted value and error happens for each data point. All these residuals are then squared (to ensure that they are positive), and added together. This is the so-called sum-of-squares.\nYou can imagine that if you draw a line through the data that doesn’t fit the data all that well, the error associated with each data point increases. The sum-of-squares then also increases. Equally, the closer the data are to the line, the smaller the error. This results in a smaller sum-of-squares.\nThe linear line where the sum-of-squares is at its smallest, is called the line of best fit. This line acts as a model for our data.\nFor finch 473 we have the following values:\n\nthe observed beak depth is 9.5 mm\nthe observed beak length is 10.5 mm\nthe fitted value is 9.11 mm\nthe error is 0.39 mm\n\n5.3.2 Linear regression\nOnce we have the line of best fit, we can perform a linear regression. What we’re doing with the regression, is asking:\n\nIs the line of best fit a better predictor of our data than a horizontal line across the average value?\n\nVisually, that looks like this:\n\n\n\n\n\n\n\nFigure 5.5: Regression: is the slope different from zero?\n\n\n\n\nWhat we’re actually testing is whether the slope (\\(\\beta_1\\)) of the line of best fit is any different from zero.\nTo find the answer, we perform an ANOVA. This gives us a p-value of 1.68e-78.\nNeedless to say, this p-value is extremely small, and definitely smaller than any common significance threshold, such as \\(p &lt; 0.05\\). This suggests that beak length is a statistically significant predictor of beak depth.\nIn this case the model has an intercept (\\(\\beta_0\\)) of -0.34 and a slope (\\(\\beta_1\\)) of 0.9. We can use this to write a simple linear equation, describing our data. Remember that this takes the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhich in our case is\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nand gives us\n\\[\nbeak\\ depth = -0.34 + 0.90 \\times beak\\ length\n\\]\n\n5.3.3 Assumptions\nIn example above we just got on with things once we suspected that there was a linear relationship between beak depth and beak length. However, for the linear regression to be valid, several assumptions need to be met. If any of those assumptions are violated, we can’t trust the results. The following four assumptions need to be met, with a 5th point being a case of good scientific practice:\n\nData should be linear\nResiduals are normally distributed\nEquality of variance\nThe residuals are independent\n(no influential points)\n\nAs we did many times during the Core statistics sessions, we mainly rely on diagnostic plots to check these assumptions. For this particular model they look as follows:\n\n\n\n\n\n\n\nFigure 5.6: Diagnostic plots for G. fortis (1975) model\n\n\n\n\nThese plots look very good to me. For a recap on how to interpret these plots, see CS2: ANOVA.\nTaken together, we can see the relationship between beak depth and beak length as a linear one, described by a (linear) model that has a predicted value for each data point, and an associated error.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "href": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.1 Putting the “G” into GLM",
    "text": "6.1 Putting the “G” into GLM\nIn the previous linear model example all the assumptions were met. But what if we have data where that isn’t the case? For example, what if we have data where we can’t describe the relationship between the predictor and response variables in a linear way?\nOne of the ways we can deal with this is by using a generalised linear model, also abbreviated as GLM. In a way it’s an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nHere, the \\(\\beta_0\\) value is the constant or intercept, whereas each subsequent \\(\\beta_i\\) is a unique regression coefficient for each \\(X_i\\) predictor variable. So far so good.\nHowever, the GLM makes the linear model more flexible in two ways:\n\n\n\n\n\n\nImportant\n\n\n\n\nIn a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a link function.\nA standard linear model assumes a continuous, normally distributed outcome, whereas with GLM the outcome can be both continuous or integer. Furthermore, the outcome does not have to be normally distributed. Indeed, the outcome can follow a different kind of distribution, such as binomial, Poisson, exponential etc.\n\n\n\nWe’ll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.\nThe link function and different distributions are closely…err, linked. To make sense of what the link function is doing it’s useful to understand the different distributional assumptions. So we’ll start with those.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#distributions",
    "href": "materials/glm-intro-glm.html#distributions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.2 Distributions",
    "text": "6.2 Distributions\nIn the examples of a standard linear model we’ve seen that the residuals needed to be normally distributed. We’ve mainly used the Q-Q plot to assess this assumption of normality.\nBut what does “normal” actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.\nWe can see this in Figure 6.1. The mean of the normal distribution is indicated with the dashed blue line.\n\n\n\n\n\n\n\nFigure 6.1: Normal distribution\n\n\n\n\nWe can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of G. fortis beaks in 1975.\nThe individual values of the residuals from this linear model are shown in Figure 6.1, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see Figure 5.6).\nAll this means is that assuming that these residuals may come from a normal distribution isn’t such a daft suggestion after all.\nNow look at the example in Figure 6.2. This shows the classification of beak shape for a number of finches. Their beaks are either classed as blunt or pointed. Various (continuous) measurements were taken from each bird, with the beak length shown here.\n\n\n\n\n\n\n\nFigure 6.2: Classification in beak shape\n\n\n\n\nWe’ll look into this example in more detail later. For now it’s important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (blunt or pointed). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (Figure 6.2 B).\nSo what do we do? Well, the normal distribution is not the only one there is. In Figure 6.3 there are a few examples of distributions (including the normal one).\n\n\n\n\n\n\n\nFigure 6.3: Different distributions\n\n\n\n\nDifferent distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.\nIn order to understand how this can help us, we need to be aware of two more concepts: linear predictors and link functions.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#linear-predictors",
    "href": "materials/glm-intro-glm.html#linear-predictors",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.3 Linear predictors",
    "text": "6.3 Linear predictors\nThe nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.\nAs mentioned before, predictors are in the form of a linear combination, where each predictor variable is multiplied by a coefficient and all the terms are added together:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nFortunately, this is no different for generalised linear models! We still have a linear combination but, as we’ll see, if the relationship is not linear then we need an additional step before we can model the data in this way.\nAt this point, we have two options at our disposal (well, there are more, but let’s not muddy the waters too much).\n\n\n\n\n\n\nImportant\n\n\n\n\nTransform our data and use a normal linear model on the transformed data\nTransform the linear predictor\n\n\n\nThe first option, to transform our data, seems like a useful option and can work. It keeps things familiar (we’d still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables (Stevens 1946).\nThe second option would be to transform the linear predictor. This enables us to map a non-linear outcome (or response variable) to a linear model. This transformation is done using a link function.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#link-functions",
    "href": "materials/glm-intro-glm.html#link-functions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.4 Link functions",
    "text": "6.4 Link functions\nSimply put: link functions connect the predictors in our model to the response variables in a linear way.\nHowever, and similar to the standard linear model, there are two parts to each model:\n\nthe coefficients for each predictor (linking each parameter to a predictor)\nthe error or random component (which specifies a probability distribution)\n\nWhich link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from (Glen 2021)):\n\n\ndistribution\ndata type\nlink name\n\n\n\nbinomial\nbinary / proportion\nlogit\n\n\nnormal\nany real number\nidentity\n\n\npoisson\ncount data\nlog\n\n\n\nLet’s again look at the earlier example of beak shape.\n\n\n\n\n\n\n\nFigure 6.4: Beak shape classification\n\n\n\n\nWe’ve seen the data in Figure 6.4 A before, where we had information on what beak shape our observed finches had, plotted against their beak length.\nLet’s say we now want to make some predictions about what beak shape we would expect, given a certain beak length. In this scenario we’d need some way of modelling the response variable (beak shape; blunt or pointed) as a function of the predictor variable (beak length).\nThe issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in Figure 6.2 A) but this is really bad practice. Why? Well, what such a linear model represents is the probability - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (Figure 6.4 B).\nSimply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can’t describe these data as a linear relationship.\nInstead, we’ll use a logistic model to analyse these data. We’ll cover the practicalities of how to do this in more detail in a later chapter, but for now it’s sufficient to realise that one of the ways we could model these data could look like this:\n\n\n\n\n\n\n\nFigure 6.5: Logistic model for beak classification\n\n\n\n\nUsing this sigmoidal curve ensures that our predicted probabilities do not exceed the \\([0, 1]\\) range.\nNow, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the logit link function. This links the non-linear response variable (beak shape) to a linear model, using beak length as a predictor.\nWe’ll practice how to perform this analysis in the next section.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#key-points",
    "href": "materials/glm-intro-glm.html#key-points",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.5 Key points",
    "text": "6.5 Key points\n\n\n\n\n\n\nNote\n\n\n\n\nGLMs allow us to map a non-linear outcome to a linear model\nThe link function determines how this occurs, transforming the linear predictor\n\n\n\n\n\n\n\nGlen, Stephanie. 2021. “Link Function.” Statistics How To: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Background",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#libraries-and-functions",
    "href": "materials/glm-practical-logistic-binary.html#libraries-and-functions",
    "title": "\n7  Binary response\n",
    "section": "\n7.1 Libraries and functions",
    "text": "7.1 Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n7.1.1 Libraries\n\n7.1.2 Functions\n\n\n\n\n7.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n\n7.1.4 Functions\n\n\n\n\n\n\n\nThe example in this section uses the following data set:\ndata/finches_early.csv\nThese data come from an analysis of gene flow across two finch species (Lamichhaney et al. 2020). They are slightly adapted here for illustrative purposes.\nThe data focus on two species, Geospiza fortis and G. scandens. The original measurements are split by a uniquely timed event: a particularly strong El Niño event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as early (pre-1983) and late (1983 onwards).\nHere we are looking only at the early data. We are specifically focussing on the beak shape classification, which we saw earlier in Figure 6.5.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "href": "materials/glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "title": "\n7  Binary response\n",
    "section": "\n7.2 Load and visualise the data",
    "text": "7.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\n\n\n\n\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n\n\n\n\nLooking at the data, we can see that the pointed_beak column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the pointed_beak data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a pointed beak (1), and those with a blunt one (0).\nWe can force Python to temporarily covert the data to a factor, by making the pointed_beak column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea…\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\n\n\n\n\n\n\n\n\n\nOf course this is rubbish - we can’t have a beak classification outside the range of \\([0, 1]\\). It’s either blunt (0) or pointed (1).\nBut for the sake of exploration, let’s look at the assumptions:\n\n\nR\nPython\n\n\n\n\nlm_bks &lt;- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_bks_py)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThey’re pretty extremely bad.\n\nThe response is not linear (Residual Plot, binary response plot, common sense).\nThe residuals do not appear to be distributed normally (Q-Q Plot)\nThe variance is not homogeneous across the predicted values (Location-Scale Plot)\nBut - there is always a silver lining - we don’t have influential data points.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#creating-a-suitable-model",
    "href": "materials/glm-practical-logistic-binary.html#creating-a-suitable-model",
    "title": "\n7  Binary response\n",
    "section": "\n7.3 Creating a suitable model",
    "text": "7.3 Creating a suitable model\nSo far we’ve established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what can we do?\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the \\([0, 1]\\) range.\nWe described our standard linear relationship as follows:\n\\(Y = \\beta_0 + \\beta_1X\\)\nWe can now map this to our non-linear relationship via the logistic link function:\n\\(Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}\\)\nNote that the \\(\\beta_0 + \\beta_1X\\) part is identical to the formula of a straight line.\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n\n\n\n\n\nEuler’s number (\\(\\exp\\)): would you like to know more?\n\n\n\n\n\nIn mathematics, \\(\\rm e\\) represents a constant of around 2.718. Another notation is \\(\\exp\\), which is often used when notations become a bit cumbersome. Here, I exclusively use the \\(\\exp\\) notation for consistency.\n\n\n\n\n\n\n\n\n\nThe logistic function\n\n\n\nThe shape of the logistic function is hugely influenced by the different parameters, in particular \\(\\beta_1\\). The plots below show different situations, where \\(\\beta_0 = 0\\) in all cases, but \\(\\beta_1\\) varies.\nThe first plot shows the logistic function in its simplest form, with the others showing the effect of varying \\(\\beta_1\\).\n\n\n\n\n\n\n\n\n\nwhen \\(\\beta_1 = 1\\), this gives the simplest logistic function\nwhen \\(\\beta_1 = 0\\) gives a horizontal line, with \\(Y = \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\)\n\nwhen \\(\\beta_1\\) is negative flips the curve around, so it slopes down\nwhen \\(\\beta_1\\) is very large then the curve becomes extremely steep\n\n\n\nWe can fit such an S-shaped curve to our early_finches data set, by creating a generalised linear model.\n\n\nR\nPython\n\n\n\nIn R we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks:\n\nglm_bks &lt;- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\nIf you forget to set the family argument, then the glm() function will perform a standard linear model fit, identical to what the lm() function would do.\n\n\nIn Python we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks_py:\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()\n\nThe format of this function is similar to that used by the ols() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial. This is buried deep inside the statsmodels package and needs to be defined as sm.families.Binomial().",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#model-output",
    "href": "materials/glm-practical-logistic-binary.html#model-output",
    "title": "\n7  Binary response\n",
    "section": "\n7.4 Model output",
    "text": "7.4 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\n\nR\nPython\n\n\n\n\nsummary(glm_bks)\n\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nprint(glm_bks_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Mon, 15 Jan 2024   Deviance:                       9.1879\nTime:                        10:25:50   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n\n\n\n\n\nThere’s a lot to unpack here, but let’s start with what we’re familiar with: coefficients!",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#parameter-interpretation",
    "href": "materials/glm-practical-logistic-binary.html#parameter-interpretation",
    "title": "\n7  Binary response\n",
    "section": "\n7.5 Parameter interpretation",
    "text": "7.5 Parameter interpretation\n\n\nR\nPython\n\n\n\nThe coefficients or parameters can be found in the Coefficients block. The main numbers to extract from the output are the two numbers underneath Estimate.Std:\nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the coef column:\n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n\n\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\nThe \\(p\\) values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the \\(p\\) values obtained in the summary output of a linear model. As with continuous predictors in simple models, these \\(p\\) values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these \\(p\\) values aren’t great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a \\(p\\) value for each level rather than for the predictor as a whole).\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\\[ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} \\]\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\n\nR\nPython\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n\n[1] 0.9994131\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n\n[1] 7.410155e-05\n\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n# import the math library\nimport math\n\n\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n\n0.9994130595039192\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n\n7.410155028945912e-05\n\n\n\n\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I’m visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n\n\n\n\n\nCode available here\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nglm_bks %&gt;% \n  augment(type.predict = \"response\") %&gt;% \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n\n\n\n\n(ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Predicted probabilities for beak classification\n\n\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It’s also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#parameter-estimation-explained",
    "href": "materials/glm-practical-logistic-binary.html#parameter-estimation-explained",
    "title": "\n7  Binary response\n",
    "section": "\n7.6 Parameter estimation explained",
    "text": "7.6 Parameter estimation explained\nNow that we know how to interpret the coefficients or parameters, let’s have a look at how they’re actually determined. To understand this, we need to take a step back.\n\nSum of squares (OLS)\nMLE\nDeviance",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#assumptions",
    "href": "materials/glm-practical-logistic-binary.html#assumptions",
    "title": "\n7  Binary response\n",
    "section": "\n7.7 Assumptions",
    "text": "7.7 Assumptions\n\nGAMLSS",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#exercises",
    "href": "materials/glm-practical-logistic-binary.html#exercises",
    "title": "\n7  Binary response\n",
    "section": "\n7.8 Exercises",
    "text": "7.8 Exercises\n\n7.8.1 Diabetes\n\nLevel: \nFor this exercise we’ll be using the data from data/diabetes.csv.\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called test_result, glucose and diastolic.\nWe want to see if the glucose tolerance is a meaningful predictor for predictions on a diabetes test. To investigate this, do the following:\n\nLoad and visualise the data\nCreate a suitable model\nDetermine if there are any statistically significant predictors\nCalculate the probability of a positive diabetes test result for a glucose tolerance test value of glucose = 150\n\n\n\n7.9 Answer\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\n\nR\nPython\n\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Mon, 15 Jan 2024   Deviance:                       752.20\nTime:                        10:25:54   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\n\nR\nPython\n\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#answer",
    "href": "materials/glm-practical-logistic-binary.html#answer",
    "title": "\n7  Binary response\n",
    "section": "\n7.9 Answer",
    "text": "7.9 Answer\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\n\nR\nPython\n\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Mon, 15 Jan 2024   Deviance:                       752.20\nTime:                        10:25:54   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\n\nR\nPython\n\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 145))\n\n[1] 0.6685441\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 145))\n\n0.6685441044999503\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 67%.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#summary",
    "href": "materials/glm-practical-logistic-binary.html#summary",
    "title": "\n7  Binary response\n",
    "section": "\n7.10 Summary",
    "text": "7.10 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a logistic regression to model a binary response\nWe can feed new observations into the model and get probabilities for the outcome\n\n\n\n\n\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant, Peter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow Between Two Species of Darwin’s Finches.” Nature Ecology & Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.",
    "crumbs": [
      "Binary and proportional data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  }
]