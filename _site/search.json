[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course overview",
    "section": "",
    "text": "Core aims\nTo introduce sufficient understanding and coding experience for analysing data with non-continuous response variables.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "index.html#core-aims",
    "href": "index.html#core-aims",
    "title": "Course overview",
    "section": "",
    "text": "Course aims\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nConstruct\n\na logistic model for binary response variables\na logistic model for proportion response variables\na Poisson model for count response variables\na Negative Binomial model for count response variables (to be added later)\n\nPlot the data and the fitted curve in each case for both continuous and categorical predictors\nAssess the significance of fit\nAssess assumption of the model",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Course overview",
    "section": "Authors",
    "text": "Authors\nAbout the author(s):\n\nVicki Hodgson  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation; coding\nMatt Castle  Affiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: conceptualisation; writing\nRob Nicholls  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: conceptualisation\nMartin van Rongen  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - review & editing; conceptualisation; coding",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Course overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "R and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR \nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Setup</span>"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Data\nDownload",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "4  References",
    "section": "",
    "text": "Glen, Stephanie. 2021. “Link Function.” Statistics How\nTo: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant,\nPeter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow\nBetween Two Species of Darwin’s Finches.” Nature Ecology\n& Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html",
    "href": "materials/glm-intro-lm.html",
    "title": "\n5  Linear models\n",
    "section": "",
    "text": "5.1 Data\nFor this example, we’ll be using the several data sets about Darwin’s finches. They are part of a long-term genetic and phenotypic study on the evolution of several species of finches. The exact details are less important for now, but there are data on multiple species where several phenotypic characteristics were measured (see Figure 5.1).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#data",
    "href": "materials/glm-intro-lm.html#data",
    "title": "\n5  Linear models\n",
    "section": "",
    "text": "Figure 5.1: Finches phenotypes (courtesy of HHMI BioInteractive)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#exploring-data",
    "href": "materials/glm-intro-lm.html#exploring-data",
    "title": "\n5  Linear models\n",
    "section": "\n5.2 Exploring data",
    "text": "5.2 Exploring data\nIt’s always a good idea to explore your data visually. Here we are focussing on the (potential) relationship between beak length (blength) and beak depth (bdepth).\nOur data contains measurements from two years (year) and two species (species). If we plot beak depth against beak length, colour our data by species and look across the two time points (1975 and 2012), we get the following graph:\n\n\n\n\n\n\n\nFigure 5.2: Beak depth and length for G. fortis and G. scandens\n\n\n\n\nIt seems that there is a potential linear relationship between beak depth and beak length. There are some differences between the two species and two time points with, what seems, more spread in the data in 2012. The data for both species also seem to be less separated than in 1975.\nFor the current purpose, we’ll focus on one group of data: those of Geospiza fortis in 1975.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-lm.html#linear-model",
    "href": "materials/glm-intro-lm.html#linear-model",
    "title": "\n5  Linear models\n",
    "section": "\n5.3 Linear model",
    "text": "5.3 Linear model\nLet’s look at the G. fortis data more closely, assuming that the have a linear relationship. We can visualise that as follows:\n\n\n\n\n\n\n\nFigure 5.3: Beak depth vs beak length G. fortis (1975)\n\n\n\n\nIf you recall from the Core statistics linear regression session, what we’re doing here is assuming that there is a linear relationship between the response variable (in this case bdepth) and predictor variable (here, blength).\nWe can get more information on this linear relationship by defining a linear model, which has the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhere \\(Y\\) is the response variable (the thing we’re interested in), \\(X\\) the predictor variable and \\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients. More explicitly for our data, we get:\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nBut how do we find this model? The computer uses a method called least-squares regression. There are several steps involved in this.\n\n5.3.1 Line of best fit\nThe computer tries to find the line of best fit. This is a linear line that best describes your data. We could draw a linear line through our cloud of data points in many ways, but the least-squares method converges to a single solution, where the sum of squared residual deviations is at its smallest.\nTo understand this a bit better, it’s helpful to realise that each data point consists of a fitted value (the beak depth predicted by the model at a given beak length), combined with the error. The error is the difference between the fitted value and the data point.\nLet’s look at this for one of the observations, for example finch 473:\n\n\n\n\n\n\n\nFigure 5.4: Beak depth vs beak length (finch 473, 1975)\n\n\n\n\nObtaining the fitted value and error happens for each data point. All these residuals are then squared (to ensure that they are positive), and added together. This is the so-called sum-of-squares.\nYou can imagine that if you draw a line through the data that doesn’t fit the data all that well, the error associated with each data point increases. The sum-of-squares then also increases. Equally, the closer the data are to the line, the smaller the error. This results in a smaller sum-of-squares.\nThe linear line where the sum-of-squares is at its smallest, is called the line of best fit. This line acts as a model for our data.\nFor finch 473 we have the following values:\n\nthe observed beak depth is 9.5 mm\nthe observed beak length is 10.5 mm\nthe fitted value is 9.11 mm\nthe error is 0.39 mm\n\n5.3.2 Linear regression\nOnce we have the line of best fit, we can perform a linear regression. What we’re doing with the regression, is asking:\n\nIs the line of best fit a better predictor of our data than a horizontal line across the average value?\n\nVisually, that looks like this:\n\n\n\n\n\n\n\nFigure 5.5: Regression: is the slope different from zero?\n\n\n\n\nWhat we’re actually testing is whether the slope (\\(\\beta_1\\)) of the line of best fit is any different from zero.\nTo find the answer, we perform an ANOVA. This gives us a p-value of 1.68e-78.\nNeedless to say, this p-value is extremely small, and definitely smaller than any common significance threshold, such as \\(p &lt; 0.05\\). This suggests that beak length is a statistically significant predictor of beak depth.\nIn this case the model has an intercept (\\(\\beta_0\\)) of -0.34 and a slope (\\(\\beta_1\\)) of 0.9. We can use this to write a simple linear equation, describing our data. Remember that this takes the form of:\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\nwhich in our case is\n\\[\nbeak\\ depth = \\beta_0 + \\beta_1 \\times beak\\ length\n\\]\nand gives us\n\\[\nbeak\\ depth = -0.34 + 0.90 \\times beak\\ length\n\\]\n\n5.3.3 Assumptions\nIn example above we just got on with things once we suspected that there was a linear relationship between beak depth and beak length. However, for the linear regression to be valid, several assumptions need to be met. If any of those assumptions are violated, we can’t trust the results. The following four assumptions need to be met, with a 5th point being a case of good scientific practice:\n\nData should be linear\nResiduals are normally distributed\nEquality of variance\nThe residuals are independent\n(no influential points)\n\nAs we did many times during the Core statistics sessions, we mainly rely on diagnostic plots to check these assumptions. For this particular model they look as follows:\n\n\n\n\n\n\n\nFigure 5.6: Diagnostic plots for G. fortis (1975) model\n\n\n\n\nThese plots look very good to me. For a recap on how to interpret these plots, see CS2: ANOVA.\nTaken together, we can see the relationship between beak depth and beak length as a linear one, described by a (linear) model that has a predicted value for each data point, and an associated error.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Linear models</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html",
    "href": "materials/glm-intro-glm.html",
    "title": "\n6  Generalise your model\n",
    "section": "",
    "text": "6.1 Putting the “G” into GLM\nIn the previous linear model example all the assumptions were met. But what if we have data where that isn’t the case? For example, what if we have data where we can’t describe the relationship between the predictor and response variables in a linear way?\nOne of the ways we can deal with this is by using a generalised linear model, also abbreviated as GLM. In a way it’s an extension of the linear model we discussed in the previous section. As with the normal linear model, the predictor variables in the model are in a linear combination, such as:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nHere, the \\(\\beta_0\\) value is the constant or intercept, whereas each subsequent \\(\\beta_i\\) is a unique regression coefficient for each \\(X_i\\) predictor variable. So far so good.\nHowever, the GLM makes the linear model more flexible in two ways:\nWe’ll introduce each of these elements below, then illustrate how they are used in practice, using different types of data.\nThe link function and different distributions are closely…err, linked. To make sense of what the link function is doing it’s useful to understand the different distributional assumptions. So we’ll start with those.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "href": "materials/glm-intro-glm.html#putting-the-g-into-glm",
    "title": "\n6  Generalise your model\n",
    "section": "",
    "text": "Important\n\n\n\n\nIn a standard linear model the linear combination (e.g. like we see above) becomes the predicted outcome value. With a GLM a transformation is specified, which turns the linear combination into the predicted outcome value. This is called a link function.\nA standard linear model assumes a continuous, normally distributed outcome, whereas with GLM the outcome can be both continuous or integer. Furthermore, the outcome does not have to be normally distributed. Indeed, the outcome can follow a different kind of distribution, such as binomial, Poisson, exponential etc.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#distributions",
    "href": "materials/glm-intro-glm.html#distributions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.2 Distributions",
    "text": "6.2 Distributions\nIn the examples of a standard linear model we’ve seen that the residuals needed to be normally distributed. We’ve mainly used the Q-Q plot to assess this assumption of normality.\nBut what does “normal” actually mean? It assumes that the residuals are coming from a normal or Gaussian distribution. This distribution has a symmetrical bell-shape, where the centre is the mean, and half of the data are on either side of this.\nWe can see this in Figure 6.1. The mean of the normal distribution is indicated with the dashed blue line.\n\n\n\n\n\n\n\nFigure 6.1: Normal distribution\n\n\n\n\nWe can use the linear model we created previously, where we looked at the possible linear relationship between beak depth and beak length. This is based on measurements of G. fortis beaks in 1975.\nThe individual values of the residuals from this linear model are shown in Figure 6.1, panel B (in red), with the corresponding theoretical normal distribution in the background. We can see that the residuals follow this distribution reasonably well, which matches our conclusions from the Q-Q plot (see Figure 5.6).\nAll this means is that assuming that these residuals may come from a normal distribution isn’t such a daft suggestion after all.\nNow look at the example in Figure 6.2. This shows the classification of beak shape for a number of finches. Their beaks are either classed as blunt or pointed. Various (continuous) measurements were taken from each bird, with the beak length shown here.\n\n\n\n\n\n\n\nFigure 6.2: Classification in beak shape\n\n\n\n\nWe’ll look into this example in more detail later. For now it’s important to note that the response variable (the beak shape classification) is not continuous. Here it is a binary response (blunt or pointed). As a result, the assumptions for a regular linear model go out of the window. If we were foolish enough to fit a linear model to these data (see blue line in A), then the residuals would look rather non-normal (Figure 6.2 B).\nSo what do we do? Well, the normal distribution is not the only one there is. In Figure 6.3 there are a few examples of distributions (including the normal one).\n\n\n\n\n\n\n\nFigure 6.3: Different distributions\n\n\n\n\nDifferent distributions are useful for different types of data. For example, a logistic distribution is particularly useful in the context of binary or proportional response data. The Poisson distribution is useful when we have count data as a response.\nIn order to understand how this can help us, we need to be aware of two more concepts: linear predictors and link functions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#linear-predictors",
    "href": "materials/glm-intro-glm.html#linear-predictors",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.3 Linear predictors",
    "text": "6.3 Linear predictors\nThe nice thing about linear models is that the predictors are, well, linear. Straight lines make for easy interpretation of any potential relationship between predictor and response.\nAs mentioned before, predictors are in the form of a linear combination, where each predictor variable is multiplied by a coefficient and all the terms are added together:\n\\[\n\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ...\n\\]\nFortunately, this is no different for generalised linear models! We still have a linear combination but, as we’ll see, if the relationship is not linear then we need an additional step before we can model the data in this way.\nAt this point, we have two options at our disposal (well, there are more, but let’s not muddy the waters too much).\n\n\n\n\n\n\nImportant\n\n\n\n\nTransform our data and use a normal linear model on the transformed data\nTransform the linear predictor\n\n\n\nThe first option, to transform our data, seems like a useful option and can work. It keeps things familiar (we’d still use a standard linear model) and so all is well with the world. Up to the point of interpreting the data. If we, for example, log-transform our data, how do we interpret this? After all, the predictions of the linear model are directly related to the outcome or response variable. Transforming the data is usually done so that the residuals of the linear model resemble a more normal distribution. An unwanted side-effect of this is that this also changes the ratio scale properties of the measured variables (Stevens 1946).\nThe second option would be to transform the linear predictor. This enables us to map a non-linear outcome (or response variable) to a linear model. This transformation is done using a link function.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#link-functions",
    "href": "materials/glm-intro-glm.html#link-functions",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.4 Link functions",
    "text": "6.4 Link functions\nSimply put: link functions connect the predictors in our model to the response variables in a linear way.\nHowever, and similar to the standard linear model, there are two parts to each model:\n\nthe coefficients for each predictor (linking each parameter to a predictor)\nthe error or random component (which specifies a probability distribution)\n\nWhich link function you use depends on your analysis. Some common link functions and corresponding distributions are (adapted from (Glen 2021)):\n\n\ndistribution\ndata type\nlink name\n\n\n\nbinomial\nbinary / proportion\nlogit\n\n\nnormal\nany real number\nidentity\n\n\npoisson\ncount data\nlog\n\n\n\nLet’s again look at the earlier example of beak shape.\n\n\n\n\n\n\n\nFigure 6.4: Beak shape classification\n\n\n\n\nWe’ve seen the data in Figure 6.4 A before, where we had information on what beak shape our observed finches had, plotted against their beak length.\nLet’s say we now want to make some predictions about what beak shape we would expect, given a certain beak length. In this scenario we’d need some way of modelling the response variable (beak shape; blunt or pointed) as a function of the predictor variable (beak length).\nThe issue we have is that the response variable is not continuous, but binary! We could fit a standard linear model to these data (blue line in Figure 6.2 A) but this is really bad practice. Why? Well, what such a linear model represents is the probability - or how likely it is - that an observed finch has a pointed beak, given a certain beak length (Figure 6.4 B).\nSimply fitting a linear line through those data suggests that it is possible to have a higher than 1 and lower-than-zero probability that a beak would be pointed! That, of course, makes no sense. So, we can’t describe these data as a linear relationship.\nInstead, we’ll use a logistic model to analyse these data. We’ll cover the practicalities of how to do this in more detail in a later chapter, but for now it’s sufficient to realise that one of the ways we could model these data could look like this:\n\n\n\n\n\n\n\nFigure 6.5: Logistic model for beak classification\n\n\n\n\nUsing this sigmoidal curve ensures that our predicted probabilities do not exceed the \\([0, 1]\\) range.\nNow, what happened behind the scenes is that the generalised linear model has taken the linear predictor and transformed it using the logit link function. This links the non-linear response variable (beak shape) to a linear model, using beak length as a predictor.\nWe’ll practice how to perform this analysis in the next section.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-intro-glm.html#key-points",
    "href": "materials/glm-intro-glm.html#key-points",
    "title": "\n6  Generalise your model\n",
    "section": "\n6.5 Key points",
    "text": "6.5 Key points\n\n\n\n\n\n\nNote\n\n\n\n\nGLMs allow us to map a non-linear outcome to a linear model\nThe link function determines how this occurs, transforming the linear predictor\n\n\n\n\n\n\n\nGlen, Stephanie. 2021. “Link Function.” Statistics How To: Elementary Statistics for the Rest of Us! https://www.statisticshowto.com/link-function/.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generalise your model</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html",
    "href": "materials/glm-practical-logistic-binary.html",
    "title": "\n7  Binary response\n",
    "section": "",
    "text": "7.1 Libraries and functions\nThe example in this section uses the following data set:\ndata/finches_early.csv\nThese data come from an analysis of gene flow across two finch species (Lamichhaney et al. 2020). They are slightly adapted here for illustrative purposes.\nThe data focus on two species, Geospiza fortis and G. scandens. The original measurements are split by a uniquely timed event: a particularly strong El Niño event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as early (pre-1983) and late (1983 onwards).\nHere we are looking only at the early data. We are specifically focussing on the beak shape classification, which we saw earlier in Figure 6.5.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#libraries-and-functions",
    "href": "materials/glm-practical-logistic-binary.html#libraries-and-functions",
    "title": "\n7  Binary response\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n7.1.1 Libraries\n\n7.1.2 Functions\n\n\n\n\n7.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n\n7.1.4 Functions",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "href": "materials/glm-practical-logistic-binary.html#load-and-visualise-the-data",
    "title": "\n7  Binary response\n",
    "section": "\n7.2 Load and visualise the data",
    "text": "7.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nearly_finches &lt;- read_csv(\"data/finches_early.csv\")\n\n\n\n\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n\n\n\n\nLooking at the data, we can see that the pointed_beak column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the pointed_beak data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a pointed beak (1), and those with a blunt one (0).\nWe can force Python to temporarily covert the data to a factor, by making the pointed_beak column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea…\n\n\nR\nPython\n\n\n\n\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n(ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\n\n\n\n\n\n\n\n\n\nOf course this is rubbish - we can’t have a beak classification outside the range of \\([0, 1]\\). It’s either blunt (0) or pointed (1).\nBut for the sake of exploration, let’s look at the assumptions:\n\n\nR\nPython\n\n\n\n\nlm_bks &lt;- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n\n\n\n\n\n\n\n\n\nFirst, we create a linear model:\n\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n\nNext, we can create the diagnostic plots:\n\ndgplots(lm_bks_py)\n\n\n\n\n\n\n\n\n\n\n\n\nThey’re pretty extremely bad.\n\nThe response is not linear (Residual Plot, binary response plot, common sense).\nThe residuals do not appear to be distributed normally (Q-Q Plot)\nThe variance is not homogeneous across the predicted values (Location-Scale Plot)\nBut - there is always a silver lining - we don’t have influential data points.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#creating-a-suitable-model",
    "href": "materials/glm-practical-logistic-binary.html#creating-a-suitable-model",
    "title": "\n7  Binary response\n",
    "section": "\n7.3 Creating a suitable model",
    "text": "7.3 Creating a suitable model\nSo far we’ve established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what can we do?\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the \\([0, 1]\\) range.\nWe described our standard linear relationship as follows:\n\\(Y = \\beta_0 + \\beta_1X\\)\nWe can now map this to our non-linear relationship via the logistic link function:\n\\(Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}\\)\nNote that the \\(\\beta_0 + \\beta_1X\\) part is identical to the formula of a straight line.\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n\n\n\n\n\nEuler’s number (\\(\\exp\\)): would you like to know more?\n\n\n\n\n\nIn mathematics, \\(\\rm e\\) represents a constant of around 2.718. Another notation is \\(\\exp\\), which is often used when notations become a bit cumbersome. Here, I exclusively use the \\(\\exp\\) notation for consistency.\n\n\n\n\n\n\n\n\n\nThe logistic function\n\n\n\nThe shape of the logistic function is hugely influenced by the different parameters, in particular \\(\\beta_1\\). The plots below show different situations, where \\(\\beta_0 = 0\\) in all cases, but \\(\\beta_1\\) varies.\nThe first plot shows the logistic function in its simplest form, with the others showing the effect of varying \\(\\beta_1\\).\n\n\n\n\n\n\n\n\n\nwhen \\(\\beta_1 = 1\\), this gives the simplest logistic function\nwhen \\(\\beta_1 = 0\\) gives a horizontal line, with \\(Y = \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\)\n\nwhen \\(\\beta_1\\) is negative flips the curve around, so it slopes down\nwhen \\(\\beta_1\\) is very large then the curve becomes extremely steep\n\n\n\nWe can fit such an S-shaped curve to our early_finches data set, by creating a generalised linear model.\n\n\nR\nPython\n\n\n\nIn R we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks:\n\nglm_bks &lt;- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\nIf you forget to set the family argument, then the glm() function will perform a standard linear model fit, identical to what the lm() function would do.\n\n\nIn Python we have a few options to do this, and by far the most familiar function would be glm(). Here we save the model in an object called glm_bks_py:\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()\n\nThe format of this function is similar to that used by the ols() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial. This is buried deep inside the statsmodels package and needs to be defined as sm.families.Binomial().",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#model-output",
    "href": "materials/glm-practical-logistic-binary.html#model-output",
    "title": "\n7  Binary response\n",
    "section": "\n7.4 Model output",
    "text": "7.4 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\n\nR\nPython\n\n\n\n\nsummary(glm_bks)\n\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\nprint(glm_bks_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Thu, 01 Feb 2024   Deviance:                       9.1879\nTime:                        07:29:39   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n\n\n\n\n\nThere’s a lot to unpack here, but let’s start with what we’re familiar with: coefficients!",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#parameter-interpretation",
    "href": "materials/glm-practical-logistic-binary.html#parameter-interpretation",
    "title": "\n7  Binary response\n",
    "section": "\n7.5 Parameter interpretation",
    "text": "7.5 Parameter interpretation\n\n\nR\nPython\n\n\n\nThe coefficients or parameters can be found in the Coefficients block. The main numbers to extract from the output are the two numbers underneath Estimate.Std:\nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the coef column:\n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n\n\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\nThe \\(p\\) values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the \\(p\\) values obtained in the summary output of a linear model. As with continuous predictors in simple models, these \\(p\\) values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these \\(p\\) values aren’t great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a \\(p\\) value for each level rather than for the predictor as a whole).\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\\[ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} \\]\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\n\nR\nPython\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n\n[1] 0.9994131\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n\n[1] 7.410155e-05\n\n\n\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n# import the math library\nimport math\n\n\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n\n0.9994130595039192\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n\n7.410155028945912e-05\n\n\n\n\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I’m visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n\n\n\n\n\nCode available here\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nglm_bks %&gt;% \n  augment(type.predict = \"response\") %&gt;% \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n\n\n\n\n(ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Predicted probabilities for beak classification\n\n\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It’s also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#assumptions",
    "href": "materials/glm-practical-logistic-binary.html#assumptions",
    "title": "\n7  Binary response\n",
    "section": "\n7.6 Assumptions",
    "text": "7.6 Assumptions\nAs explained in the background chapter, we can’t really use the standard diagnostic plots to assess assumptions. We’re not going to go into a lot of detail for now, but there is one thing that we can do: look for influential points using the Cook’s distance plot.\n\n\nR\nPython\n\n\n\n\nplot(glm_bks , which = 4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting the Cook’s distances from the glm object\n\n\n\n\n\nInstead of using the plot() function, we can also extract the values directly from the glm object. We can use the augment() function to do this and create a lollipop or stem plot:\n\nglm_bks %&gt;% \n  augment() %&gt;%            # get underlying data\n  select(.cooksd) %&gt;%      # select the Cook's d\n  mutate(obs = 1:n()) %&gt;%  # create an index column\n  ggplot(aes(x = obs, y = .cooksd)) +\n  geom_point() +\n  geom_segment(aes(x = obs, y = .cooksd, xend = obs, yend = 0))\n\n\n\n\n\n\n\n\n\n\n\n\nAs always, there are different ways of doing this. Here we extract the Cook’s d values from the glm object and put them in a Pandas DataFrame. We can then use that to plot them in a lollipop or stem plot.\n\n# extract the Cook's distances\nglm_bks_py_resid = pd.DataFrame(glm_bks_py.\n                                get_influence().\n                                summary_frame()[\"cooks_d\"])\n\n# add row index \nglm_bks_py_resid['obs'] = glm_bks_py_resid.reset_index().index\n\nWe now have two columns:\n\nglm_bks_py_resid\n\n         cooks_d  obs\n0   1.854360e-07    0\n1   3.388262e-07    1\n2   3.217960e-05    2\n3   1.194847e-05    3\n4   6.643975e-06    4\n..           ...  ...\n56  1.225519e-05   56\n57  2.484468e-05   57\n58  6.781364e-06   58\n59  1.850240e-07   59\n60  3.532360e-05   60\n\n[61 rows x 2 columns]\n\n\nWe can use these to create the plot:\n\n(ggplot(glm_bks_py_resid,\n         aes(x = \"obs\",\n             y = \"cooks_d\")) +\n     geom_segment(aes(x = \"obs\", y = \"cooks_d\", xend = \"obs\", yend = 0)) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nThis shows that there are no very obvious influential points. You could regard point 34 as potentially influential (it’s got a Cook’s distance of around 0.8), but I’m not overly worried.\nIf we were worried, we’d remove the troublesome data point, re-run the analysis and see if that changes the statistical outcome. If so, then our entire (statistical) conclusion hinges on one data point, which is not a very robust bit of research. If it doesn’t change our significance, then all is well, even though that data point is influential.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#assessing-significance",
    "href": "materials/glm-practical-logistic-binary.html#assessing-significance",
    "title": "\n7  Binary response\n",
    "section": "\n7.7 Assessing significance",
    "text": "7.7 Assessing significance\nWe can ask several questions.\nIs the model well-specified?\nRoughly speaking this asks “can our model predict our data reasonably well?”\n\n\nR\nPython\n\n\n\nUnfortunately, there isn’t a single command that does this for us, and we have to lift some of the numbers from the summary output ourselves.\n\npchisq(9.1879, 59, lower.tail = FALSE)\n\n[1] 1\n\n\nHere, we’ve used the pchisq function (which calculates the correct probability for us – ask if you want a hand-wavy explanation). The first argument to it is the residual deviance value from the summary table, the second argument to is the residual degrees of freedom argument from the same table.\n\n\n\nfrom scipy.stats import chi2\n\nchi2.sf(9.1879, 59)\n\n0.9999999999999916\n\n\n\n\n\nThis gives us a probability of 1. We can interpret this as the probability that the model is actually good. There aren’t any strict conventions on how to interpret this value but, for me, a tiny value would indicate a rubbish model.\nIs the overall model better than the null model?\n\n\nR\nPython\n\n\n\n\npchisq(84.5476 - 9.1879, 60 - 59, lower.tail = FALSE)\n\n[1] 3.923163e-18\n\n\nHere we’ve used the pchisq function again (if you didn’t ask before, you probably aren’t going to ask now).\n\n\nFirst we need to define the null model:\n\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ 1\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_null_py = model.fit()\n\nprint(glm_bks_null_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       60\nModel Family:                Binomial   Df Model:                            0\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -42.274\nDate:                Thu, 01 Feb 2024   Deviance:                       84.548\nTime:                        07:29:43   Pearson chi2:                     61.0\nNo. Iterations:                     3   Pseudo R-squ. (CS):              0.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0328      0.256      0.128      0.898      -0.469       0.535\n==============================================================================\n\n\nIn order to compare our original fitted model to the null model we need to know the deviances of both models and the residual degrees of freedom of both models, which we could get from the summary method.\n\nchi2.sf(84.5476 - 9.1879, 60 - 59)\n\n3.9231627082752525e-18\n\n\n\n\n\nThe first argument is the difference between the null and residual deviances and the second argument is the difference in degrees of freedom between the null and residual models. All of these values can be lifted from the summary table.\nThis gives us a probability of pretty much zero. This value is doing a formal test to see whether our fitted model is significantly different from the null model. Here we can treat this a classical hypothesis test and since this p-value is less than 0.05 then we can say that our fitted model (with blength as a predictor variable) is definitely better than the null model (which has no predictor variables). Woohoo!\nAre any of the individual predictors significant?\nFinally, we’ll use the anova function from before to determine which predictor variables are important, and specifically in this case whether the beak length predictor is significant.\n\n\nR\nPython\n\n\n\n\nanova(glm_bks, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: pointed_beak\n\nTerms added sequentially (first to last)\n\n        Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                       60     84.548              \nblength  1    75.36        59      9.188 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe anova() function is a true workhorse within R! This time we’ve used it to create an Analysis of Deviance table. This is exactly equivalent to an ordinary ANOVA table where we have rows corresponding to each predictor variable and a p-value telling us whether that variable is significant or not.\nThe p-value for the blength predictor is written under then Pr(&gt;Chi) column and we can see that it is less than &lt; 2.2e-16. So, beak length is a significant predictor.\nThis shouldn’t be surprising since we already saw that our overall model was better than the null model, which in this case is exactly the same as asking whether the beak length term is significant. However, in more complicated models with multiple predictors these two comparisons (and p-values) won’t be the same.\n\n\nAlas, for some inexplicable reason this is not (yet?) possible to do in Python. At least, unbeknownst to me…",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#exercises",
    "href": "materials/glm-practical-logistic-binary.html#exercises",
    "title": "\n7  Binary response\n",
    "section": "\n7.8 Exercises",
    "text": "7.8 Exercises\n\n7.8.1 Diabetes\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/diabetes.csv.\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called test_result, glucose and diastolic.\nWe want to see if the glucose tolerance is a meaningful predictor for predictions on a diabetes test. To investigate this, do the following:\n\nLoad and visualise the data\nCreate a suitable model\nDetermine if there are any statistically significant predictors\nCalculate the probability of a positive diabetes test result for a glucose tolerance test value of glucose = 150\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\n\n\n\nLooking at the data, we can see that the test_result column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\nWe’ll have to deal with this soon. For now, we can plot the data, by outcome:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe could just give Python the test_result data directly, but then it would view the values as numeric. Which doesn’t really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\nWe can force Python to temporarily covert the data to a factor, by making the test_result column an object type. We can do this directly inside the ggplot() function.\n\n(ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n\nR\nPython\n\n\n\n\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nCreate a suitable model\n\n\nR\nPython\n\n\n\nWe’ll use the glm() function to create a generalised linear model. Here we save the model in an object called glm_dia:\n\nglm_dia &lt;- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n\nThe format of this function is similar to that used by the lm() function for linear models. The important difference is that we must specify the family of error distribution to use. For logistic regression we must set the family to binomial.\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n\n\n\n\nLet’s look at the model parameters:\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.611732   0.442289  -12.69   &lt;2e-16 ***\nglucose      0.039510   0.003398   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Thu, 01 Feb 2024   Deviance:                       752.20\nTime:                        07:29:45   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n\n\n\n\n\nWe can see that glucose is a significant predictor for the test_result (the \\(p\\) value is much smaller than 0.05).\nKnowing this, we’re interested in the coefficients. We have an intercept of -5.61 and 0.0395 for glucose. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\\[ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} \\]\nCalculating probabilities\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given glucose value. If we do this for glucose = 150, we get the following:\n\n\nR\nPython\n\n\n\n\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 150))\n\n[1] 0.5962827\n\n\n\n\n\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 150))\n\n0.5962826992967878\n\n\n\n\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 60 %.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-binary.html#summary",
    "href": "materials/glm-practical-logistic-binary.html#summary",
    "title": "\n7  Binary response\n",
    "section": "\n7.9 Summary",
    "text": "7.9 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe use a logistic regression to model a binary response\nWe can feed new observations into the model and get probabilities for the outcome\n\n\n\n\n\n\n\nLamichhaney, Sangeet, Fan Han, Matthew T. Webster, B. Rosemary Grant, Peter R. Grant, and Leif Andersson. 2020. “Female-Biased Gene Flow Between Two Species of Darwin’s Finches.” Nature Ecology & Evolution 4 (7): 979–86. https://doi.org/10.1038/s41559-020-1183-9.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Binary response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html",
    "href": "materials/glm-practical-logistic-proportion.html",
    "title": "\n8  Proportional response\n",
    "section": "",
    "text": "8.1 Libraries and functions\nThe example in this section uses the following data set:\ndata/challenger.csv\nThese data, obtained from the faraway package in R, contain information related to the explosion of the USA Space Shuttle Challenger on 28 January, 1986. An investigation after the disaster traced back to certain joints on one of the two solid booster rockets, each containing O-rings that ensured no exhaust gases could escape from the booster.\nThe night before the launch was unusually cold, with temperatures below freezing. The final report suggested that the cold snap during the night made the o-rings stiff, and unable to adjust to changes in pressure. As a result, exhaust gases leaked away from the solid booster rockets, causing one of them to break loose and rupture the main fuel tank, leading to the final explosion.\nThe question we’re trying to answer in this session is: based on the data from the previous flights, would it have been possible to predict the failure of most o-rings on the Challenger flight?",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#libraries-and-functions",
    "href": "materials/glm-practical-logistic-proportion.html#libraries-and-functions",
    "title": "\n8  Proportional response\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n8.1.1 Libraries\n\n8.1.2 Functions\n\n\n\n\n8.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n\n8.1.4 Functions",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#load-and-visualise-the-data",
    "href": "materials/glm-practical-logistic-proportion.html#load-and-visualise-the-data",
    "title": "\n8  Proportional response\n",
    "section": "\n8.2 Load and visualise the data",
    "text": "8.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nchallenger &lt;- read_csv(\"data/challenger.csv\")\n\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nchallenger_py = pd.read_csv(\"data/challenger.csv\")\n\n\n\n\nThe data set contains several columns:\n\n\ntemp, the launch temperature in degrees Fahrenheit\n\ndamage, the number of o-rings that showed erosion\n\nBefore we have a further look at the data, let’s calculate the proportion of damaged o-rings (prop_damaged) and the total number of o-rings (total) and update our data set.\n\n\nR\nPython\n\n\n\n\nchallenger &lt;-\nchallenger %&gt;%\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nchallenger\n\n# A tibble: 23 × 5\n    temp damage total intact prop_damaged\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1    53      5     6      1        0.833\n 2    57      1     6      5        0.167\n 3    58      1     6      5        0.167\n 4    63      1     6      5        0.167\n 5    66      0     6      6        0    \n 6    67      0     6      6        0    \n 7    67      0     6      6        0    \n 8    67      0     6      6        0    \n 9    68      0     6      6        0    \n10    69      0     6      6        0    \n# ℹ 13 more rows\n\n\n\n\n\nchallenger_py['total'] = 6\nchallenger_py['intact'] = challenger_py['total'] - challenger_py['damage']\nchallenger_py['prop_damaged'] = challenger_py['damage'] / challenger_py['total']\n\n\n\n\nPlotting the proportion of damaged o-rings against the launch temperature shows the following picture:\n\n\nR\nPython\n\n\n\n\nggplot(challenger, aes(x = temp, y = prop_damaged)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nThe point on the left is the data point corresponding to the coldest flight experienced before the disaster, where five damaged o-rings were found. Fortunately, this did not result in a disaster.\nHere we’ll explore if we could have reasonably predicted the failure of both o-rings on the Challenger flight, where the launch temperature was 31 degrees Fahrenheit.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#creating-a-suitable-model",
    "href": "materials/glm-practical-logistic-proportion.html#creating-a-suitable-model",
    "title": "\n8  Proportional response\n",
    "section": "\n8.3 Creating a suitable model",
    "text": "8.3 Creating a suitable model\nWe only have 23 data points in total. So we’re building a model on not that much data - we should keep this in mind when we draw our conclusions!\nWe are using a logistic regression for a proportion response in this case, since we’re interested in the proportion of o-rings that are damaged.\nWe can define this as follows:\n\n\nR\nPython\n\n\n\n\nglm_chl &lt;- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger)\n\nDefining the relationship for proportion responses is a bit annoying, where you have to give the glm model a two-column matrix to specify the response variable.\nHere, the first column corresponds to the number of damaged o-rings, whereas the second column refers to the number of intact o-rings. We use the cbind() function to bind these two together into a matrix.\n\n\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_py = model.fit()",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#model-output",
    "href": "materials/glm-practical-logistic-proportion.html#model-output",
    "title": "\n8  Proportional response\n",
    "section": "\n8.4 Model output",
    "text": "8.4 Model output\nThat’s the easy part done! The trickier part is interpreting the output. First of all, we’ll get some summary information.\n\n\nR\nPython\n\n\n\nNext, we can have a closer look at the results:\n\nsummary(glm_chl)\n\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe can see that the p-values of the intercept and temp are significant. We can also use the intercept and temp coefficients to construct the logistic equation, which we can use to sketch the logistic curve.\n\n\n\nprint(glm_chl_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -14.837\nDate:                  Tue, 06 Feb 2024   Deviance:                       16.912\nTime:                          16:12:12   Pearson chi2:                     28.1\nNo. Iterations:                       7   Pseudo R-squ. (CS):             0.6155\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.6630      3.296      3.538      0.000       5.202      18.124\ntemp          -0.2162      0.053     -4.066      0.000      -0.320      -0.112\n==============================================================================\n\n\n\n\n\n\\[E(prop \\ failed\\ orings) = \\frac{\\exp{(11.66 -  0.22 \\times temp)}}{1 + \\exp{(11.66 -  0.22 \\times temp)}}\\]\nLet’s see how well our model would have performed if we would have fed it the data from the ill-fated Challenger launch.\n\n\nR\nPython\n\n\n\n\nggplot(challenger, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85)\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\n\n\nWe can get the predicted values for the model as follows:\n\nchallenger_py['predicted_values'] = glm_chl_py.predict()\n\nchallenger_py.head()\n\n   temp  damage  total  intact  prop_damaged  predicted_values\n0    53       5      6       1      0.833333          0.550479\n1    57       1      6       5      0.166667          0.340217\n2    58       1      6       5      0.166667          0.293476\n3    63       1      6       5      0.166667          0.123496\n4    66       0      6       6      0.000000          0.068598\n\n\nThis would only give us the predicted values for the data we already have. Instead we want to extrapolate to what would have been predicted for a wider range of temperatures. Here, we use a range of \\([25, 85]\\) degrees Fahrenheit.\n\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_py.predict(model)\n\nmodel.head()\n\n   temp      pred\n0    25  0.998087\n1    26  0.997626\n2    27  0.997055\n3    28  0.996347\n4    29  0.995469\n\n\n\n(ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating predicted values\n\n\n\n\n\n\n\nR\nPython\n\n\n\nAnother way of doing this it to generate a table with data for a range of temperatures, from 25 to 85 degrees Fahrenheit, in steps of 1. We can then use these data to generate the logistic curve, based on the fitted model.\n\n# create a table with sequential numbers ranging from 25 to 85\nmodel &lt;- tibble(temp = seq(25, 85, by = 1)) %&gt;% \n  # add a new column containing the predicted values\n  mutate(.pred = predict(glm_chl, newdata = ., type = \"response\"))\n\nggplot(model, aes(temp, .pred)) +\n  geom_line()\n\n\n\n\n\n\n\n\n# plot the curve and the original data\nggplot(model, aes(temp, .pred)) +\n  geom_line(colour = \"blue\") +\n  geom_point(data = challenger, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")\n\n\n\n\n\n\n\nIt seems that there was a high probability of both o-rings failing at that launch temperature. One thing that the graph shows is that there is a lot of uncertainty involved in this model. We can tell, because the fit of the line is very poor at the lower temperature range. There is just very little data to work on, with the data point at 53 F having a large influence on the fit.\n\n\nWe already did this above, since this is the most straightforward way of plotting the model in Python.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#exercises",
    "href": "materials/glm-practical-logistic-proportion.html#exercises",
    "title": "\n8  Proportional response\n",
    "section": "\n8.5 Exercises",
    "text": "8.5 Exercises\n\n8.5.1 Predicting failure\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nThe data point at 53 degrees Fahrenheit is quite influential for the analysis. Remove this data point and repeat the analysis. Is there still a predicted link between launch temperature and o-ring failure?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nFirst, we need to remove the influential data point:\n\nchallenger_new &lt;- challenger %&gt;% filter(temp != 53)\n\nWe can create a new generalised linear model, based on these data:\n\nglm_chl_new &lt;- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger_new)\n\nWe can get the model parameters as follows:\n\nsummary(glm_chl_new)\n\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger_new)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  5.68223    4.43138   1.282   0.1997  \ntemp        -0.12817    0.06697  -1.914   0.0556 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.375  on 21  degrees of freedom\nResidual deviance: 12.633  on 20  degrees of freedom\nAIC: 27.572\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nggplot(challenger_new, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85) +\n  # add a vertical line at 53 F temperature\n  geom_vline(xintercept = 53, linetype = \"dashed\")\n\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n\n\n\n\n\n\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\nBefore we can make any firm conclusions, though, we need to check our model:\n\n1- pchisq(12.633,20)\n\n[1] 0.8925695\n\n\nWe get quite a high score (around 0.9) for this, which tells us that our goodness of fit is pretty rubbish – our points are not very close to our curve, overall.\nIs the model any better than the null though?\n\n1 - pchisq(16.375 - 12.633, 21 - 20)\n\n[1] 0.0530609\n\nanova(glm_chl_new, test = 'Chisq')\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(damage, intact)\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  \nNULL                    21     16.375           \ntemp  1   3.7421        20     12.633  0.05306 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHowever, the model is not significantly better than the null in this case, with a p-value here of just over 0.05 for both of these tests (they give a similar result since, yet again, we have just the one predictor variable).\n\n\nFirst, we need to remove the influential data point:\n\nchallenger_new_py = challenger_py.query(\"temp != 53\")\n\nWe can create a new generalised linear model, based on these data:\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_py = model.fit()\n\nWe can get the model parameters as follows:\n\nprint(glm_chl_new_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       20\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -11.786\nDate:                  Tue, 06 Feb 2024   Deviance:                       12.633\nTime:                          16:12:15   Pearson chi2:                     16.6\nNo. Iterations:                       6   Pseudo R-squ. (CS):             0.1564\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.6822      4.431      1.282      0.200      -3.003      14.368\ntemp          -0.1282      0.067     -1.914      0.056      -0.259       0.003\n==============================================================================\n\n\nGenerate new model data:\n\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_new_py.predict(model)\n\nmodel.head()\n\n   temp      pred\n0    25  0.922585\n1    26  0.912920\n2    27  0.902177\n3    28  0.890269\n4    29  0.877107\n\n\n\n(ggplot(challenger_new_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1) +\n     # add a vertical line at 53 F temperature\n     geom_vline(xintercept = 53, linetype = \"dashed\"))\n\n\n\n\n\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\nBefore we can make any firm conclusions, though, we need to check our model:\n\nchi2.sf(12.633, 20)\n\n0.8925694610786307\n\n\nWe get quite a high score (around 0.9) for this, which tells us that our goodness of fit is pretty rubbish – our points are not very close to our curve, overall.\nIs the model any better than the null though?\nFirst we need to define the null model:\n\n# create a linear model\nmodel = smf.glm(formula = \"damage + intact ~ 1\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_null_py = model.fit()\n\nprint(glm_chl_new_null_py.summary())\n\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            0\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -13.657\nDate:                  Tue, 06 Feb 2024   Deviance:                       16.375\nTime:                          16:12:16   Pearson chi2:                     16.8\nNo. Iterations:                       6   Pseudo R-squ. (CS):         -2.220e-16\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.0445      0.418     -7.286      0.000      -3.864      -2.226\n==============================================================================\n\n\n\nchi2.sf(16.375 - 12.633, 21 - 20)\n\n0.053060897703157646\n\n\nHowever, the model is not significantly better than the null in this case, with a p-value here of just over 0.05 for both of these tests (they give a similar result since, yet again, we have just the one predictor variable).\n\n\n\nSo, could NASA have predicted what happened? This model is not significantly different from the null, i.e., temperature is not a significant predictor. Note that it’s only marginally non-significant, and we do have a high goodness-of-fit value.\nIt is possible that if more data points were available that followed a similar trend, the story might be different). Even if we did use our non-significant model to make a prediction, it doesn’t give us a value anywhere near 5 failures for a temperature of 53 degrees Fahrenheit. So overall, based on the model we’ve fitted with these data, there was no indication that a temperature just a few degrees cooler than previous missions could have been so disastrous for the Challenger.",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-logistic-proportion.html#summary",
    "href": "materials/glm-practical-logistic-proportion.html#summary",
    "title": "\n8  Proportional response\n",
    "section": "\n8.6 Summary",
    "text": "8.6 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nWe can use a logistic model for proportion response variables",
    "crumbs": [
      "Binary outcomes",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proportional response</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html",
    "href": "materials/significance-testing.html",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "",
    "text": "9.1 Libraries and functions",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html#libraries-and-functions",
    "href": "materials/significance-testing.html#libraries-and-functions",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\ninstall.packages(\"lmtest\")\nlibrary(lmtest)\n\n\n\n\nfrom scipy.stats import *",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html#deviance",
    "href": "materials/significance-testing.html#deviance",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "\n9.2 Deviance",
    "text": "9.2 Deviance\nSeveral of the tests and metrics we’ll discuss below are based heavily on deviance. So, what is deviance, and where does it come from?\nFitting a model using maximum likelihood estimation - the method that we use for GLMs, among other models - is all about finding the parameters that maximise the likelihood, or joint probability, of the sample. In other words, how likely is it that you would sample a set of data points like these, if they were being drawn from an underlying population where your model is true? Each model that you fit has its own likelihood.\nNow, for each dataset, there is a “saturated”, or perfect, model. This model has the same number of parameters in it as there are data points, meaning the data are fitted exactly - as if connecting the dots between them. The saturated model has the largest possible likelihood of any model fitted to the dataset.\nOf course, we don’t actually use the saturated model for drawing real conclusions, but we can use it as a baseline for comparison. We compare each model that we fit to this saturated model, to calculate the deviance. Deviance is defined as the difference between the log-likelihood of your fitted model and the log-likelihood of the saturated model (multiplied by 2).\nBecause deviance is all about capturing the discrepancy between fitted and actual values, it’s performing a similar function to the residual sum of squares (RSS) in a standard linear model. In fact, the RSS is really just a specific type of deviance.\n\n\nDifferent models and their deviances",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html#significance-testing",
    "href": "materials/significance-testing.html#significance-testing",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "\n9.3 Significance testing",
    "text": "9.3 Significance testing\nThere are a few different potential sources of p-values for a generalised linear model.\nHere, we’ll briefly discuss the p-values that are reported “as standard” in a typical GLM model output.\nThen, we’ll spend most of our time focusing on likelihood ratio tests, perhaps the most effective way to assess significance in a GLM.\n\n9.3.1 Revisiting the diabetes dataset\nAs a worked example, we’ll use a logistic regression fitted to the diabetes dataset that we saw in a previous section.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\ndiabetes_py.head()\n\n   glucose  diastolic  test_result\n0      148         72            1\n1       85         66            0\n2      183         64            1\n3       89         66            0\n4      137         40            1\n\n\n\n\n\nAs a reminder, this dataset contains three variables:\n\n\ntest_result, binary results of a diabetes test result (1 for positive, 0 for negative)\n\nglucose, the results of a glucose tolerance test\n\ndiastolic blood pressure\n\n\n\nR\nPython\n\n\n\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\n\n\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\n\n\n\n\n9.3.2 Wald tests\nLet’s use the summary function to see the model we’ve just fitted.\n\n\nR\nPython\n\n\n\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nprint(glm_dia_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      724\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -374.00\nDate:                Fri, 17 May 2024   Deviance:                       748.01\nTime:                        12:43:46   Pearson chi2:                     720.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.2282\nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept            -8.5711      2.703     -3.171      0.002     -13.869      -3.273\nglucose               0.0547      0.021      2.614      0.009       0.014       0.096\ndiastolic             0.0424      0.036      1.165      0.244      -0.029       0.114\nglucose:diastolic    -0.0002      0.000     -0.796      0.426      -0.001       0.000\n=====================================================================================\n\n\n\n\n\nWhichever language you’re using, you may have spotted some p-values being reported directly here in the model summaries. Specifically, each individual parameter, or coefficient, has its own z-value and associated p-value.\nA hypothesis test has automatically been performed for each of the parameters in your model, including the intercept and interaction. In each case, something called a Wald test has been performed.\nThe null hypothesis for these Wald tests is that the value of the coefficient = 0. The idea is that if a coefficient isn’t significantly different from 0, then that parameter isn’t useful and could be dropped from the model. These tests are the equivalent of the t-tests that are calculated as part of the summary output for standard linear models.\nImportantly, these Wald tests don’t tell you about the significance of the overall model. For that, we’re going to need something else: a likelihood ratio test.\n\n9.3.3 Likelihood ratio tests (LRTs)\nWhen we were assessing the significance of standard linear models, we were able to use the F-statistic to determine:\n\nthe significance of the model versus a null model, and\nthe significance of individual predictors.\n\nWe can’t use these F-tests for GLMs, but we can use LRTs in a really similar way, to calculate p-values for both the model as a whole, and for individual variables.\nThese tests are all built on the idea of deviance, or the likelihood ratio, as discussed above on this page. We can compare any two models fitted to the same dataset by looking at the difference in their deviances, also known as the difference in their log-likelihoods, or more simply as a likelihood ratio.\nHelpfully, this likelihood ratio approximately follows a chi-square distribution, which we can capitalise on that to calculate a p-value. All we need is the number of degrees of freedom, which is equal to the difference in the number of parameters of the two models you’re comparing.\n\n\n\n\n\n\nWarning\n\n\n\nImportantly, we are only able to use this sort of test when one of the two models that we are comparing is a “simpler” version of the other, i.e., one model has a subset of the parameters of the other model.\nSo while we could perform an LRT just fine between these two models: Y ~ A + B + C and Y ~ A + B + C + D, or between any model and the null (Y ~ 1), we would not be able to use this test to compare Y ~ A + B + C and Y ~ A + B + D.\n\n\nTesting the model versus the null\nSince LRTs involve making a comparison between two models, we must first decide which models we’re comparing, and check that one model is a “subset” of the other.\nLet’s use an example from a previous section of the course, where we fitted a logistic regression to the diabetes dataset.\n\n\nR\nPython\n\n\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without predictors, respectively).\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nglm_null &lt;- glm(test_result ~ 1, \n                family = binomial, \n                data = diabetes)\n\nThen, we use the lrtest function from the lmtest package to perform the test itself; we include both the models that we want to compare, listing them one after another.\n\nlrtest(glm_dia, glm_null)\n\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ 1\n  #Df LogLik Df  Chisq Pr(&gt;Chisq)    \n1   4 -374.0                         \n2   1 -468.3 -3 188.59  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see from the output that our chi-square statistic is significant, with a really small p-value. This tells us that, for the difference in degrees of freedom (here, that’s 3), the change in deviance is actually quite big. (In this case, you can use summary(glm_dia) to see those deviances - 936 versus 748!)\nIn other words, our model is better than the null.\n\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without our predictor, respectively).\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\nmodel = smf.glm(formula = \"test_result ~ 1\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_null_py = model.fit()\n\nUnlike in R, there isn’t a nice neat function for extracting the \\(\\chi^2\\) value, so we have to do a little bit of work by hand.\n\n# calculate the likelihood ratio (i.e. the chi-square value)\nlrstat = -2*(glm_null_py.llf - glm_dia_py.llf)\n\n# calculate the associated p-value\npvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_null_py.df_model)\n\nprint(lrstat, pvalue)\n\n188.59314837444526 1.2288700360045209e-40\n\n\nThis gives us the likelihood ratio, based on the log-likelihoods that we’ve extracted directly from the models, which approximates a chi-square distribution.\nWe’ve also calculated the associated p-value, by providing the difference in degrees of freedom between the two models (in this case, that’s simply 1, but for more complicated models it’s easier to extract the degrees of freedom directly from the model as we’ve done here).\nHere, we have a large chi-square statistic and a small p-value. This tells us that, for the difference in degrees of freedom (here, that’s 1), the change in deviance is actually quite big. (In this case, you can use summary(glm_dia) to see those deviances - 936 versus 748!)\nIn other words, our model is better than the null.\n\n\n\n\n9.3.4 Testing individual predictors\nAs well as testing the overall model versus the null, we might want to test particular predictors to determine whether they are individually significant.\nThe way to achieve this is essentially to perform a series of “targeted” likelihood ratio tests. In each LRT, we’ll compare two models that are almost identical - one with, and one without, our variable of interest in each case.\n\n\nR\nPython\n\n\n\nThe first step is to construct a new model that doesn’t contain our predictor of interest. Let’s test the glucose:diastolic interaction.\n\nglm_dia_add &lt;- glm(test_result ~ glucose + diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nNow, we use the lrtest function to compare the models with and without the interaction:\n\nlrtest(glm_dia, glm_dia_add)\n\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ glucose + diastolic\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1   4 -374.00                     \n2   3 -374.32 -1 0.6288     0.4278\n\n\nThis tells us that our interaction glucose:diastolic isn’t significant - our more complex model doesn’t have a meaningful reduction in deviance.\nThis might, however, seem like a slightly clunky way to test each individual predictor. Luckily, we can also use our trusty anova function with an extra argument to tell us about individual predictors.\nBy specifying that we want to use a chi-squared test, we are able to construct an analysis of deviance table (as opposed to an analysis of variance table) that will perform the likelihood ratio tests for us for each predictor:\n\nanova(glm_dia, test=\"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: test_result\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    \nNULL                                727     936.60             \nglucose            1  184.401       726     752.20  &lt; 2e-16 ***\ndiastolic          1    3.564       725     748.64  0.05905 .  \nglucose:diastolic  1    0.629       724     748.01  0.42779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou’ll spot that the p-values we get from the analysis of deviance table match the p-values you could calculate yourself using lrtest; this is just more efficient when you have a complex model!\n\n\nThe first step is to construct a new model that doesn’t contain our predictor of interest. Let’s test the glucose:diastolic interaction.\n\nmodel = smf.glm(formula = \"test_result ~ glucose + diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_add_py = model.fit()\n\nWe’ll then use the same code we used above, to compare the models with and without the interaction:\n\nlrstat = -2*(glm_dia_add_py.llf - glm_dia_py.llf)\n\npvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_dia_add_py.df_model)\n\nprint(lrstat, pvalue)\n\n0.6288201373599804 0.42778842576800746\n\n\nThis tells us that our interaction glucose:diastolic isn’t significant - our more complex model doesn’t have a meaningful reduction in deviance.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html#goodness-of-fit",
    "href": "materials/significance-testing.html#goodness-of-fit",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "\n9.4 Goodness-of-fit",
    "text": "9.4 Goodness-of-fit\nGoodness-of-fit is all about how well a model fits the data, and typically involves summarising the discrepancy between the actual data points, and the fitted/predicted values that the model produces.\nThough closely linked, it’s important to realise that goodness-of-fit and significance don’t come hand-in-hand automatically: we might find a model that is significantly better than the null, but is still overall pretty rubbish at matching the data. So, to understand the quality of our model better, we should ideally perform both types of test.\n\n9.4.1 Chi-square tests\nOnce again, we can make use of deviance and chi-square tests, this time to assess goodness-of-fit.\nAbove, we used likelihood ratio tests to assess the null hypothesis that our candidate fitted model and the null model had the same deviance.\nNow, however, we will test the null hypothesis that the fitted model and the saturated (perfect) model have the same deviance, i.e., that they both fit the data equally well. In most hypothesis tests, we want to reject the null hypothesis, but in this case, we’d like it to be true.\n\n\nR\nPython\n\n\n\nRunning a goodness-of-fit chi-square test in R can be done using the pchisq function. We need to include two arguments: 1) the residual deviance, and 2) the residual degrees of freedom. Both of these can be found in the summary output, but you can use the $ syntax to call these properties directly like so:\n\n1 - pchisq(glm_dia$deviance, glm_dia$df.residual)\n\n[1] 0.2605931\n\n\n\n\nThe syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:\n\npvalue = chi2.sf(glm_dia_py.deviance, glm_dia_py.df_resid)\n\nprint(pvalue)\n\n0.26059314630406843\n\n\n\n\n\nYou can think about this p-value, roughly, as “the probability that this model is good”. We’re not below our significance threshold, which means that we’re not rejecting our null hypothesis (which is a good thing) - but it’s also not a huge probability. This suggests that there’s probably other variables we could measure and include in a future experiment, to give a better overall model.\n\n9.4.2 AIC values\nYou might remember AIC values from standard linear modelling. AIC values are useful, because they tell us about overall model quality, factoring in both goodness-of-fit and model complexity.\nOne of the best things about the Akaike information criterion (AIC) is that it isn’t specific to linear models - it works for models fitted with maximum likelihood estimation.\nIn fact, if you look at the formula for AIC, you’ll see why:\n\\[\nAIC = 2k - 2ln(\\hat{L})\n\\]\nwhere \\(k\\) represents the number of parameters in the model, and \\(\\hat{L}\\) is the maximised likelihood function. In other words, the two parts of the equation represent the complexity of the model, versus the log-likelihood.\nThis means that AIC can be used for model comparison for GLMs in precisely the same way as it’s used for linear models: lower AIC indicates a better-quality model.\n\n\nR\nPython\n\n\n\nThe AIC value is given as standard, near the bottom of the summary output (just below the deviance values). You can also print it directly using the $ syntax:\n\nsummary(glm_dia)\n\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n\nglm_dia$aic\n\n[1] 756.0069\n\n\nIn even better news for R users, the step function works for GLMs just as it does for linear models, so long as you include the test = LRT argument.\n\nstep(glm_dia, test = \"LRT\")\n\nStart:  AIC=756.01\ntest_result ~ glucose * diastolic\n\n                    Df Deviance    AIC     LRT Pr(&gt;Chi)\n- glucose:diastolic  1   748.64 754.64 0.62882   0.4278\n&lt;none&gt;                   748.01 756.01                 \n\nStep:  AIC=754.64\ntest_result ~ glucose + diastolic\n\n            Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;           748.64 754.64                     \n- diastolic  1   752.20 756.20   3.564  0.05905 .  \n- glucose    1   915.52 919.52 166.884  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:  glm(formula = test_result ~ glucose + diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n(Intercept)      glucose    diastolic  \n   -6.49941      0.03836      0.01407  \n\nDegrees of Freedom: 727 Total (i.e. Null);  725 Residual\nNull Deviance:      936.6 \nResidual Deviance: 748.6    AIC: 754.6\n\n\n\n\nThe AIC value isn’t printed as standard with the model summary, but you can access it easily like so:\n\nprint(glm_dia_py.aic)\n\n756.0068586069744\n\n\n\n\n\n\n9.4.3 Pseudo r-squared\nWe can’t use \\(R^2\\) values to represent the amount of variance explained in a GLM. This is primarily because, while linear models are fitted by minimising the squared residuals, GLMs are fitted by maximising the likelihood - an entirely different procedure.\nHowever, because \\(R^2\\) values are so useful in linear modelling, statisticians have developed something called a “pseudo \\(R^2\\)” for GLMs.\n\n\n\n\n\n\nDebate about pseudo \\(R^2\\) values\n\n\n\nThere are two main areas of debate:\n\nWhich version of pseudo \\(R^2\\) to use?\n\nThere are many. Some of the most popular are McFadden’s, Nagelkerke’s, Cox & Snell’s, and Tjur’s. They all have slightly different formulae and in some cases can give quite different results. This post does a nice job of discussing some of them and providing some comparisons.\n\nShould pseudo \\(R^2\\) values be calculated at all?\n\nWell, it depends what you want them for. Most statisticians tend to advise that pseudo \\(R^2\\) values are only really useful for model comparisons (i.e., comparing different GLMs fitted to the same dataset). This is in contrast to the way that we use \\(R^2\\) values in linear models, as a measure of effect size that is generalisable across studies.\nSo, if you choose to use pseudo \\(R^2\\) values, try to be thoughtful about it; and avoid the temptation to over-interpret!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/significance-testing.html#summary",
    "href": "materials/significance-testing.html#summary",
    "title": "\n9  Significance & goodness-of-fit\n",
    "section": "\n9.5 Summary",
    "text": "9.5 Summary\nLikelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and goodness-of-fit. To determine the quality of a model and draw conclusions from it, it’s important to assess both of these things.\n\n\n\n\n\n\nKey points\n\n\n\n\nDeviance is the difference between predicted and actual values, and is calculated by comparing a model’s log-likelihood to that of the perfect “saturated” model\nUsing deviance, likelihood ratio tests can be used in lieu of F-tests for generalised linear models\nSimilarly, a chi-square goodness-of-fit test can also be performed using likelihood/deviance\nThe Akaike information criterion is also based on likelihood, and can be used to compare the quality of GLMs fitted to the same dataset\nOther metrics that may be of use are Wald test p-values and pseudo \\(R^2\\) values",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Significance & goodness-of-fit</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html",
    "href": "materials/checking-assumptions.html",
    "title": "\n10  Checking assumptions\n",
    "section": "",
    "text": "10.1 Libraries and functions",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#libraries-and-functions",
    "href": "materials/checking-assumptions.html#libraries-and-functions",
    "title": "\n10  Checking assumptions\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(ggResidpanel)\n\n\n\n\nfrom scipy.stats import *",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#assumption-1-distribution-of-response-variable",
    "href": "materials/checking-assumptions.html#assumption-1-distribution-of-response-variable",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.2 Assumption 1: Distribution of response variable",
    "text": "10.2 Assumption 1: Distribution of response variable\nAlthough we don’t expect our response variable \\(y\\) to be continuous and normally distributed (as we did in linear modelling), we do still expect its distribution to come from the “exponential family” of distributions.\nThe exponential family contains the following distributions, among others:\n\nnormal\nexponential\nPoisson\nBernoulli\nbinomial (for fixed number of trials)\nchi-squared\n\nYou can use a histogram to visualise the distribution of your response variable, but it is typically most useful just to think about the nature of your response variable. For instance, binary variables will follow a Bernoulli distribution, proportional variables follow a binomial distribution, and most count variables will follow a Poisson distribution.\nIf you have a very unusual variable that doesn’t follow one of these exponential family distributions, however, then a GLM will not be an appropriate choice. In other words, a GLM is not necessarily a magic fix!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#assumption-2-correct-link-function",
    "href": "materials/checking-assumptions.html#assumption-2-correct-link-function",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.3 Assumption 2: Correct link function",
    "text": "10.3 Assumption 2: Correct link function\nA closely-related assumption to assumption 1 above, is that we have chosen the correct link function for our model.\nIf we have done so, then there should be a linear relationship between our transformed model and our response variable; in other words, if we have chosen the right link function, then we have correctly “linearised” our model.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#assumption-3-independence",
    "href": "materials/checking-assumptions.html#assumption-3-independence",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.4 Assumption 3: Independence",
    "text": "10.4 Assumption 3: Independence\nWe expect that the each observation or datapoint in our sample is independent of all the others. Specifically, we expect that our set of \\(y\\) response variables are independent of one another.\nFor this to be true, we have to make sure:\n\nthat we aren’t treating technical replicates as true/biological replicates;\nthat we don’t have observations/datapoints in our sample that are artificially similar to each other (compared to other datapoints);\nthat we don’t have any nuisance/confounding variables that create “clusters” or hierarchy in our dataset;\nthat we haven’t got repeated measures, i.e., multiple measurements/rows per individual in our sample\n\nThere is no diagnostic plot for assessing this assumption. To determine whether your data are independent, you need to understand your experimental design.\nYou might find this page useful if you’re looking for more information on what counts as truly independent data.",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#good-science-no-influential-observations",
    "href": "materials/checking-assumptions.html#good-science-no-influential-observations",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.5 Good science: No influential observations",
    "text": "10.5 Good science: No influential observations\nAs with linear models, though this isn’t always considered a “formal” assumption, we do want to ensure that there aren’t any datapoints that are overly influencing our model.\nA datapoint is overly influential, i.e., has high leverage, if removing that point from the dataset would cause large changes in the model coefficients. Datapoints with high leverage are typically those that don’t follow the same general “trend” as the rest of the data.\nThe easiest way to check for overly influential points is to construct a Cook’s distance plot.\nLet’s try that out, using the diabetes example dataset.\n\n\nR\nPython\n\n\n\n\ndiabetes &lt;- read_csv(\"data/diabetes.csv\")\n\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglm_dia &lt;- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\n\n\n\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\n\n\n\nOnce our model is fitted, we can fit a Cook’s distance plot:\n\n\nR\nPython\n\n\n\n\nresid_panel(glm_dia, plots = \"cookd\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nGood news - there don’t appear to be any overly influential points!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#dispersion",
    "href": "materials/checking-assumptions.html#dispersion",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.6 Dispersion",
    "text": "10.6 Dispersion\nAnother thing that we want to check, primarily in Poisson regression, is whether our dispersion parameter is correct.\n\n\n\n\n\n\nFirst, let’s unpack what dispersion is!\n\n\n\n\n\nDispersion, in statistics, is a general term to describe the variability, scatter, or spread of a distribution. Variance is a common measure of dispersion that hopefully you are familiar with.\nIn a normal distribution, the mean (average) and the variance (dispersion) are independent of each other; we need both numbers, or parameters, to understand the shape of the distribution.\nOther distributions, however, require different parameters to describe them in full. For a Poisson distribution, we need just one parameter \\(lambda\\), which captures the expected rate of occurrences/expected count. The mean and variance of a Poisson distribution are actually expected to be the same.\nIn the context of a model, you can think about the dispersion as the degree to which the data are spread out around the model curve. A dispersion parameter of 1 means the data are spread out exactly as we expect; &lt;1 is called underdispersion; and &gt;1 is called overdispersion.\n\n\n\n\n10.6.1 A “hidden assumption”\nWhen we fit a linear model, because we’re assuming a normal distribution, we take the time to estimate the dispersion - by measuring the variance.\nWhen performing Poisson regression, however, we make an extra “hidden” assumption, in setting the dispersion parameter to 1. In other words, we expect the errors to have a certain spread to them that matches our theoretical distribution/model. This means we don’t have to waste time and statistical power in estimating the dispersion.\nHowever, if our data are underdispersed or overdispersed, then we might be violating this assumption we’ve made.\nUnderdispersion is quite rare. It’s far more likely that you’ll encounter overdispersion; in Poisson regression, this is usually caused by the presence of lots of zeroes in your response variable (known as zero-inflation).\nIn these situations, you may wish to fit a different GLM to the data. Negative binomial regression, for instance, is a common alternative for zero-inflated count data.\n\n10.6.2 Checking the dispersion parameter\nThe easiest way to check dispersion in a model is to calculate the ratio of the residual deviance to the residual degrees of freedom.\nLet’s practice doing this using a Poisson regression fitted to the islands dataset that you saw earlier in the course.\n\n\nR\nPython\n\n\n\n\nislands &lt;- read_csv(\"data/islands.csv\")\n\nRows: 35 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): species, area\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglm_isl &lt;- glm(species ~ area,\n               data = islands, family = \"poisson\")\n\n\n\n\nislands_py = pd.read_csv(\"data/islands.csv\")\n\nmodel = smf.glm(formula = \"species ~ area\",\n                family = sm.families.Poisson(),\n                data = islands_py)\n\nglm_isl_py = model.fit()\n\n\n\n\nIf we take a look at the model output, we can see the two quantities we care about - residual deviance and residual degrees of freedom:\n\n\nR\nPython\n\n\n\n\nsummary(glm_isl)\n\n\nCall:\nglm(formula = species ~ area, family = \"poisson\", data = islands)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 4.241129   0.041322  102.64   &lt;2e-16 ***\narea        0.035613   0.001247   28.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 856.899  on 34  degrees of freedom\nResidual deviance:  30.437  on 33  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 3\n\n\n\n\n\nprint(glm_isl_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                species   No. Observations:                   35\nModel:                            GLM   Df Residuals:                       33\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -139.33\nDate:                Fri, 17 May 2024   Deviance:                       30.437\nTime:                        12:40:44   Pearson chi2:                     30.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.2411      0.041    102.636      0.000       4.160       4.322\narea           0.0356      0.001     28.551      0.000       0.033       0.038\n==============================================================================\n\n\n\n\n\nThe residual deviance is 30.437, on 33 residual degrees of freedom. All we need to do is divide one by the other to get our dispersion parameter.\n\n\nR\nPython\n\n\n\n\nglm_isl$deviance/glm_isl$df.residual\n\n[1] 0.922334\n\n\n\n\n\nprint(glm_isl_py.deviance/glm_isl_py.df_resid)\n\n0.9223340414458532\n\n\n\n\n\nThe dispersion parameter here is 0.922. That’s pretty good - not far off 1 at all.\nBut how can we check whether it is significantly different from 1?\nWell, you’ve actually already got the knowledge you need to do this, from the previous course section on significance testing. Specifically, the chi-squared goodness-of-fit test can be used to check whether the dispersion is within sensible limits.\nYou may have noticed that the two values we’re using for the dispersion parameter are the same two numbers that we used in those chi-squared tests. For this Poisson regression fitted to the islands dataset, that goodness-of-fit test would look like this:\n\n\nR\nPython\n\n\n\n\n1 - pchisq(glm_isl$deviance, glm_isl$df.residual)\n\n[1] 0.595347\n\n\n\n\n\npvalue = chi2.sf(glm_isl_py.deviance, glm_isl_py.df_resid)\n\nprint(pvalue)\n\n0.5953470127463187\n\n\n\n\n\nIf our chi-squared goodness-of-fit test returns a large (insignificant) p-value, as it does here, that tells us that we don’t need to worry about the dispersion.\nIf our chi-squared goodness-of-fit test returned a small, significant p-value, this would tell us our model doesn’t fit the data well. And, since dispersion is all about the spread of points around the model, it makes sense that these two things are so closely related!",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/checking-assumptions.html#summary",
    "href": "materials/checking-assumptions.html#summary",
    "title": "\n10  Checking assumptions\n",
    "section": "\n10.7 Summary",
    "text": "10.7 Summary\nWhile generalised linear models make fewer assumptions than standard linear models, we do still expect certain things to be true about the model and our variables for GLMs to be valid. Checking most of these assumptions requires understanding your dataset, and diagnostic plots play a less heavy role.\n\n\n\n\n\n\nKey points\n\n\n\n\nFor a generalised linear model, we assume that we have chosen the correct link function, that our response variable follows a distribution from the exponential family, and that our data are independent\nTo assess these assumptions, we need to understand our dataset and variables\nWe can also use visualisation to determine whether we have overly influential (high leverage) datapoints\nFor Poisson regression, we should also investigate the dispersion parameter of our model, which we expect to be close to 1",
    "crumbs": [
      "Model assessment",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Checking assumptions</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html",
    "href": "materials/glm-practical-poisson.html",
    "title": "\n11  Analysing count data\n",
    "section": "",
    "text": "11.1 Libraries and functions\nThe examples in this section use the following data sets:\ndata/islands.csv\nThis is a data set comprising 35 observations of two variables (one dependent and one predictor). This records the number of species recorded on different small islands along with the area (km2) of the islands. The variables are species and area.\nThe second data set is on seat belts.\nThe seatbelts data set is a multiple time-series data set that was commissioned by the Department of Transport in 1984 to measure differences in deaths before and after front seat belt legislation was introduced on 31st January 1983. It provides monthly total numerical data on a number of incidents including those related to death and injury in Road Traffic Accidents (RTA’s). The data set starts in January 1969 and observations run until December 1984.\nYou can find the file in data/seatbelts.csv",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#libraries-and-functions",
    "href": "materials/glm-practical-poisson.html#libraries-and-functions",
    "title": "\n11  Analysing count data\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n11.1.1 Libraries\n\n11.1.2 Functions\n\n\n\n\n11.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n\n11.1.4 Functions",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#load-and-visualise-the-data",
    "href": "materials/glm-practical-poisson.html#load-and-visualise-the-data",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.2 Load and visualise the data",
    "text": "11.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nislands &lt;- read_csv(\"data/islands.csv\")\n\nLet’s have a glimpse at the data:\n\nislands\n\n# A tibble: 35 × 2\n   species  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1     114  12.1\n 2     130  13.4\n 3     113  13.7\n 4     109  14.5\n 5     118  16.8\n 6     136  19.0\n 7     149  19.6\n 8     162  20.6\n 9     145  20.9\n10     148  21.0\n# ℹ 25 more rows\n\n\n\n\n\nislands_py = pd.read_csv(\"data/islands.csv\")\n\nLet’s have a glimpse at the data:\n\nislands_py.head()\n\n   species       area\n0      114  12.076133\n1      130  13.405439\n2      113  13.723525\n3      109  14.540359\n4      118  16.792122\n\n\n\n\n\nLooking at the data, we can see that there are two columns: species, which contains the number of species recorded on each island and area, which contains the surface area of the island in square kilometers.\nWe can plot the data:\n\n\nR\nPython\n\n\n\n\nggplot(islands, aes(x = area, y = species)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n(ggplot(islands_py, aes(x = \"area\", y = \"species\")) +\n  geom_point())\n\n\n\n\n\n\n\n\n\n\nIt looks as though area may have an effect on the number of species that we observe on each island. We note that the response variable is count data and so we try to construct a Poisson regression.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#constructing-a-model",
    "href": "materials/glm-practical-poisson.html#constructing-a-model",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.3 Constructing a model",
    "text": "11.3 Constructing a model\n\n\nR\nPython\n\n\n\n\nglm_isl &lt;- glm(species ~ area,\n               data = islands, family = \"poisson\")\n\nand we look at the model summary:\n\nsummary(glm_isl)\n\n\nCall:\nglm(formula = species ~ area, family = \"poisson\", data = islands)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 4.241129   0.041322  102.64   &lt;2e-16 ***\narea        0.035613   0.001247   28.55   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 856.899  on 34  degrees of freedom\nResidual deviance:  30.437  on 33  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the two numbers underneath Estimate.Std in the Coefficients table:\n(Intercept)    4.241129\narea           0.035613\n\n\n\n# create a generalised linear model\nmodel = smf.glm(formula = \"species ~ area\",\n                family = sm.families.Poisson(),\n                data = islands_py)\n# and get the fitted parameters of the model\nglm_isl_py = model.fit()\n\nLet’s look at the model output:\n\nprint(glm_isl_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                species   No. Observations:                   35\nModel:                            GLM   Df Residuals:                       33\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -139.33\nDate:                Thu, 13 Jun 2024   Deviance:                       30.437\nTime:                        08:30:32   Pearson chi2:                     30.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.2411      0.041    102.636      0.000       4.160       4.322\narea           0.0356      0.001     28.551      0.000       0.033       0.038\n==============================================================================\n\n\n\n\n\nThese are the coefficients of the Poisson model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of island size:\n\\[ E(species) = \\exp(4.24 + 0.036 \\times area) \\]\nInterpreting this requires a bit of thought (not much, but a bit). The intercept coefficient, 4.24, is related to the number of species we would expect on an island of zero area (this is statistics, not real life. You’d do well to remember that before you worry too much about what that even means). But in order to turn this number into something meaningful we have to exponentiate it. Since exp(4.24) ≈ 70, we can say that the baseline number of species the model expects on any island is 70. This isn’t actually the interesting bit though.\nThe coefficient of area is the fun bit. For starters we can see that it is a positive number which does mean that increasing area leads to increasing numbers of species. Good so far.\nBut what does the value 0.036 actually mean? Well, if we exponentiate it as well, we get exp(0.036) ≈ 1.04. This means that for every increase in area of 1 km^2 (the original units of the area variable), the number of species on the island is multiplied by 1.04. So, an island of area 1 km^2 will have 1.04 x 70 ≈ 72 species.\nSo, in order to interpret Poisson coefficients, you have to exponentiate them.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#plotting-the-poisson-regression",
    "href": "materials/glm-practical-poisson.html#plotting-the-poisson-regression",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.4 Plotting the Poisson regression",
    "text": "11.4 Plotting the Poisson regression\n\n\nR\nPython\n\n\n\n\nggplot(islands, aes(area, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = poisson)) +\n  xlim(10,50)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nmodel = pd.DataFrame({'area': list(range(10, 50))})\n\nmodel[\"pred\"] = glm_isl_py.predict(model)\n\nmodel.head()\n\n   area        pred\n0    10   99.212463\n1    11  102.809432\n2    12  106.536811\n3    13  110.399326\n4    14  114.401877\n\n\n\n(ggplot(islands_py,\n         aes(x = \"area\",\n             y = \"species\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"area\", y = \"pred\"), colour = \"blue\", size = 1))",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#assumptions",
    "href": "materials/glm-practical-poisson.html#assumptions",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.5 Assumptions",
    "text": "11.5 Assumptions\nAs we mentioned earlier, Poisson regressions require that the variance of the data at any point is the same as the mean of the data at that point. We checked that earlier by looking at the residual deviance values.\nWe can look for influential points using the Cook’s distance plot:\n\n\nR\nPython\n\n\n\n\nplot(glm_isl , which = 4)\n\n\n\n\n\n\n\n\n\n\n# extract the Cook's distances\nglm_isl_py_resid = pd.DataFrame(glm_isl_py.\n                                get_influence().\n                                summary_frame()[\"cooks_d\"])\n\n# add row index \nglm_isl_py_resid['obs'] = glm_isl_py_resid.reset_index().index\n\nWe can use these to create the plot:\n\n(ggplot(glm_isl_py_resid,\n         aes(x = \"obs\",\n             y = \"cooks_d\")) +\n     geom_segment(aes(x = \"obs\", y = \"cooks_d\", xend = \"obs\", yend = 0)) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nNone of our points have particularly large Cook’s distances and so life is rosy.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#assessing-significance",
    "href": "materials/glm-practical-poisson.html#assessing-significance",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.6 Assessing significance",
    "text": "11.6 Assessing significance\nWe can ask the same three questions we asked before.\n\nIs the model well-specified?\nIs the overall model better than the null model?\nAre any of the individual predictors significant?\n\nAgain, in this case, questions 2 and 3 are effectively asking the same thing because we still only have a single predictor variable.\nTo assess if the model is any good we’ll again use the residual deviance and the residual degrees of freedom.\n\n\nR\nPython\n\n\n\n\n1 - pchisq(30.437, 33)\n\n[1] 0.5953482\n\n\n\n\n\nchi2.sf(30.437, 33)\n\n0.5953481872979622\n\n\n\n\n\nThis gives a probability of 0.60. This suggests that this model is actually a reasonably decent one and that the data are pretty well supported by the model. For Poisson models this has an extra interpretation. This can be used to assess whether we have significant over-dispersion in our data.\nFor a Poisson model to be appropriate we need that the variance of the data to be exactly the same as the mean of the data. Visually, this would correspond to the data spreading out more for higher predicted values of species. However, we don’t want the data to spread out too much. If that happens then a Poisson model wouldn’t be appropriate.\nThe easy way to check this is to look at the ratio of the residual deviance to the residual degrees of freedom (in this case 0.922). For a Poisson model to be valid, this ratio should be about 1. If the ratio is significantly bigger than 1 then we say that we have over-dispersion in the model and we wouldn’t be able to trust any of the significance testing that we are about to do using a Poisson regression.\nThankfully the probability we have just created (0.60) is exactly the right one we need to look at to assess whether we have significant over-dispersion in our model.\nSecondly, to assess whether the overall model, with all of the terms, is better than the null model we’ll look at the difference in deviances and the difference in degrees of freedom:\n\n\nR\nPython\n\n\n\n\n1 - pchisq(856.899 - 30.437, 34 - 33)\n\n[1] 0\n\n\n\n\n\nchi2.sf(856.899 - 30.437, 34 - 33)\n\n9.524927068555617e-182\n\n\n\n\n\nThis gives a reported p-value of pretty much zero, which is pretty damn small. So, yes, this model is better than nothing at all and species does appear to change with some of our predictors\nFinally, we’ll construct an analysis of deviance table to look at the individual terms:\n\n\nR\nPython\n\n\n\n\nanova(glm_isl , test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: species\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                    34     856.90              \narea  1   826.46        33      30.44 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe p-value in this table is just as small as we’d expect given our previous result (&lt;2.2e-16 is pretty close to 0), and we have the nice consistent result that area definitely has an effect on species.\n\n\nAs mentioned before, this is not quite possible in Python.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#exercises",
    "href": "materials/glm-practical-poisson.html#exercises",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.7 Exercises",
    "text": "11.7 Exercises\n\n11.7.1 Seat belts\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/seatbelts.csv.\nI’d like you to do the following:\n\nLoad the data\nVisualise the data and create a poisson regression model\nPlot the regression model on top of the data\nAssess if the model is a decent predictor for the number of fatalities\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nLoad and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\nseatbelts &lt;- read_csv(\"data/seatbelts.csv\")\n\n\n\n\nseatbelts_py = pd.read_csv(\"data/seatbelts.csv\")\n\nLet’s have a glimpse at the data:\n\nseatbelts_py.head()\n\n   casualties  drivers  front  rear  ...  van_killed  law  year  month\n0         107     1687    867   269  ...          12    0  1969    Jan\n1          97     1508    825   265  ...           6    0  1969    Feb\n2         102     1507    806   319  ...          12    0  1969    Mar\n3          87     1385    814   407  ...           8    0  1969    Apr\n4         119     1632    991   454  ...          10    0  1969    May\n\n[5 rows x 10 columns]\n\n\n\n\n\nThe data tracks the number of drivers killed in road traffic accidents, before and after the seat belt law was introduced. The information on whether the law was in place is encoded in the law column as 0 (law not in place) or 1 (law in place).\nThere are many more observations when the law was not in place, so we need to keep this in mind when we’re interpreting the data.\nFirst we have a look at the data comparing no law vs law:\n\n\nR\nPython\n\n\n\nWe have to convert the law column to a factor, otherwise R will see it as numerical.\n\nseatbelts %&gt;% \n  ggplot(aes(as_factor(law), casualties)) +\n   geom_boxplot()\n\n\n\n\n\n\n\nThe data are recorded by month and year, so we can also display the number of drivers killed by year:\n\nseatbelts %&gt;% \n  ggplot(aes(year, casualties)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe have to convert the law column to a factor, otherwise R will see it as numerical.\n\n(ggplot(seatbelts_py,\n         aes(x = seatbelts_py.law.astype(object),\n             y = \"casualties\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\nThe data are recorded by month and year, so we can also display the number of casualties by year:\n\n(ggplot(seatbelts_py,\n         aes(x = \"year\",\n             y = \"casualties\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\nThe data look a bit weird. There is quite some variation within years (keeping in mind that the data are aggregated monthly). The data also seems to wave around a bit… with some vague peaks (e.g. 1972 - 1973) and some troughs (e.g. around 1976).\nSo my initial thought is that these data are going to be a bit tricky to interpret. But that’s OK.\nConstructing a model\n\n\nR\nPython\n\n\n\n\nglm_stb &lt;- glm(casualties ~ year,\n               data = seatbelts, family = \"poisson\")\n\nand we look at the model summary:\n\nsummary(glm_stb)\n\n\nCall:\nglm(formula = casualties ~ year, family = \"poisson\", data = seatbelts)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 37.168958   2.796636   13.29   &lt;2e-16 ***\nyear        -0.016373   0.001415  -11.57   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 984.50  on 191  degrees of freedom\nResidual deviance: 850.41  on 190  degrees of freedom\nAIC: 2127.2\n\nNumber of Fisher Scoring iterations: 4\n\n\n(Intercept)    37.168958\nyear           -0.016373\n\n\n\n# create a linear model\nmodel = smf.glm(formula = \"casualties ~ year\",\n                family = sm.families.Poisson(),\n                data = seatbelts_py)\n# and get the fitted parameters of the model\nglm_stb_py = model.fit()\n\n\nprint(glm_stb_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             casualties   No. Observations:                  192\nModel:                            GLM   Df Residuals:                      190\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -1061.6\nDate:                Thu, 13 Jun 2024   Deviance:                       850.41\nTime:                        08:30:34   Pearson chi2:                     862.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.5026\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     37.1690      2.797     13.291      0.000      31.688      42.650\nyear          -0.0164      0.001    -11.569      0.000      -0.019      -0.014\n==============================================================================\n\n\n======================\n                 coef  \n----------------------\nIntercept     37.1690 \nyear          -0.0164 \n======================\n\n\n\nThese are the coefficients of the Poisson model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of island size:\n\\[ E(casualties) = \\exp(37.17 - 0.164 \\times year) \\]\nAssessing significance\nIs the model well-specified?\n\n\nR\nPython\n\n\n\n\n1 - pchisq(850.41, 190)\n\n[1] 0\n\n\n\n\n\nchi2.sf(850.41, 190)\n\n3.1319689119997022e-84\n\n\n\n\n\nThis value indicates that the model is actually pretty good. Remember, it is between \\([0, 1]\\) and the closer to zero, the better the model.\nHow about the overall fit?\n\n\nR\nPython\n\n\n\n\n1 - pchisq(984.50 - 850.41, 191 - 190)\n\n[1] 0\n\n\n\n\nFirst we need to define the null model:\n\n# create a linear model\nmodel = smf.glm(formula = \"casualties ~ 1\",\n                family = sm.families.Poisson(),\n                data = seatbelts_py)\n# and get the fitted parameters of the model\nglm_stb_null_py = model.fit()\n\nprint(glm_stb_null_py.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:             casualties   No. Observations:                  192\nModel:                            GLM   Df Residuals:                      191\nModel Family:                 Poisson   Df Model:                            0\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -1128.6\nDate:                Thu, 13 Jun 2024   Deviance:                       984.50\nTime:                        08:30:35   Pearson chi2:                 1.00e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):          1.942e-13\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      4.8106      0.007    738.670      0.000       4.798       4.823\n==============================================================================\n\n\n\nchi2.sf(984.50 - 850.41, 191 - 190)\n\n5.2214097202831414e-31\n\n\n\n\n\nAgain, this indicates that the model is markedly better than the null model.\nPlotting the regression\n\n\nR\nPython\n\n\n\n\nggplot(seatbelts, aes(year, casualties)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = poisson)) +\n  xlim(1970, 1985)\n\n\n\n\n\n\n\n\n\n\nmodel = pd.DataFrame({'year': list(range(1968, 1985))})\n\nmodel[\"pred\"] = glm_stb_py.predict(model)\n\nmodel.head()\n\n   year        pred\n0  1968  140.737690\n1  1969  138.452153\n2  1970  136.203733\n3  1971  133.991827\n4  1972  131.815842\n\n\n\n(ggplot(seatbelts_py,\n         aes(x = \"year\",\n             y = \"casualties\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"year\", y = \"pred\"), colour = \"blue\", size = 1))\n\n\n\n\n\n\n\n\n\n\nConclusions\nThe model we constructed appears to be a decent predictor for the number of fatalities.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-poisson.html#summary",
    "href": "materials/glm-practical-poisson.html#summary",
    "title": "\n11  Analysing count data\n",
    "section": "\n11.8 Summary",
    "text": "11.8 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nPoisson regression is useful when dealing with count data",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Analysing count data</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html",
    "href": "materials/glm-practical-negative_binomial.html",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "",
    "text": "12.1 Libraries and functions\nIn the previous chapter we looked at how to analyse count data. We used a Poisson regression to do this. A key assumption in a Poisson regression is that the mean of the count data is equal to the variance.\nThat’s great - until the observed variance isn’t equal to the mean. If, instead, the observed variance in your data exceeds the mean of the counts we have overdispersion. Similarly, if it’s lower we have underdispersion.\nQueue negative binomial models.\nNegative binomial models are also used for count data, but these models don’t require that the variance of the data exactly matches the mean of the data, and so they can be used in situations where your data exhibit overdispersion.\nWe’ll explore this with the galapagos example. You can find the data in:\ndata/galapagos.csv\nThere are 30 Galapagos islands and 4 variables in the data. The relationship between the number of plant species (species) and several geographic variables is of interest.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#libraries-and-functions",
    "href": "materials/glm-practical-negative_binomial.html#libraries-and-functions",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\n12.1.1 Libraries\n\nlibrary(MASS)\nlibrary(performance)\n\n\n12.1.2 Functions\n\n# fits a negative binomial model\nMASS::glm.nb()\nperformance::check_overdispersion()\n\n\n\n\n\n12.1.3 Libraries\n\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n\n12.1.4 Functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nendemics – the number of endemic species\n\narea – the area of the island km2\n\n\nelevation – the highest elevation of the island (m).\n\nnearest – the distance from the nearest island (km)",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#load-and-visualise-the-data",
    "href": "materials/glm-practical-negative_binomial.html#load-and-visualise-the-data",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "\n12.2 Load and visualise the data",
    "text": "12.2 Load and visualise the data\nFirst we load the data, then we visualise it.\n\n\nR\nPython\n\n\n\n\ngalapagos &lt;- read_csv(\"data/galapagos.csv\")\n\nLet’s have a glimpse at the data:\n\ngalapagos\n\n# A tibble: 30 × 5\n   species endemics  area elevation nearest\n     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1      58       23 25.1        346     0.6\n 2      31       21  1.24       109     0.6\n 3       3        3  0.21       114     2.8\n 4      25        9  0.1         46     1.9\n 5       2        1  0.05        77     1.9\n 6      18       11  0.34       119     8  \n 7      24        0  0.08        93     6  \n 8      10        7  2.33       168    34.1\n 9       8        4  0.03        71     0.4\n10       2        2  0.18       112     2.6\n# ℹ 20 more rows\n\n\n\n\n\ngalapagos_py = pd.read_csv(\"data/galapagos.csv\")\n\nLet’s have a glimpse at the data:\n\ngalapagos_py.head()\n\n   species  endemics   area  elevation  nearest\n0       58        23  25.09        346      0.6\n1       31        21   1.24        109      0.6\n2        3         3   0.21        114      2.8\n3       25         9   0.10         46      1.9\n4        2         1   0.05         77      1.9\n\n\n\n\n\nWe can plot the data:\n\n\nR\nPython\n\n\n\n\ngalapagos %&gt;% \n  pairs(lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks as though endemics and elevation might be related to species, but area and nearest are harder to work out.\nGiven that the response variable, species, is a count variable we try to construct a Poisson regression. We decide that there is no biological reason to look for interaction between the various predictor variables and so we don’t construct a model with any interactions. Remember that this may or may not be a sensible thing to do in general.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#constructing-a-model",
    "href": "materials/glm-practical-negative_binomial.html#constructing-a-model",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "\n12.3 Constructing a model",
    "text": "12.3 Constructing a model\n\n\nR\nPython\n\n\n\n\nglm_gal &lt;- glm(species ~ area + endemics + elevation + nearest,\n               data = galapagos, family = \"poisson\")\n\nand we look at the model summary:\n\nsummary(glm_gal)\n\n\nCall:\nglm(formula = species ~ area + endemics + elevation + nearest, \n    family = \"poisson\", data = galapagos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.794e+00  5.332e-02  52.399  &lt; 2e-16 ***\narea        -1.266e-04  2.559e-05  -4.947 7.53e-07 ***\nendemics     3.325e-02  9.164e-04  36.283  &lt; 2e-16 ***\nelevation    3.799e-04  9.432e-05   4.028 5.63e-05 ***\nnearest      9.049e-03  1.327e-03   6.819 9.18e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  315.88  on 25  degrees of freedom\nAIC: 486.71\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\nNow, this time, before we start looking at interpreting the model coefficients were going to jump straight into assessing whether the model is well-specified (spoiler alert: we do this because I already know that it isn’t…).\nWe can formally check this with our trusty “Is the model well-posed” probability value:\n\n\nR\nPython\n\n\n\n\n1 - pchisq(315.88, 25)\n\n[1] 0\n\n\n\n\n\n\n\n\nThis gives a big fat 0, so no, there are definitely things wrong with our model and we can’t really trust anything that’s being spat out at this stage. The issue in this case lies with overdispersion.\n\n\n\n\n\n\nDispersion parameter\n\n\n\nOne way of assessing dispersion is by calculating the dispersion parameter, \\(\\theta\\). This takes the residual deviance and divides it by the number of degrees of freedom.\nThe residual deviance is 315.88, but we only have 25 degrees of freedom in the model. The number of degrees of freedom are low because we only have 30 data points but have 4 parameters in our model, leading to \\(30 - 4 - 1 = 25\\) degrees of freedom. So we have:\n\\[\\theta = 315.88 / 25 = 12.64\\]\nThe Poisson regression is assuming the value is 1. But the actual \\(\\theta\\) is nowhere near close to 1, so that’s is a bad idea.\n\n\nSo, with that conclusion, we won’t bother looking at the analysis of deviance table or asking whether the model is better than the null model. Instead we need to find a better fitting model…\nFor count response data options are limited, but the main alternative to a Poisson model is something called a negative binomial model.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#negative-binomial-model",
    "href": "materials/glm-practical-negative_binomial.html#negative-binomial-model",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "\n12.4 Negative binomial model",
    "text": "12.4 Negative binomial model\n\n\nR\nPython\n\n\n\nTo specify a negative binomial model, we use the MASS package.\n\nlibrary(MASS)\n\n\nnb_gal &lt;- glm.nb(species ~ area + endemics + elevation + nearest,\n               data = galapagos)\n\n\nsummary(nb_gal)\n\n\nCall:\nglm.nb(formula = species ~ area + endemics + elevation + nearest, \n    data = galapagos, init.theta = 2.987830946, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.4870922  0.1864426  13.340  &lt; 2e-16 ***\narea        -0.0002911  0.0001941  -1.500    0.134    \nendemics     0.0457287  0.0065890   6.940 3.92e-12 ***\nelevation    0.0003053  0.0005137   0.594    0.552    \nnearest      0.0040316  0.0079105   0.510    0.610    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.9878) family taken to be 1)\n\n    Null deviance: 151.446  on 29  degrees of freedom\nResidual deviance:  33.395  on 25  degrees of freedom\nAIC: 286.06\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.988 \n          Std. Err.:  0.898 \n\n 2 x log-likelihood:  -274.060 \n\n\nThis output is very similar to the other GLM outputs that we’ve seen but with some additional information at the bottom regarding the dispersion parameter that the negative binomial model has used, which it calls Theta (2.988). It is estimated from the data.\nAs before, the main numbers to extract from the output are the numbers underneath Estimate in the Coefficients table:\nCoefficients:\n              Estimate\n(Intercept)  2.4870922\narea        -0.0002911\nendemics     0.0457287\nelevation    0.0003053\nnearest      0.0040316\n\n\n\n\n\n\nThese are the coefficients of the Negative Binomial model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of the other variables.:\n\\[\n\\begin{split}\n  E(species) = \\exp(2.49 - 0.0003 \\times area + 0.046 \\times endemics\\\\\n  + \\ 0.0003 \\times elevation + 0.004 \\times nearest)\n\\end{split}\n\\]\n\n\n\n\n\n\nKey concept\n\n\n\nThe main difference between a Poisson regression and a negative binomial regression: in the former the dispersion parameter is assumed to be 1, whereas in the latter it is estimated from the data.\nAs such, the negative binomial model has the same form for its line of best fit as the Poisson model, but the underlying probability distribution is different.\n\n\n\n12.4.1 Assessing significance\nWe can ask the same three questions we asked before.\n\nIs the model well-specified?\nIs the overall model better than the null model?\nAre any of the individual predictors significant?\n\nTo assess whether the model is any good we’ll use the residual deviance and the residual degrees of freedom.\n\n\nR\nPython\n\n\n\n\n1 - pchisq(33.395, 25)\n\n[1] 0.1214851\n\n\nInstead of manually typing in the values, which is of course prone to errors, we can also extract them directly from the model object:\n\n1 - pchisq(nb_gal$deviance, nb_gal$df.residual)\n\n[1] 0.1214756\n\n\n\n\n\n\n\n\nThis gives a probability of 0.121. Whilst this isn’t brilliant, it is still much better than the model we had before, and now that we’ve taken account of the overdispersion issue, the fact that this probability is a bit small is probably down to the fact that the predictor variables we have in the model might not be enough to fully explain the number of species on each of the Galapagos islands.\nHowever, since we don’t have any other data to play with there’s nothing we can do about that right now.\nTo assess if the overall model, with all four terms, is better than the null model we’ll look at the difference in deviances and the difference in degrees of freedom:\n\n\nR\nPython\n\n\n\n\n1 - pchisq(151.446 - 33.395, 29 - 25)\n\n[1] 0\n\n\nOr extracting them directly from the model object:\n\n1 - pchisq(nb_gal$null.deviance - nb_gal$deviance,\n           nb_gal$df.null - nb_gal$df.residual)\n\n[1] 0\n\n\n\n\n\n\n\n\nThis gives a reported p-value of 0, which is pretty darn small. So, yes, this model is better than nothing at all and at least some of our predictors are related to the response variable in a meaningful fashion.\nFinally, we’ll construct an analysis of deviance table to look at the individual predictors:\n\n\nR\nPython\n\n\n\n\nanova(nb_gal, test = \"Chisq\")\n\nWarning in anova.negbin(nb_gal, test = \"Chisq\"): tests made without\nre-estimating 'theta'\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nAnalysis of Deviance Table\n\nModel: Negative Binomial(2.9878), link: log\n\nResponse: species\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                         29    151.446              \narea       1   33.873        28    117.574 5.884e-09 ***\nendemics   1   83.453        27     34.121 &lt; 2.2e-16 ***\nelevation  1    0.468        26     33.653    0.4939    \nnearest    1    0.257        25     33.395    0.6121    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nYou might get a warning message about theta not being recalculated but this isn’t something to worry about.\n\n\n\n\n\n\nFor more detail on the deviance table, refer to the chapter on Significance testing & goodness-of-fit.\nWe can now see that it looks like two of our predictor variables aren’t actually significant predictors at all, and that only the area and number of endemic species (endemics) on each island is a significant predictor of the number of plant species on each Galapagos island. We can check this further using backward stepwise elimination.\n\n\nR\nPython\n\n\n\n\nstep(nb_gal)\n\nStart:  AIC=284.06\nspecies ~ area + endemics + elevation + nearest\n\n            Df Deviance    AIC\n- nearest    1   33.653 282.32\n- elevation  1   33.831 282.50\n&lt;none&gt;           33.395 284.06\n- area       1   35.543 284.21\n- endemics   1   70.764 319.43\n\nStep:  AIC=282.32\nspecies ~ area + endemics + elevation\n\n            Df Deviance    AIC\n- elevation  1   33.814 280.78\n&lt;none&gt;           33.351 282.32\n- area       1   35.795 282.76\n- endemics   1   70.183 317.15\n\nStep:  AIC=280.77\nspecies ~ area + endemics\n\n\nWarning: glm.fit: algorithm did not converge\n\n\n           Df Deviance    AIC\n- area      1   35.179 280.72\n&lt;none&gt;          33.232 280.77\n- endemics  1  114.180 359.72\n\nStep:  AIC=280.66\nspecies ~ endemics\n\n           Df Deviance    AIC\n&lt;none&gt;          33.204 280.66\n- endemics  1  137.826 383.28\n\n\n\nCall:  glm.nb(formula = species ~ endemics, data = galapagos, init.theta = 2.695721184, \n    link = log)\n\nCoefficients:\n(Intercept)     endemics  \n    2.63124      0.04378  \n\nDegrees of Freedom: 29 Total (i.e. Null);  28 Residual\nNull Deviance:      137.8 \nResidual Deviance: 33.2     AIC: 282.7\n\n\n\n\n\n\n\n\nThis shows that only endemics is an appropriate predictor. See the Core statistics notes on Backwards stepwise elimination for more information.\nOur new best model only contains endemics as a predictor, so we should fit this model and check that it still is an adequate model.\n\n\nR\nPython\n\n\n\n\nnb_gal_min &lt;- glm.nb(species ~ endemics, data = galapagos)\n\n\nsummary(nb_gal_min)\n\n\nCall:\nglm.nb(formula = species ~ endemics, data = galapagos, init.theta = 2.695721175, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 2.631244   0.164242   16.02   &lt;2e-16 ***\nendemics    0.043785   0.004233   10.34   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.6957) family taken to be 1)\n\n    Null deviance: 137.826  on 29  degrees of freedom\nResidual deviance:  33.204  on 28  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.696 \n          Std. Err.:  0.789 \n\n 2 x log-likelihood:  -276.662 \n\n\nIf we look at the deviance residuals (33.204) and the residual degrees of freedom (28), we can use the pchisq() function to get an overall assessment of whether this model is well-specified.\n\n1 - pchisq(nb_gal_min$deviance, nb_gal_min$df.residual)\n\n[1] 0.2283267\n\n\nAnd we get a probability of 0.228 which is better than before, not amazing, but it might be adequate for what we have. Woohoo!\n\n\n\n\n\n\nThe model equation for a negative binomial curve is the same as for a Poisson model and so, lifting the coefficients from the summary output we have the following relationship in our model:\n\\(E(species) = \\exp(2.63 + 0.044 \\times endemics)\\)\n\n12.4.2 Model suitability\nAs we saw above, the model we created was not terribly well-specified. We can visualise it as follows:\n\n\nR\nPython\n\n\n\n\nggplot(galapagos, aes(endemics, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that at the lower range of endemics, the model is predicting values that are a bit too high. In the mid-range it predicts values that are too low and at the higher end the model is, well, rubbish.\nLet’s compare it directly to our original Poisson regression (setting the limits of the y-axes to match):\n\n\n\n\n\n\nShow code\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\np1 &lt;- ggplot(galapagos, aes(endemics, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE,\n              method.args = list(family = poisson)) +\n  ylim(0, 500) +\n  labs(title = \"Poisson\")\n\np2 &lt;- ggplot(galapagos, aes(endemics, species)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE) +\n  ylim(0, 500) +\n  labs(title = \"Negative binomial\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo. although we’ve dealt with the overdispersion we had in the original Poisson regression, we still have a pretty poor model. So, what is a poor researcher to do? We’ll let you explore this in an exercise.\n\n\n\n\n\n\nImportant\n\n\n\nComplete Exercise 12.5.1.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#exercises",
    "href": "materials/glm-practical-negative_binomial.html#exercises",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "\n12.5 Exercises",
    "text": "12.5 Exercises\n\n12.5.1 Galapagos models\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nLevel: \nFor this exercise we’ll be using the data from data/galapagos.csv.\nThe fit for both the Poisson and Negative binomial regressions is not very good, suggesting that the model is not well-specified. How could we improve this?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n\nThere appears to be a power relationship between species and endemics. So, one thing we could do is log-transform the response (species) and predictor (endemics) variables.\n\n\nR\nPython\n\n\n\n\nggplot(data = galapagos,\n       aes(x = log(endemics),\n           y = log(species))) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe could add a regression line to this. Additionally, we could just log-transform endemics and fit a negative binomial model to this. Let’s do all of this and plot them together with the original Poisson and Negative binomial models.\n\n\nR\nPython\n\n\n\n\np3 &lt;- ggplot(galapagos, aes(log(endemics), log(species))) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE,\n              method.args = list(family = poisson)) +\n  labs(title = \"Linear model of log-log\")\n\n\np4 &lt;- ggplot(galapagos, aes(log(endemics), species)) +\n  geom_point() +\n  geom_smooth(method = \"glm.nb\", se = FALSE, fullrange = TRUE) +\n  ylim(0, 500) +\n  labs(title = \"Negative binomial of log(endemics)\")\n\n\np1 + p2 + p3 + p4 +\n  plot_annotation(tag_levels = \"A\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 11 rows containing missing values or values outside the scale range\n(`geom_smooth()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: In lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n    ...) :\n extra argument 'family' will be disregarded\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this it is clear that the negative binomial model fitted to species ~ log(endemics) in panel D produces a much better fit than the original fit in panel B.\nEqually, looking at the relationship between log(species) ~ log(endemics) in panel C it illustrates that this is pretty well-modelled using a linear line.\nThere is a slight issue though. If you look carefully then you see that in both panels C and D there is a stray value left of zero. There is also a warning message, saying Removed 1 row containing non-finite outside the scale range. If you’d look at the data, you’d notice that there is one island where the number of endemics is equal to 0. If we take the log(0) we get minus infinity. Which has little biological relevance, of course.\nWe could adjust for this by adding a “pseudo-count”, or adding 1 to all of the counts. If that is acceptable or not is a matter of debate and I’ll leave it to you to ponder over this. I would say that, whatever you do, make sure that you are transparent and clear on what you are doing and what the justification for it it.",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  },
  {
    "objectID": "materials/glm-practical-negative_binomial.html#summary",
    "href": "materials/glm-practical-negative_binomial.html#summary",
    "title": "\n12  Dealing with overdispersion\n",
    "section": "\n12.6 Summary",
    "text": "12.6 Summary\n\n\n\n\n\n\nKey points\n\n\n\n\nNegative binomial regression relaxes the assumption made by Poisson regressions that the variance is equal to the mean.\nIn a negative binomial regression the dispersion parameter \\(\\theta\\) is estimated from the data, whereas in a regular Poisson regression it is assumed to be \\(1\\).",
    "crumbs": [
      "Count outcomes",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Dealing with overdispersion</span>"
    ]
  }
]