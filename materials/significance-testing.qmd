---
title: "Significance & goodness-of-fit"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

Generalised linear models are fitted a little differently to standard linear models - namely, using maximum likelihood estimation instead of ordinary least squares for estimating the model coefficients.

As a result, we can no longer use F-tests for significance, or interpret $R^2$ values in quite the same way. This section will discuss new techniques for significance and goodness-of-fit testing, specifically for use with GLMs.

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R
```{r}
#| eval: false
install.packages("lmtest")
library(lmtest)
```
## Python
```{python}
#| eval: false
from scipy.stats import *
```
:::

:::

## Deviance

Several of the tests and metrics we'll discuss below are based heavily on deviance. So, what is deviance, and where does it come from?

Fitting a model using maximum likelihood estimation - the method that we use for GLMs, among other models - is all about finding the parameters that maximise the **likelihood**, or joint probability, of the sample. In other words, how likely is it that you would sample a set of data points like these, if they were being drawn from an underlying population where your model is true? Each model that you fit has its own likelihood.

Now, for each dataset, there is a "saturated", or perfect, model. This model has the same number of parameters in it as there are data points, meaning the data are fitted exactly - as if connecting the dots between them. The **saturated model** has the largest possible likelihood of any model fitted to the dataset.

Of course, we don't actually use the saturated model for drawing real conclusions, but we can use it as a baseline for comparison. We compare each model that we fit to this saturated model, to calculate the **deviance**. Deviance is defined as the difference between the log-likelihood of your fitted model and the log-likelihood of the saturated model (multiplied by 2). 

Because deviance is all about capturing the discrepancy between fitted and actual values, it's performing a similar function to the residual sum of squares (RSS) in a standard linear model. In fact, the RSS is really just a specific type of deviance.

![Different models and their deviances](images/deviance.png){width=70%}

## Significance testing

There are a few different potential sources of p-values for a generalised linear model. 

Here, we'll briefly discuss the p-values that are reported "as standard" in a typical GLM model output.

Then, we'll spend most of our time focusing on likelihood ratio tests, perhaps the most effective way to assess significance in a GLM.

### Revisiting the diabetes dataset

As a worked example, we'll use a logistic regression fitted to the `diabetes` dataset that we saw in a previous section.

::: {.panel-tabset group="language"}
## R

```{r}
diabetes <- read_csv("data/diabetes.csv")
```

## Python

```{python}
diabetes_py = pd.read_csv("data/diabetes.csv")

diabetes_py.head()
```
:::

As a reminder, this dataset contains three variables:

- `test_result`, binary results of a diabetes test result (1 for positive, 0 for negative)
- `glucose`, the results of a glucose tolerance test
- `diastolic` blood pressure

::: {.panel-tabset group="language"}
## R

```{r}
glm_dia <- glm(test_result ~ glucose * diastolic,
                  family = "binomial",
                  data = diabetes)
```

## Python

```{python}
model = smf.glm(formula = "test_result ~ glucose * diastolic", 
                family = sm.families.Binomial(), 
                data = diabetes_py)
                
glm_dia_py = model.fit()
```
:::

### Wald tests

Let's use the `summary` function to see the model we've just fitted.

::: {.panel-tabset group="language"}
## R

```{r}
summary(glm_dia)
```

## Python

```{python}
print(glm_dia_py.summary())
```
:::

Whichever language you're using, you may have spotted some p-values being reported directly here in the model summaries. Specifically, each individual parameter, or coefficient, has its own z-value and associated p-value.

A hypothesis test has automatically been performed for each of the parameters in your model, including the intercept and interaction. In each case, something called a **Wald test** has been performed.

The null hypothesis for these Wald tests is that the value of the coefficient = 0. The idea is that if a coefficient isn't significantly different from 0, then that parameter isn't useful and could be dropped from the model. These tests are the equivalent of the t-tests that are calculated as part of the `summary` output for standard linear models.

Importantly, these Wald tests *don't* tell you about the significance of the overall model. For that, we're going to need something else: a likelihood ratio test.

### Likelihood ratio tests (LRTs)

When we were assessing the significance of standard linear models, we were able to use the F-statistic to determine:

- the significance of the model versus a null model, and
- the significance of individual predictors.

We can't use these F-tests for GLMs, but we can use LRTs in a really similar way, to calculate p-values for both the model as a whole, and for individual variables.

These tests are all built on the idea of deviance, or the likelihood ratio, as discussed above on this page. We can compare any two models fitted to the same dataset by looking at the difference in their deviances, also known as the difference in their log-likelihoods, or more simply as a likelihood ratio.

Helpfully, this likelihood ratio approximately follows a chi-square distribution, which we can capitalise on that to calculate a p-value. All we need is the number of degrees of freedom, which is equal to the difference in the number of parameters of the two models you're comparing.

::: {.callout-warning}
Importantly, we are only able to use this sort of test when one of the two models that we are comparing is a "simpler" version of the other, i.e., one model has a subset of the parameters of the other model. 

So while we could perform an LRT just fine between these two models: `Y ~ A + B + C` and `Y ~ A + B + C + D`, or between any model and the null (`Y ~ 1`), we would not be able to use this test to compare `Y ~ A + B + C` and `Y ~ A + B + D`.
:::

#### Testing the model versus the null

Since LRTs involve making a comparison between two models, we must first decide which models we're comparing, and check that one model is a "subset" of the other.

Let's use an example from a previous section of the course, where we fitted a logistic regression to the `diabetes` dataset. 

::: {.panel-tabset group="language"}
## R

The first step is to create the two models that we want to compare: our original model, and the null model (with and without predictors, respectively).

```{r}
glm_dia <- glm(test_result ~ glucose * diastolic,
                  family = "binomial",
                  data = diabetes)

glm_null <- glm(test_result ~ 1, 
                family = binomial, 
                data = diabetes)
```

Then, we use the `lrtest` function from the `lmtest` package to perform the test itself; we include both the models that we want to compare, listing them one after another.

```{r}
lrtest(glm_dia, glm_null)
```

We can see from the output that our chi-square statistic is significant, with a really small p-value. This tells us that, for the difference in degrees of freedom (here, that's 3), the change in deviance is actually quite big. (In this case, you can use `summary(glm_dia)` to see those deviances - 936 versus 748!)

In other words, our model is better than the null.

## Python

The first step is to create the two models that we want to compare: our original model, and the null model (with and without our predictor, respectively).

```{python}
model = smf.glm(formula = "test_result ~ glucose * diastolic", 
                family = sm.families.Binomial(), 
                data = diabetes_py)
                
glm_dia_py = model.fit()

model = smf.glm(formula = "test_result ~ 1",
                family = sm.families.Binomial(),
                data = diabetes_py)

glm_null_py = model.fit()
```

Unlike in R, there isn't a nice neat function for extracting the $\chi^2$ value, so we have to do a little bit of work by hand.

```{python}
# calculate the likelihood ratio (i.e. the chi-square value)
lrstat = -2*(glm_null_py.llf - glm_dia_py.llf)

# calculate the associated p-value
pvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_null_py.df_model)

print(lrstat, pvalue)
```

This gives us the likelihood ratio, based on the log-likelihoods that we've extracted directly from the models, which approximates a chi-square distribution. 

We've also calculated the associated p-value, by providing the difference in degrees of freedom between the two models (in this case, that's simply 1, but for more complicated models it's easier to extract the degrees of freedom directly from the model as we've done here).

Here, we have a large chi-square statistic and a small p-value. This tells us that, for the difference in degrees of freedom (here, that's 1), the change in deviance is actually quite big. (In this case, you can use `summary(glm_dia)` to see those deviances - 936 versus 748!)

In other words, our model is better than the null.
:::

### Testing individual predictors

As well as testing the overall model versus the null, we might want to test particular predictors to determine whether they are individually significant.

The way to achieve this is essentially to perform a series of "targeted" likelihood ratio tests. In each LRT, we'll compare two models that are almost identical - one with, and one without, our variable of interest in each case.

::: {.panel-tabset group="language"}
## R

The first step is to construct a new model that doesn't contain our predictor of interest. Let's test the `glucose:diastolic` interaction.

```{r}
glm_dia_add <- glm(test_result ~ glucose + diastolic,
                  family = "binomial",
                  data = diabetes)
```

Now, we use the `lrtest` function to compare the models with and without the interaction:

```{r}
lrtest(glm_dia, glm_dia_add)
```

This tells us that our interaction `glucose:diastolic` isn't significant - our more complex model doesn't have a meaningful reduction in deviance.

This might, however, seem like a slightly clunky way to test each individual predictor. Luckily, we can also use our trusty `anova` function with an extra argument to tell us about individual predictors. 

By specifying that we want to use a chi-squared test, we are able to construct an analysis of deviance table (as opposed to an analysis of variance table) that will perform the likelihood ratio tests for us for each predictor:

```{r}
anova(glm_dia, test="Chisq")
```

You'll spot that the p-values we get from the analysis of deviance table match the p-values you could calculate yourself using `lrtest`; this is just more efficient when you have a complex model!

## Python

The first step is to construct a new model that doesn't contain our predictor of interest. Let's test the `glucose:diastolic` interaction.

```{python}
model = smf.glm(formula = "test_result ~ glucose + diastolic", 
                family = sm.families.Binomial(), 
                data = diabetes_py)
                
glm_dia_add_py = model.fit()
```

We'll then use the same code we used above, to compare the models with and without the interaction:

```{python}
lrstat = -2*(glm_dia_add_py.llf - glm_dia_py.llf)

pvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_dia_add_py.df_model)

print(lrstat, pvalue)
```

This tells us that our interaction `glucose:diastolic` isn't significant - our more complex model doesn't have a meaningful reduction in deviance.
:::

## Goodness-of-fit

Goodness-of-fit is all about how well a model fits the data, and typically involves summarising the discrepancy between the actual data points, and the fitted/predicted values that the model produces.

Though closely linked, it's important to realise that goodness-of-fit and significance don't come hand-in-hand automatically: we might find a model that is significantly better than the null, but is still overall pretty rubbish at matching the data. So, to understand the quality of our model better, we should ideally perform both types of test. 

### Chi-square tests

Once again, we can make use of deviance and chi-square tests, this time to assess goodness-of-fit.

Above, we used likelihood ratio tests to assess the null hypothesis that our candidate fitted model and the null model had the same deviance.

Now, however, we will test the null hypothesis that the fitted model and the saturated (perfect) model have the same deviance, i.e., that they both fit the data equally well. In most hypothesis tests, we want to reject the null hypothesis, but in this case, we'd like it to be true.

::: {.panel-tabset group="language"}
## R

Running a goodness-of-fit chi-square test in R can be done using the `pchisq` function. We need to include two arguments: 1) the residual deviance, and 2) the residual degrees of freedom. Both of these can be found in the `summary` output, but you can use the `$` syntax to call these properties directly like so:

```{r}
1 - pchisq(glm_dia$deviance, glm_dia$df.residual)
```

## Python

The syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:

```{python}
pvalue = chi2.sf(glm_dia_py.deviance, glm_dia_py.df_resid)

print(pvalue)
```
:::

You can think about this p-value, roughly, as "the probability that this model is good". We're not below our significance threshold, which means that we're not rejecting our null hypothesis (which is a good thing) - but it's also not a huge probability. This suggests that there's probably other variables we could measure and include in a future experiment, to give a better overall model.

### AIC values

You might remember AIC values from standard linear modelling. AIC values are useful, because they tell us about overall model quality, factoring in both goodness-of-fit and model complexity.

One of the best things about the Akaike information criterion (AIC) is that it isn't specific to linear models - it works for models fitted with maximum likelihood estimation.

In fact, if you look at the formula for AIC, you'll see why:

$$
AIC = 2k - 2ln(\hat{L})
$$

where $k$ represents the number of parameters in the model, and $\hat{L}$ is the maximised likelihood function. In other words, the two parts of the equation represent the complexity of the model, versus the log-likelihood.

This means that AIC can be used for model comparison for GLMs in precisely the same way as it's used for linear models: lower AIC indicates a better-quality model.

::: {.panel-tabset group="language"}
## R

The AIC value is given as standard, near the bottom of the `summary` output (just below the deviance values). You can also print it directly using the `$` syntax:

```{r}
summary(glm_dia)

glm_dia$aic
```

In even better news for R users, the `step` function works for GLMs just as it does for linear models, so long as you include the `test = LRT` argument.

```{r}
step(glm_dia, test = "LRT")
```

## Python

The AIC value isn't printed as standard with the model summary, but you can access it easily like so:

```{python}
print(glm_dia_py.aic)
```
:::

### Pseudo r-squared

We can't use $R^2$ values to represent the amount of variance explained in a GLM. This is primarily because, while linear models are fitted by minimising the squared residuals, GLMs are fitted by maximising the likelihood - an entirely different procedure.

However, because $R^2$ values are so useful in linear modelling, statisticians have developed something called a "pseudo $R^2$" for GLMs.

::: {.callout-note}
#### Debate about pseudo $R^2$ values

There are two main areas of debate:

1. Which version of pseudo $R^2$ to use? 

There are many. Some of the most popular are McFadden's, Nagelkerke's, Cox & Snell's, and Tjur's. They all have slightly different formulae and in some cases can give quite different results. [This post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/) does a nice job of discussing some of them and providing some comparisons.

2. Should pseudo $R^2$ values be calculated at all? 

Well, it depends what you want them for. Most statisticians tend to advise that pseudo $R^2$ values are only really useful for model comparisons (i.e., comparing different GLMs fitted to the same dataset). This is in contrast to the way that we use $R^2$ values in linear models, as a measure of effect size that is generalisable across studies.

So, if you choose to use pseudo $R^2$ values, try to be thoughtful about it; and avoid the temptation to over-interpret! 
:::

## Summary

Likelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and goodness-of-fit. To determine the quality of a model and draw conclusions from it, it's important to assess both of these things.

::: {.callout-tip}
#### Key points

- Deviance is the difference between predicted and actual values, and is calculated by comparing a model's log-likelihood to that of the perfect "saturated" model 
- Using deviance, likelihood ratio tests can be used in lieu of F-tests for generalised linear models
- Similarly, a chi-square goodness-of-fit test can also be performed using likelihood/deviance
- The Akaike information criterion is also based on likelihood, and can be used to compare the quality of GLMs fitted to the same dataset
- Other metrics that may be of use are Wald test p-values and pseudo $R^2$ values
:::
