---
title: "Overdispersed count data"
lightbox: true
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
## Learning outcomes

-   Understand what dispersion is and why it's important
-   Be able to diagnose or recognise overdispersion in count data
-   Fit a negative binomial regression model to overdispersed count data
-   Know the difference between Poisson and negative binomial regression
-   Assess whether assumptions are met and evaluate fit of a negative binomial model
:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries

```{r}
#| eval: false
library(MASS)
library(performance)
library(tidyverse)
library(broom)
```

## Python

### Libraries

```{python}
#| eval: false
# A maths library
import math
import pandas as pd
from plotnine import *
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import *
import numpy as np
from statsmodels.discrete.discrete_model import NegativeBinomial
```
:::
:::

## Removing dispersion assumptions

In the previous chapter we looked at how to analyse count data. We used a Poisson regression to do this, which makes the assumption that the dispersion parameter $\approx 1$ (for a refresher on dispersion, see [Section @sec-mat_dispersion]).

So what happens if the data is over- or under-dispersed?

Cue negative binomial models.

Negative binomial models are also used for count data, but they have one crucial difference from Poisson regression: instead of making an assumption about the dispersion parameter, they actually estimate it as part of the model fitting.

## Parasites dataset

We'll explore this with the `parasites` dataset. 

These data are about parasites found on the gills of freshwater fish.

The ecologists who conducted the research observed a total of 64 fish, which varied by:

-   which `lake` they were found in
-   the `fish_length` (cm)

They were interested in how these factors influenced the `parasite_count` of small parasites found on each fish.

## Load and visualise the data

First we load the data, then we visualise it.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
parasites <- read_csv("data/parasites.csv")

head(parasites)
```

```{r}
#| warning: false

ggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +
  geom_point()

ggplot(parasites, aes(y = parasite_count, x = lake, colour = fish_length)) +
  geom_violin() +
  geom_jitter()
```

## Python

```{python}
parasites = pd.read_csv("data/parasites.csv")

parasites.head()
```

```{python}
#| warning: false
p = (ggplot(parasites, aes(x = "fish_length", y = "parasite_count",
                           colour = "lake")) +
       geom_point())

p.show()

p = (ggplot(parasites, aes(x = "lake", y = "parasite_count",
                       colour = "fish_length")) +
   geom_violin() +
   geom_jitter())

p.show()
```
:::

We get a reasonably clear picture here; it seems that `lake C` might have a higher number of parasites overall, and that across all three lakes, bigger fish have more parasites too.

## Constructing a Poisson model

Given that the response variable, `parasite_count`, is a count variable, let's try to construct a Poisson regression. 

We'll construct the full model, with an interaction.

::: {.panel-tabset group="language"}
## R

```{r}
glm_para <- glm(parasite_count ~ lake * fish_length,
                data = parasites, family = "poisson")
```

and we look at the model summary:

```{r}
summary(glm_para)
```

## Python

```{python}
model = smf.glm(formula = "parasite_count ~ lake * fish_length",
                family = sm.families.Poisson(),
                data = parasites)

glm_para = model.fit()
```

Let's look at the model output:

```{python}
print(glm_para.summary())
```
:::

### Checking dispersion

Before we launch into any significance testing, we're going to check one of the assumptions: that the dispersion parameter = 1 (you might've guessed where this is going...)

::: {.panel-tabset group="language"}
## R

```{r}
check_overdispersion(glm_para)
```

## Python
```{python}
print(glm_para.deviance/glm_para.df_resid)

pvalue = chi2.sf(glm_para.deviance, glm_para.df_resid)
print(pvalue)
```
:::

Oh no - there is *definitely* overdispersion here.

We won’t bother looking at the analysis of deviance table or asking whether the model is better than the null model. The Poisson regression we've fitted here is not right, and we need to fit a different type of model.

The main alternative to a Poisson model, used for overdispersed count data, is something called a negative binomial model. 

## Negative binomial model

::: {.panel-tabset group="language"}
## R

To specify a negative binomial model, we use the `glm.nb` function from the `MASS` package.

The syntax is the same as `glm`, except we don't need to specify a `family` argument.

```{r}
nb_para <- glm.nb(parasite_count ~ lake * fish_length, parasites)

summary(nb_para)
```

This output is very similar to the other GLM outputs that we’ve seen but with some additional information at the bottom regarding the dispersion parameter that the negative binomial model has used, which in R is called theta ($\theta$), `r round(nb_para$theta, 3)`. 

This number is estimated from the data. This is what makes negative binomial regression different from Poisson regression.

## Python

We can continue to use `statsmodels`, specifically the `NegativeBinomial` function.

The syntax for getting this to work is a little different, and takes several steps.

First, we identify our response variable:

```{python}
Y = parasites['parasite_count']
```

Now, we need to set up our set of predictors. The function we're using requires us to provide a matrix where each row is a unique observation (fish, in this case) and each column is a predictor.

Because we want to include the `lake:fish_length` interaction, we actually need to manually generate those columns ourselves, like this:

```{python}
# 1. Create dummy variables for 'lake' (switch to True/False values)
lake_dummies = pd.get_dummies(parasites['lake'], drop_first=True)

# 2. Create interaction terms manually: lake_dummies * fish_length
interactions = lake_dummies.multiply(parasites['fish_length'], axis=0)
interactions.columns = [f'{col}:fish_length' for col in interactions.columns]

# 3. Combine all predictors: fish_length, lake dummies, interactions
X = pd.concat([parasites['fish_length'], lake_dummies, interactions], axis=1)

# 4. Add a constant/intercept (required), and make sure that we're using floats
X = sm.add_constant(X).astype(float)
```

Now, we can fit our model and look at the summary:

```{python, nb_para fit}
from statsmodels.discrete.discrete_model import NegativeBinomial

# Specify the model (Y, X)
model = NegativeBinomial(Y, X)

nb_para = model.fit()

print(nb_para.summary())
```

In particular, we might like to know what the dispersion parameter is.

By default, `NegativeBinomial` from `statsmodels` provides something called alpha ($\alpha$), which is not the same as the significance threshold. To convert this into the dispersion parameter you're using to seeing (sometimes referred to as theta, $\theta$), you need $\frac{1}{\alpha}$.

```{python}
print(1/nb_para.params['alpha'])
```
:::

### Extract equation

If we wanted to express our model as a formal equation, we need to extract the coefficients:

::: {.panel-tabset group="language"}
## R

```{r}
coefficients(nb_para)
```

## Python

```{python}
print(nb_para.params)
```
:::

```{r}
#| echo: false

b0 <- round(unname(coefficients(nb_para)[1]), 2)
b1b <- round(unname(coefficients(nb_para)[2]), 2)
b1c <- round(unname(coefficients(nb_para)[3]), 3)
b2 <- round(unname(coefficients(nb_para)[4]), 3)
b3b <- round(unname(coefficients(nb_para)[5]), 4)
b3c <- round(unname(coefficients(nb_para)[6]), 4)
```


These are the coefficients of the negative binomial model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of the other variables:

$$
\begin{split}
E(species) = \exp(`r b0` + \begin{pmatrix} 0 \\ `r b1b` \\ `r b1c` \end{pmatrix} \begin{pmatrix} A \\ B \\ C \end{pmatrix} + `r b2` \times fishlength + \begin{pmatrix} 0 \\ `r b3b` \\ `r b3c` \end{pmatrix} \begin{pmatrix} A \\ B \\ C \end{pmatrix} \times fishlength)
\end{split}
$$

### Comparing Poisson and negative binomial

The main difference between a Poisson regression and a negative binomial regression is this: in the former the dispersion parameter is assumed to be 1, whereas in the latter it is estimated from the data.

They both, however, use the same link function - the log link function.

This means they will both have equations in the same form, as we just showed above. It also means that they can produce some quite similar-looking models.

Let's visualise and compare them.

First, let's plot the Poisson model:

::: {.panel-tabset group="language"}
## R

```{r}
#| warning: false
#| message: false

ggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE, 
              method.args = list(family = "poisson"))
```

## Python

To do this, we need to produce a set of predictions, which we can then plot.

```{python}
# Get unique lake values from your dataset
lake_levels = parasites['lake'].unique()

# Create prediction grid for each lake
new_data = pd.concat([
    pd.DataFrame({
        'fish_length': np.linspace(10, 40, 100),
        'lake': lake
    }) for lake in lake_levels
])

# Predict
new_data['pred'] = glm_para.predict(new_data)

new_data.head()
```

```{python}
p = (ggplot(parasites, aes(x = "fish_length",
                       y = "parasite_count",
                       colour = "lake")) +
   geom_point() +
   geom_line(new_data, aes(x = "fish_length", y = "pred",
                           colour = "lake"), size = 1))

p.show()
```
:::

Now, let's plot the negative binomial model:

::: {.panel-tabset group="language"}
## R

All we have to do is switch the `method` for `geom_smooth` over to `glm.nb`:

```{r}
#| warning: false
#| message: false

ggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE)
```

## Python

```{python}
# Get unique lake values from your dataset
lake_levels = parasites['lake'].unique()

# Create prediction grid for each lake
new_data_nb = pd.concat([
    pd.DataFrame({
        'fish_length': np.linspace(10, 40, 100),
        'lake': lake
    }) for lake in lake_levels
])

# Repeat the transformations we used when fitting the model
lake_dummies = pd.get_dummies(new_data['lake'], drop_first=True)
interactions = lake_dummies.multiply(new_data_nb['fish_length'], axis=0)
interactions.columns = [f'{col}:fish_length' for col in interactions.columns]
X_new = pd.concat([new_data_nb['fish_length'], lake_dummies, interactions], axis=1)
X_new = sm.add_constant(X_new).astype(float)

# Now we can safely predict:
new_data_nb['pred'] = nb_para.predict(X_new)

new_data_nb.head()
```

```{python}
p = (ggplot(parasites, aes(x = "fish_length",
                       y = "parasite_count",
                       colour = "lake")) +
   geom_point() +
   geom_line(new_data_nb, aes(x = "fish_length", y = "pred",
                              colour = "lake"), size = 1))

p.show()
```
:::

It's subtle - the two sets of curves look quite similar - but they're not quite the same.

Looking at them side-by-side may help:

```{r}
#| echo: false
#| message: false
library(patchwork)

poi <- ggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE, 
              method.args = list(family = "poisson")) +
  labs(title = "Poisson")

nb <- ggplot(parasites, aes(y = parasite_count, x = fish_length, colour = lake)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  labs(title = "Negative binomial")

poi + nb + plot_layout(guides = "collect")
```


### Checking assumptions & model quality

These steps are performed in precisely the same way we would do for a Poisson regression.

Remember, the things we checked for in a Poisson regression were:

-   Response distribution
-   Correct link function
-   Independence
-   Influential points
-   Collinearity
-   Dispersion

For free, I'll just tell you now that the first three assumptions are met.

Plus, we don't have to worry about dispersion with a negative binomial model. 

So, all we really need to pay attention to is whether there are any influential points to worry about, or any collinearity.

::: {.panel-tabset group="language"}
## R

Once again, the `performance` package comes in handy for assessing all of these.

```{r}
check_model(nb_para, check = c('pp_check',
                               'vif',
                               'outliers'))

check_outliers(nb_para, threshold = list('cook'=0.5))

check_collinearity(nb_para)
```

## Python

We check these things similarly to how we've done it previously.

For influential points, we're actually going to refit our model via `smf.glm` (which will give us access to `get_influence`). However, we'll use the `alpha` value that was estimated from the model we fitted with `NegativeBinomial`, rather than making any assumptions about its value.

```{python}
alpha_est = nb_para.params['alpha']

model = smf.glm(formula='parasite_count ~ lake * fish_length',
                data=parasites,
                family=sm.families.NegativeBinomial(alpha = alpha_est))

glm_nb_para = model.fit()
```

Now we can check influential points:

```{python}
influence = glm_nb_para.get_influence()

cooks = influence.cooks_distance[0]

influential_points = np.where(cooks > 0.5)[0] # Set appropriate threshold here

print("Influential points:", influential_points)
```

There seem to be a couple of points with a higher Cook's distance. If you investigate a bit further, they both still have a Cook's distance of <1, and neither of them cause dramatic changes to the model fit if removed.

The method that we're using here to assess influence is quite sensitive, so a threshold of 1 might be more appropriate.

Collinearity/variance inflation factors:

```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor
from patsy import dmatrix

# Drop the response variable
X = parasites.drop(columns='parasite_count')

# Create design matrix based on model formula
X = dmatrix("lake * fish_length", data=parasites, return_type='dataframe')

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i)
                   for i in range(X.shape[1])]

print(vif_data)
```

:::

The posterior predictive check looks alright, and there's no high leverage points to worry about, but we seem to have some potential issues around our main effect of `lake` along with the `lake:fish_length` interaction.

This is often a sign that that the interaction term is unnecessary. Let's test that theory with some significance testing and model comparison.

### Significance & model comparison

Let's see whether any of our predictors are significant (main effects and/or interaction effect).

::: {.panel-tabset group="language"}
## R

We can use the trusty `anova` and `step` functions:

```{r}
anova(nb_para, test = "Chisq")

step(nb_para)
```

Alternatively, we could fit an additive model and compare the two directly:

```{r}
nb_para_add <- glm.nb(parasite_count ~ lake + fish_length, parasites)

anova(nb_para, nb_para_add)
```

This tells us that the interaction term is not significant.

## Python

Let's start by fitting a new additive model without our interaction.

Again, we start by specifying our response variable.

```{python}
Y = parasites['parasite_count']
```

This next bit is much simpler than before, without the need to generate the interaction:

```{python}
# Create dummy variables for 'lake' (switch to True/False values)
lake_dummies = pd.get_dummies(parasites['lake'], drop_first=True)

# Combine the predictors: fish_length, lake dummies
X = pd.concat([parasites['fish_length'], lake_dummies], axis=1)

# Add a constant/intercept (required), and make sure that we're using floats
X = sm.add_constant(X).astype(float)
```

Now, we can fit our model and look at the summary.

```{python, nb_para_add fit}
#| message: false
#| warning: false
# Specify the model (Y, X)
model = NegativeBinomial(Y, X)

nb_para_add = model.fit()
```

Now, we want to compare the two models (`nb_para` and `nb_para_add`).

```{python}
lrstat = -2*(nb_para_add.llf - nb_para.llf)

pvalue = chi2.sf(lrstat, nb_para_add.df_resid - nb_para.df_resid)

print(lrstat, pvalue)
```

This tells us that the interaction term is not significant.
:::

Dropping the interaction will also solve our VIF problem:

::: {.panel-tabset group="language"}
## R

```{r}
check_collinearity(nb_para_add)
```

## Python

```{python}
# Drop the response variable
X = parasites.drop(columns='parasite_count')

# Create design matrix based on model formula
X = dmatrix("lake + fish_length", data=parasites, return_type='dataframe')

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i)
                   for i in range(X.shape[1])]

print(vif_data)
```

:::

Excellent. Seems we have found a well-fitting model for our `parasites` data!

## Exercises

### Bacteria colonies {#sec-exr_bacteria}

::: {.callout-exercise #ex-bacteria}
{{< level 3 >}}

In this exercise, we'll use the `bacteria` dataset.

Each row of the dataset represents a petri dish on which bacterial colonies were grown. Each dish was given one of three antibacterial treatments, and then after enough time had passed (between 12 and 24 hours), the number of colonies on the dish were counted. Due to variation in the lab, not all dishes were assessed at the same time.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
bacteria <- read_csv("data/bacteria.csv")

head(bacteria)
```

## Python

```{python}
bacteria = pd.read_csv("data/bacteria.csv")

bacteria.head()
```
:::

The dataset contains three variables in total:

-   The response variable `colony_count`
-   The `treatment` (control/high/low)
-   The `incubation_time` (hrs)

You should:

1. Fit an additive (no interaction) negative binomial model
2. Visualise the model*
3. Evaluate whether the assumptions are met & the model fits well
4. Test the significance of the model versus the null**

*R users, this will involve a bit of thinking; consider using the `broom::augment` function to help you.

**Python users, this will involve adapting previous code in a new way.

:::: {.callout-answer collapse="true"}

#### Fit an additive model

::: {.panel-tabset group="language"}
## R

```{r}
nb_petri <- glm.nb(colony_count ~ treatment + incubation_time, bacteria)

summary(nb_petri)
```

## Python

Identify response variable:

```{python}
Y = bacteria['colony_count']
```

Set up predictors:

```{python}
# Create dummy variable for categorical treatment predictor
treatment_dummies = pd.get_dummies(bacteria['treatment'], drop_first=True)

# Combine predictors
X = pd.concat([bacteria['incubation_time'], treatment_dummies], axis=1)

# Add a constant/intercept (required), cast to float
X = sm.add_constant(X).astype(float)
```

Fit model:

```{python, nb_petri fit}
model = NegativeBinomial(Y, X)

nb_petri = model.fit()

print(nb_petri.summary())
```
:::

#### Visualise the model

::: {.panel-tabset group="language"}
## R

Because we've fitted a negative binomial model without an interaction term, we can't just use `geom_smooth` (it will automatically include the interaction, which is incorrect).

So, we're going to use the `broom::augment` function to extract some fitted values. We need to exponentiate these values to get the *actual* predicted counts, which we can then model with `geom_line`, like this:

```{r}
library(broom)

# created augmented model object
petri_aug <- augment(nb_petri)
# add a new predicted column (exp(fitted))
petri_aug$.predicted <- exp(petri_aug$.fitted) 

ggplot(petri_aug, aes(y = colony_count, x = incubation_time, colour = treatment)) +
  geom_point() +
  geom_line(mapping = aes(y = .predicted), linewidth = 1)
```

Here's a more efficient way of doing exactly the same thing - we put the augmented object in directly as our data, and do the exponentiation inside the `geom_line` `aes` function:

```{r}
#| eval: false
ggplot(augment(nb_petri), aes(y = colony_count, x = incubation_time, colour = treatment)) +
  geom_point() +
  geom_line(mapping = aes(y = exp(.fitted)), linewidth = 1)
```

Notice how this is subtly different from the model we would've visualised, if we'd just used `geom_smooth`:

```{r}
#| message: false
ggplot(petri_aug, aes(y = colony_count, x = incubation_time, colour = treatment)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE) +
  labs(title = "This is not the model we wanted!")
```


## Python

We can copy and adapt the code we used for the `parasites` example in the main body of the chapter - just remember to change all the names!

```{python}
# Get unique treatment values
treatment_levels = bacteria['treatment'].unique()

# Create prediction grid for each lake
new_data_nb = pd.concat([
    pd.DataFrame({
        'incubation_time': np.linspace(12, 24, 100), # set sensible start & end!
        'treatment': treatment
    }) for treatment in treatment_levels
])

# Repeat the transformations we used when fitting the model
treatment_dummies = pd.get_dummies(new_data_nb['treatment'], drop_first=True)
X_new = pd.concat([new_data_nb['incubation_time'], treatment_dummies], axis=1)
X_new = sm.add_constant(X_new).astype(float)

# Now we can safely predict:
new_data_nb['pred'] = nb_petri.predict(X_new)

new_data_nb.head()
```

```{python}
p = (ggplot(bacteria, aes(x = "incubation_time",
                      y = "colony_count",
                      colour = "treatment")) +
   geom_point() +
   geom_line(new_data_nb, aes(x = "incubation_time", y = "pred",
                              colour = "treatment"), size = 1))
                              
p.show()
```
:::

#### Evaluate assumptions & fit

We don't need to worry about dispersion - we're explicitly modelling that, rather than making any assumptions.

We can be reasonably sure we've chosen the right response variable distribution and link function; these are clearly count data.

With the info we've got, we don't have any reason to be worried about independence. If, however, we found out more about the design - for example, that the petri dishes had been processed in batches, perhaps by different researchers - that might set off alarm bells.

::: {.panel-tabset group="language"}
## R

```{r}
check_model(nb_petri, check = c('pp_check', 'outliers', 'vif'))

check_collinearity(nb_petri)
check_outliers(nb_petri, threshold = list('cook'=0.5))
```

No obvious issues to report!

## Python

Refit the model with `smf.glm`, so we can access `get_influence`. Don't forget to include the estimated `alpha`.

```{python}
alpha_est = nb_petri.params['alpha']

model = smf.glm(formula='colony_count ~ treatment + incubation_time',
                data=bacteria,
                family=sm.families.NegativeBinomial(alpha = alpha_est))

glm_nb_petri = model.fit()
```

Now we can check influential points:

```{python}
influence = glm_nb_petri.get_influence()

cooks = influence.cooks_distance[0]

influential_points = np.where(cooks > 0.5)[0] # Set appropriate threshold here

print("Influential points:", influential_points)
```

We have one possible influential point; if this were a real analysis, we would want to follow up on this now, e.g., removing it and refitting, and seeing if our model shifts dramatically.

(Spoiler alert - it doesn't!)

Collinearity/variance inflation factors:

```{python}
# Drop the response variable
X = bacteria.drop(columns='colony_count')

# Create design matrix based on model formula
X = dmatrix("treatment + incubation_time", data=bacteria, return_type='dataframe')

# Calculate VIF for each feature
vif_data = pd.DataFrame()
vif_data['feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i)
                   for i in range(X.shape[1])]

print(vif_data)
```

No issues with the VIF values!
:::

#### Compare model to null

::: {.panel-tabset group="language"}
## R

```{r}
nb_petri_null <- glm.nb(colony_count ~ 1, bacteria)

anova(nb_petri, nb_petri_null)
```

## Python

We've not constructed a null model yet earlier in this chapter, so don't worry if you didn't figure this bit out on your own.

We still need to set up a `Y` response and an `X` predictor matrix. However, since we only want to regress `colony_count ~ 1`, our `X` matrix just needs to be a column of 1s, with the same length as `Y`.

We can set that up like this:

```{python}
Y = bacteria['colony_count']
X = np.ones((len(Y), 1))
```

From here, the model fitting proceeds exactly as before.

```{python}
#| message: false
#| warning: false
model = NegativeBinomial(Y, X)

nb_petri_null = model.fit()

print(nb_petri_null.summary())
```

We can see that the model, indeed, only contains a constant (intercept) with no predictors. That's what we wanted.

Now, we want to compare the two models (`nb_petri` and `nb_petri_null`). This reuses the likelihood ratio test code we've been using so far in the course:

```{python}
lrstat = -2*(nb_petri_null.llf - nb_petri.llf)

pvalue = chi2.sf(lrstat, nb_petri_null.df_resid - nb_petri.df_resid)

print(lrstat, pvalue)
```
:::

In conclusion: our additive model is significant over the null, and there are no glaring concerns with assumptions that are not met. 

Our visualisation of the model seems sensible too. The number of colonies increases with incubation time - this seems very plausible - and the high antibacterial condition definitely is stunted compared to the control, as we might expect.

Why does the low antibacterial dose condition seem to have more growth than the control? Well, this could just be a fluke. If we repeated the experiment the effect might vanish. 

Or, it might be an indication of a stress-induced growth response in those dishes (i.e., a small amount of antibacterial treatment reveals some sort of resistance).
::::
:::

### Galapagos models {#sec-exr_galapagos}

::: {.callout-exercise #ex-galapagos}

{{< level 3 >}}

For this exercise we'll be using the data from `data/galapagos.csv`.

In this dataset, each row corresponds to one of the 30 Galapagos islands. The relationship between the number of plant species (`species`) and several geographic variables is of interest:

* `endemics` – the number of endemic species
* `area` – the area of the island km<sup>2</sup>
* `elevation` – the highest elevation of the island (m)
* `nearest` – the distance from the nearest island (km)

However, fitting both a Poisson and a negative binomial regression produce quite bad results, with strange fit. 

In this exercise, you should:

1. Fit and visualise a Poisson model
2. Fit and visualise a negative binomial model
2. Explore ways to improve the fit of the model

(Hint: just because we're fitting a GLM, doesn't mean we can't still transform our data!)

::::: {.callout-warning collapse="true"}
#### A note for Python users

Due to the small (near-zero) beta coefficients for these data, the `NegativeBinomial` method we've been using struggles to converge on a successful model. Therefore, there is no Python code provided for this example.

However, you might find it useful to read through the answer regardless - this is a very interesting dataset!
:::::

:::: {.callout-answer collapse="true"}

```{r}
#| message: false
galapagos <- read_csv("data/galapagos.csv")
```

#### Fitting a Poisson model

```{r}
glm_gal <- glm(species ~ area + endemics + elevation + nearest,
               data = galapagos, family = "poisson")
```

```{r}
#| warning: false
#| message: false
p1 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE, 
              method.args = list(family = "poisson")) +
  labs(title = "Poisson")

p1
```

Our model is doing okay over on the right hand side, but it's not really doing a good job in the bottom left corner - which is where most of the data points are clustered!

#### Fitting a negative binomial model

```{r}
nb_gal <- glm.nb(species ~ area + endemics + elevation + nearest,
               data = galapagos)
```

```{r}
#| warning: false
#| message: false
p2 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  labs(title = "Negative binomial")

p2
```

This model does a better job in the lower left, but now is getting things a bit wrong over on the far right.

#### Improving the fit

Is there anything we can do to improve on these two models?

There appears to be a power relationship between `species` and `endemics`. So, one thing we could do is log-transform the response (`species`) and predictor (`endemics`) variables.

If we log-transform them both and put them on a scatterplot, they look like this - linear?!

```{r}
ggplot(data = galapagos,
       aes(x = log(endemics),
           y = log(species))) +
  geom_point()
```

We could add a regression line to this. 

Or, we could just log-transform `endemics` and fit a negative binomial model to that, without also log-transforming `species`. Let's do all of this and plot them together with the original Poisson and Negative binomial models.

```{r}
p3 <- ggplot(galapagos, aes(log(endemics), log(species))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE,
              method.args = list(family = poisson)) +
  labs(title = "Linear model of log-log")

p4 <- ggplot(galapagos, aes(log(endemics), species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  ylim(0, 500) +
  labs(title = "Negative binomial of log(endemics)")
```

```{r}
#| message: false
library(patchwork)

p1 + p2 + p3 + p4 +
  plot_annotation(tag_levels = "A")
```

From this it is clear that the negative binomial model fitted to `species ~ log(endemics)` in panel D produces a much better fit than the original fit in panel B.

Equally, looking at the relationship between `log(species) ~ log(endemics)` in panel C it illustrates that this is pretty well-modelled using a linear line.

There is a slight issue, though. 

If you look carefully then you see that in both panels C and D there is a stray value left of zero, almost escaping past the y-axis. There is also a warning message, saying `Removed 1 row containing non-finite outside the scale range`. 

With a closer look at the dataset, we can see that there is one island where the number of `endemics` is equal to 0: 

```{r}
galapagos %>% 
  arrange(endemics) %>%
  head(1)
```

If we take `log(0)` we get minus infinity, which isn't biologically possible. This is where the problem lies.

We could adjust for this by adding a "pseudo-count", or adding `1` to all of the counts. If that is acceptable or not is a matter of debate and we'll leave it to you to ponder over this. 

Whatever you do, the most important thing is to make sure that you are transparent and clear on what you are doing and what the justification for it it.
::::
:::

## Summary

::: {.callout-tip}
#### Key points

- Negative binomial regression relaxes the assumption made by Poisson regressions that the variance is equal to the mean (dispersion = 1)
- In a negative binomial regression the dispersion parameter $\theta$ is estimated from the data
- However, they both use the log link function and often produce similar-looking models
:::
