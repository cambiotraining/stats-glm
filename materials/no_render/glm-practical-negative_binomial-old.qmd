---
title: "Overdispersed count data"
lightbox: true
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: {.callout-tip}
## Learning outcomes

-   Understand what dispersion is and why it's important
-   Be able to diagnose or recognise overdispersion in count data
-   Fit a negative binomial regression model to overdispersed count data
-   Know the difference between Poisson and negative binomial regression
-   Assess whether assumptions are met and evaluate fit of a negative binomial model
:::

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

### Libraries

```{r}
#| eval: false
library(MASS)
library(performance)
library(tidyverse)
```

## Python

### Libraries

```{python}
#| eval: false
# A maths library
import math
import pandas as pd
from plotnine import *
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import *
import numpy as np
from statsmodels.discrete.discrete_model import NegativeBinomial
```
:::
:::

## Removing dispersion assumptions

In the previous chapter we looked at how to analyse count data. We used a Poisson regression to do this, which makes the assumption that the dispersion parameter $\approx 1$ (for a refresher on dispersion, see [Section @sec-mat_dispersion]).

So what happens if the data is over- or under-dispersed?

Cue negative binomial models.

Negative binomial models are also used for count data, but they have one crucial difference from Poisson regression: instead of making an assumption about the dispersion parameter, they actually estimate it as part of the model fitting.

We'll explore this with the `galapagos` example. You can find the data in:

`data/galapagos.csv`

There are 30 Galapagos islands and 4 variables in the data. The relationship between the number of plant species (`species`) and several geographic variables is of interest.

* `endemics` – the number of endemic species
* `area` – the area of the island km<sup>2</sup>
* `elevation` – the highest elevation of the island (m)
* `nearest` – the distance from the nearest island (km)

## Load and visualise the data

First we load the data, then we visualise it.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
galapagos <- read_csv("data/galapagos.csv")

head(galapagos)
```

## Python

```{python}
galapagos = pd.read_csv("data/galapagos.csv")

galapagos.head()
```
:::

We could plot the data using a series of scatterplots in `ggplot` or `plotnine`, and if you'd prefer to stick to those old classics, you are most welcome to.

However, this is a handy opportunity to introduce you to a new way of visualising datasets, via some other R/Python packages.

::: {.panel-tabset group="language"}
## R

The `ggpairs` function from `GGally` is designed to add extra functionality beyond `ggplot`.

Here, we mostly care about the first column - looking at `species` against each of the predictors - but we can get some idea for how the predictors might relate to one another as well.

```{r}
#| message: false
library(GGally)

ggpairs(galapagos)
```

## Python

In Python, the `seaborn` plotting package (which so far we haven't come across) offers some cool functionality via the `PairGrid` function.

```{python}
import seaborn as sns

g = sns.PairGrid(galapagos)
g.map_diag(sns.histplot)
g.map_offdiag(sns.scatterplot)
```
:::

It looks as though `endemics` and `elevation` might be related to `species`, but `area` and `nearest` are harder to work out.

## Constructing a model

Given that the response variable, `species`, is a count variable, let's try to construct a Poisson regression. 

To keep things simple, we won't bother with any interactions for now.

::: {.panel-tabset group="language"}
## R

```{r}
glm_gal <- glm(species ~ area + endemics + elevation + nearest,
               data = galapagos, family = "poisson")
```

and we look at the model summary:

```{r}
summary(glm_gal)
```

## Python

```{python}
model = smf.glm(formula = "species ~ area + endemics + elevation + nearest",
                family = sm.families.Poisson(),
                data = galapagos)

glm_gal = model.fit()
```

Let's look at the model output:

```{python}
print(glm_gal.summary())
```
:::

## Checking dispersion

Before we launch into any significance testing, we're going to check one of the assumptions: that the dispersion parameter = 1.

::: {.panel-tabset group="language"}
## R

```{r}
check_overdispersion(glm_gal)
```

## Python
```{python}
print(glm_gal.deviance/glm_gal.df_resid)

pvalue = chi2.sf(glm_gal.deviance, glm_gal.df_resid)
print(pvalue)
```
:::

Oh no - there is *definitely* overdispersion here.

We won’t bother looking at the analysis of deviance table or asking whether the model is better than the null model. The Poisson regression we've fitted here is not right, and we need to fit a different type of model.

The main alternative to a Poisson model, used for overdispersed count data, is something called a negative binomial model. 

## Negative binomial model

::: {.panel-tabset group="language"}
## R

To specify a negative binomial model, we use the `glm.nb` function from the `MASS` package.

The syntax is the same as `glm`, except we don't need to specify a `family` argument.

```{r}
nb_gal <- glm.nb(species ~ area + endemics + elevation + nearest,
               data = galapagos)

summary(nb_gal)
```

This output is very similar to the other GLM outputs that we’ve seen but with some additional information at the bottom regarding the dispersion parameter that the negative binomial model has used, which it calls Theta (`r round(nb_gal$theta, 3)`). It is estimated from the data.

As before, the main numbers to extract from the output are the coefficients:

```{r}
coefficients(nb_gal)
```

## Python

We can continue to use `statsmodels`, but we will need to use the `statsmodels.discrete.count_model` module.

Using `NegativeBinomialP` means that we will be explicitly estimating a dispersion parameter, which we very much want.

```{python}
#| warning: false
from statsmodels.discrete.discrete_model import NegativeBinomial

# Set up the model
X = sm.add_constant(galapagos[["area", "endemics", "elevation", "nearest"]])
Y = galapagos["species"]

model = NegativeBinomial(Y, X)

nb_gal = model.fit(maxiter=1000, tol=1e-2)

print(nb_gal.summary())
```

As before, the main numbers to extract from the output are the coefficients:

```{python}
print(nb_gal.params)
```
:::

These are the coefficients of the negative binomial model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of the other variables:

$$
\begin{split}
  E(species) = \exp(`r round(unname(coefficents(nb_gal)[1]), 2)` - round(unname(coefficents(nb_gal)[2]), 4) \times area + round(unname(coefficents(nb_gal)[3]), 3) \times endemics\\
  + \ round(unname(coefficents(nb_gal)[4]), 4) \times elevation + round(unname(coefficents(nb_gal)[5]), 3) \times nearest)
\end{split}
$$

::: {.callout-important}
#### Key concept

The main difference between a Poisson regression and a negative binomial regression: in the former the dispersion parameter is assumed to be 1, whereas in the latter it is estimated from the data.

As such, the negative binomial model has the same form for its line of best fit as the Poisson model, but the underlying probability distribution is different.
:::

### Assessing significance

We can ask the same three questions we asked before.

1. Is the model well-specified?
2. Is the overall model better than the null model?
3. Are any of the individual predictors significant?

To assess whether the model is any good we’ll use the residual deviance and the residual degrees of freedom.

::: {.panel-tabset group="language"}
## R

```{r}
1 - pchisq(33.395, 25)
```

Instead of manually typing in the values, which is of course prone to errors, we can also extract them directly from the model object:

```{r}
1 - pchisq(nb_gal$deviance, nb_gal$df.residual)
```

## Python
:::

This gives a probability of `r round(1 - pchisq(nb_gal$deviance, nb_gal$df.residual), 3)`. Whilst this isn’t brilliant, it is still much better than the model we had before, and now that we’ve taken account of the overdispersion issue, the fact that this probability is a bit small is probably down to the fact that the predictor variables we have in the model might not be enough to fully explain the number of `species` on each of the Galapagos islands. 

However, since we don’t have any other data to play with there’s nothing we can do about that right now.

To assess if the overall model, with all four terms, is better than the null model we’ll look at the difference in deviances and the difference in degrees of freedom:

::: {.panel-tabset group="language"}
## R

```{r}
1 - pchisq(151.446 - 33.395, 29 - 25)
```

Or extracting them directly from the model object:

```{r}
1 - pchisq(nb_gal$null.deviance - nb_gal$deviance,
           nb_gal$df.null - nb_gal$df.residual)
```

## Python
:::

This gives a reported p-value of `r 1 - pchisq(nb_gal$null.deviance - nb_gal$deviance, nb_gal$df.null - nb_gal$df.residual)`, which is pretty darn small. So, yes, this model is better than nothing at all and at least some of our predictors are related to the response variable in a meaningful fashion.

Finally, we’ll construct an analysis of deviance table to look at the individual predictors:

::: {.panel-tabset group="language"}
## R

```{r}
anova(nb_gal, test = "Chisq")
```

You might get a warning message about theta not being recalculated but this isn’t something to worry about.

## Python
:::

For more detail on the deviance table, refer to the chapter on [Significance testing & goodness-of-fit](significance-testing.qmd).

We can now see that it looks like two of our predictor variables aren’t actually significant predictors at all, and that only the `area` and number of endemic species (`endemics`) on each island is a significant predictor of the number of plant species on each Galapagos island. We can check this further using backward stepwise elimination.

::: {.panel-tabset group="language"}
## R

```{r}
step(nb_gal)
```

## Python
:::

This shows that only `endemics` is an appropriate predictor. See the Core statistics notes on [Backwards stepwise elimination](https://cambiotraining.github.io/corestats/materials/cs5_practical_model-comparisons.html#backwards-stepwise-elimination) for more information.

::: {.callout-note}
## Spot the difference

This minimal model is a bit surprising, because in the analysis of deviance table output the `area` variable was also highly significant. As such, we would have expected this variable to be retained during the BSE.

This is partly to do with the *order* in which the different terms are tested. In the analysis of deviance table there is a hint: `Terms added sequentially (first to last)`. Our model is `species ~ area + endemics + elevation + nearest` and when the individual p-values are calculated, it makes comparisons between the different terms in a sequential order. Or, `species ~ area + endemics + ...` will not give the same results as `species ~ endemics + area + ...`. We can check this:

::: {.panel-tabset group="language"}
## R

```{r}
m1 <- glm.nb(species ~ endemics + area + elevation + nearest, data = galapagos)

anova(m1, test = "Chisq")
```

## Python
:::

If we order the predictor variables like this, the `area` variable no longer comes back as statistically significant.

In the [significance testing](significance-testing.qmd) section we go through this in more detail and show you how you can make individual comparisons.
:::

Our new best model only contains `endemics` as a predictor, so we should fit this model and check that it still is an adequate model.

::: {.panel-tabset group="language"}
## R

```{r}
nb_gal_min <- glm.nb(species ~ endemics, data = galapagos)
```

```{r}
summary(nb_gal_min)
```

If we look at the deviance residuals (`r round(nb_gal_min$deviance, 3)`) and the residual degrees of freedom (`r nb_gal_min$df.residual`), we can use the `pchisq()` function to get an overall assessment of whether this model is well-specified.

```{r}
1 - pchisq(nb_gal_min$deviance, nb_gal_min$df.residual)
```

And we get a probability of `r round(1 - pchisq(nb_gal_min$deviance, nb_gal_min$df.residual), 3)` which is better than before, not amazing, but it might be adequate for what we have. Woohoo!

## Python
:::

The model equation for a negative binomial curve is the same as for a Poisson
model and so, lifting the coefficients from the summary output we have the following relationship in our model:

$$E(species) = \exp(2.63 + 0.044 \times endemics)$$

### Model suitability

As we saw above, the model we created was not terribly well-specified. We can visualise it as follows:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| label: fig-nb_endemics_vs_species
#| fig-cap: Negative binomial regression of `species ~ endemics`
ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE)
```

## Python
:::

We can see that at the lower range of `endemics`, the model is predicting values that are a bit too high. In the mid-range it predicts values that are too low and at the higher end the model is, well, rubbish.

Let's compare it directly to our original Poisson regression (setting the limits of the y-axes to match):

```{r}
#| echo: false
p1 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE,
              method.args = list(family = poisson)) +
  ylim(0, 500) +
  labs(title = "Poisson")

p2 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  ylim(0, 500) +
  labs(title = "Negative binomial")
```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-poisson_vs_nb
#| fig-cap: Comparing Poisson and negative binomial regression
p1 + p2 +
  plot_annotation(tag_levels = "A")
```

::: {.callout-note collapse="true"}
## Show code

::: {.panel-tabset group="language"}
## R

```{r}
#| eval: false
p1 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE,
              method.args = list(family = poisson)) +
  ylim(0, 500) +
  labs(title = "Poisson")

p2 <- ggplot(galapagos, aes(endemics, species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  ylim(0, 500) +
  labs(title = "Negative binomial")
```

## Python
:::
:::

So, @fig-poisson_vs_nb shows that, although we've dealt with the overdispersion we had in the original Poisson regression, we still have a pretty poor fitting model. So, what is a sad researcher to do? We'll let you explore this in an exercise.

:::{.callout-important}
Complete [Exercise -@sec-exr_galapagos].
:::

## Exercises

### Revisiting seatbelts {#sex-exr_seatbelts-again}

::: callout-exercise

Last chapter, we discovered that the `seatbelts` dataset from [Exercise @sec-exr_seatbelts] was overdispersed.

Fit a negative binomial regression to the dataset instead.

:::: {.callout-answer collapse="true"}

::::
:::

### Galapagos models {#sec-exr_galapagos}

:::{.callout-exercise}

{{< level 3 >}}

For this exercise we'll be using the data from `data/galapagos.csv`.

The fit for both the Poisson and Negative binomial regressions is not very good, suggesting that the model is not well-specified. How could we improve this?

::: {.callout-answer collapse="true"}

There appears to be a power relationship between `species` and `endemics`. So, one thing we could do is log-transform the response (`species`) and predictor (`endemics`) variables.

::: {.panel-tabset group="language"}
## R

```{r}
ggplot(data = galapagos,
       aes(x = log(endemics),
           y = log(species))) +
  geom_point()
```


## Python
:::

We could add a regression line to this. Additionally, we could just log-transform `endemics` and fit a negative binomial model to this. Let's do all of this and plot them together with the original Poisson and Negative binomial models.

::: {.panel-tabset group="language"}
## R

```{r}
p3 <- ggplot(galapagos, aes(log(endemics), log(species))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE,
              method.args = list(family = poisson)) +
  labs(title = "Linear model of log-log")


p4 <- ggplot(galapagos, aes(log(endemics), species)) +
  geom_point() +
  geom_smooth(method = "glm.nb", se = FALSE, fullrange = TRUE) +
  ylim(0, 500) +
  labs(title = "Negative binomial of log(endemics)")
```

```{r}
p1 + p2 + p3 + p4 +
  plot_annotation(tag_levels = "A")
```

## Python

:::

From this it is clear that the negative binomial model fitted to `species ~ log(endemics)` in panel D produces a much better fit than the original fit in panel B.

Equally, looking at the relationship between `log(species) ~ log(endemics)` in panel C it illustrates that this is pretty well-modelled using a linear line.

There is a slight issue though. If you look carefully then you see that in both panels C and D there is a stray value left of zero. There is also a warning message, saying `Removed 1 row containing non-finite outside the scale range`. If you'd look at the data, you'd notice that there is one island where the number of `endemics` is equal to 0. If we take the `log(0)` we get minus infinity. Which has little biological relevance, of course.

We could adjust for this by adding a "pseudo-count", or adding `1` to all of the counts. If that is acceptable or not is a matter of debate and I'll leave it to you to ponder over this. I would say that, whatever you do, make sure that you are transparent and clear on what you are doing and what the justification for it it.

:::
:::

## Summary

::: {.callout-tip}
#### Key points

- Negative binomial regression relaxes the assumption made by Poisson regressions that the variance is equal to the mean.
- In a negative binomial regression the dispersion parameter $\theta$ is estimated from the data, whereas in a regular Poisson regression it is assumed to be $1$.
:::
