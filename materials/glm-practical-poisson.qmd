---
title: "Analysing count data"
lightbox: true
format: html
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup_files/setup.R")
```

```{python}
#| echo: false
#| message: false
import shutup;shutup.please()
exec(open('setup_files/setup.py').read())
```

::: callout-tip
## Learning outcomes

-   Be able to identify count data
-   Fit a Poisson regression model with count data
-   Assess whether assumptions are met and test significance of the model
:::

## Libraries and functions

:::: {.callout-note collapse="true"}
## Click to expand

::: {.panel-tabset group="language"}
## R

```{r}
#| eval: false
library(performance)
library(tidyverse)
```

## Python

```{python}
#| eval: false
# A maths library
import math
import pandas as pd
from plotnine import *
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import *
```
:::
::::

The worked example in this chapter will use the `islands` dataset.

This is a dataset comprising 35 observations of two variables. For each small island in the dataset, the researcher recorded the number of unique `species`; they want to know if this can be predicted from the `area` (km<sup>2</sup>) of the island.

## Load and visualise the data

First we load the data, then we visualise it.

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
islands <- read_csv("data/islands.csv")

head(islands)
```

## Python

```{python}
islands = pd.read_csv("data/islands.csv")

islands.head()
```
:::

We can plot the data:

::: {.panel-tabset group="language"}
## R

```{r}
#| label: fig-scatter_isl
#| fig-cap: Scatterplot of area and species
ggplot(islands, aes(x = area, y = species)) +
  geom_point()
```

## Python

```{python}
#| results: hide
#| label: fig-scatter_isl_py
#| fig-cap: Scatterplot of area and species
(
  ggplot(islands, aes(x = "area", y = "species")) +
  geom_point()
)
```
:::

Each dot on the scatterplot represents an island in the dataset.

It looks as though `area` definitely has some relationship with the number of `species` that we observe.

Next step is to try to model these data.

## Constructing a model

The `species` variable is the outcome or response (since we're interested in whether it's predicted by `area`).

It qualifies as a count variable. It's bounded at 0 and $\infty$, and can only jump up in integers - in other words, you can't have less than 0 species, or 6.3 species.

This means the best place to start is a Poisson regression. We fit these in a very similar way to a logistic regression, just with a different option specified for the `family` argument.

::: {.panel-tabset group="language"}
## R

```{r}
glm_isl <- glm(species ~ area,
               data = islands, family = "poisson")
```

and we look at the model summary:

```{r}
summary(glm_isl)
```

The output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the coefficients (the two numbers underneath `Estimate` in the table above):

```{r}
coefficients(glm_isl)
```

## Python

```{python}
model = smf.glm(formula = "species ~ area",
                family = sm.families.Poisson(),
                data = islands)

glm_isl = model.fit()
```

Let's look at the model output:

```{python}
print(glm_isl.summary())
```

The output is strikingly similar to the logistic regression models (who’d have guessed, eh?) and the main numbers to extract from the output are the coefficients (the two numbers underneath `coef` in the table above):

```{python}
print(glm_isl.params)
```
:::

### The model equation

Now that we have the beta coefficients, we can place them inside an equation.

The left-hand side of this equation is the expected number of species, $E(species)$.

On the right-hand side, we take the linear equation $\beta_0 + \beta_1 * x_1$ and embed it inside the inverse link function, which in this case is just the exponential function.

```{r}
#| echo: false
b0 <- round(unname(coefficients(glm_isl)[1]), 2)
b1 <- round(unname(coefficients(glm_isl)[2]), 3)
expb0 <- round(exp(b0))
expb1 <- round(exp(b1), 2)
```

It looks like this:

$$ E(species) = \exp(`r b0` + `r b1` \times area) $$

Interpreting this requires a bit of thought (not much, but a bit).

The intercept coefficient, `r b0`, is related to the number of species we would expect on an island of zero area. But in order to turn this number into something meaningful we have to exponentiate it.

Since $\exp(`r b0`) \approx `r expb0`$, we can say that the baseline number of species the model expects on any island is `r expb0`.

The coefficient of `area` is the fun bit. For starters we can see that it is a positive number which does mean that increasing `area` leads to increasing numbers of `species`. Good so far.

But what does the value `r b1` actually mean? Well, if we exponentiate it as well, we get $\exp(`r b1`) \approx `r expb1`$. This means that for every increase in `area` of 1 km<sup>2</sup> (the original units of the area variable), the number of species on the island is multiplied by `r expb1`.

So, an island of area km<sup>2</sup> will have $`r expb1` \times `r expb0` \approx `r round(expb0*expb1)`$ species.

## Plotting the Poisson regression

::: {.panel-tabset group="language"}
## R

```{r}
#| label: fig-poisson_isl
#| fig-cap: Poisson regression on species ~ area
ggplot(islands, aes(area, species)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE, 
              method.args = list(family = "poisson")) +
  xlim(10,50)
```

## Python

First, we produce a set of predictions, which we can then plot.

```{python}
model = pd.DataFrame({'area': list(range(10, 50))})

model["pred"] = glm_isl.predict(model)

model.head()
```

```{python}
#| results: hide
#| label: fig-poisson_isl_py
#| fig-cap: Poisson regression on species ~ area
(
  ggplot(islands,
         aes(x = "area",
             y = "species")) +
     geom_point() +
     geom_line(model, aes(x = "area", y = "pred"), colour = "blue", size = 1)
)
```
:::

## Assumptions & model fit

As a reminder from last chapter, we want to consider the following things to assess whether we've fit the right model:

-   The distribution of the response variable
-   The link function
-   Independence
-   Influential points

(We don't need to worry about collinearity here - there's only one predictor!)

We're also going to talk about **dispersion**, which really comes into play when talking about count data - we'll get to that after we've run through the stuff that's familiar.

::: {.panel-tabset group="language"}
## R

With the generic `check_model` function, we can visually assess a few things quite quickly.

```{r}
check_model(glm_isl)
```

The posterior predictive check looks alright. The blue simulations seem to be following a pretty similar distribution to the actual data in green.

The leverage/Cook's distance plot also isn't identifying any data points that we need to be concerned about. We can follow up on that:

```{r}
check_outliers(glm_isl, threshold = list('cook'=0.5))
```

## Python

We can check for influential points:

```{python}
influence = glm_isl.get_influence()

cooks = influence.cooks_distance[0]
influential_points = np.where(cooks > 0.5)[0]

print("Influential points:", influential_points)
```
:::

**Influential points**: There's no evidence of any points with high Cook's distances, so life is rosy on that front.

**Independence**: From the description of the dataset, it sounds plausible that each island is independent. Of course, if we later found out that some of the islands were clustered together into a bunch of archipelagos, that would definitely be cause for concern.

**Distribution & link function**: Again, from the description of the `species` variable, we can be confident that this really is a count variable. Whether or not Poisson regression with the log link function is correct, however, will depend on what happens when we look at the dispersion parameter.

## Dispersion {#sec-mat_dispersion}

### What is dispersion?

Dispersion, in statistics, is a general term to describe the variability, scatter, or spread of a distribution. Variance is actually a type of dispersion.

In a normal distribution, the mean (average/central tendency) and the variance (dispersion) are independent of each other; we need both numbers, or parameters, to understand the shape of the distribution.

Other distributions, however, require different parameters to describe them in full. For a Poisson distribution - which we’ll learn more about when we talk about count data - we need just one parameter ($\lambda$) to describe the distribution, because the mean and variance are assumed to be the same.

In the context of a model, you can think about the dispersion as the degree to which the data are spread out around the model curve. A dispersion parameter of 1 means the data are spread out exactly as we expect; \<1 is called underdispersion; and \>1 is called overdispersion.

### A "hidden assumption"

When performing Poisson regression, we make an extra assumption: that the dispersion parameter to 1. This means we don't have to waste time and statistical power in estimating the dispersion.

However, if our data are underdispersed or overdispersed, then we might be violating this assumption we've made.

Underdispersion is quite rare. It's far more likely that you'll encounter overdispersion.

In these situations, you may wish to fit a different GLM to the data. Negative binomial regression, for instance, is a common alternative for count data.

### Assessing dispersion

The easiest way to check dispersion in a model is to calculate the ratio of the residual deviance to the residual degrees of freedom.

If we take a look at the model output, we can see the two quantities we care about - residual deviance and residual degrees of freedom:

::: {.panel-tabset group="language"}
## R

```{r}
summary(glm_isl)
```

## Python

```{python}
print(glm_isl.summary())
```
:::

The residual deviance is `r formatC(glm_isl$deviance)`, on `r formatC(glm_isl$df.residual)` residual degrees of freedom. All we need to do is divide one by the other to get our dispersion parameter.

::: {.panel-tabset group="language"}
## R

```{r}
glm_isl$deviance/glm_isl$df.residual
```

## Python

```{python}
print(glm_isl.deviance/glm_isl.df_resid)
```
:::

The dispersion parameter here is `r formatC(glm_isl$deviance/glm_isl$df.residual)`. That's pretty good - not far off 1 at all.

But how can we check whether it is *significantly* different from 1?

::: {.panel-tabset group="language"}
## R

Once again, the `performance` package comes in very helpful here.

The `check_overdispersion` function will give us both the dispersion parameter and a p-value (which is based on a chi-square test - like the goodness-of-fit ones you were running [last chapter](glm-practical-significance-testing.qmd)).

```{r}
check_overdispersion(glm_isl)
```

Here, it confirms our suspicions; the dispersion parameter is both *descriptively* close to 1, and also not *significantly* different from 1.

You'll notice that this function does give a slightly different value for the dispersion parameter, compared to when we calculated it manually above.

This is because it actually uses the sum of squared Pearson residuals instead of the residual deviance (a distinction which really isn't worth worrying about). Broadly, the conclusion should be the same, so the difference doesn't matter very much.

## Python

Well, you've actually already got the knowledge you need to do this, from the [previous chapter](glm-practical-significance-testing.qmd) on significance testing.

Specifically, the chi-squared goodness-of-fit test can be used to check whether the dispersion is within sensible limits.

```{python}
pvalue = chi2.sf(glm_isl.deviance, glm_isl.df_resid)

print(pvalue)
```

If our chi-squared goodness-of-fit test returns a large (insignificant) p-value, as it does here, that tells us that we don't need to worry about the dispersion.

If our chi-squared goodness-of-fit test returned a small, significant p-value, this would tell us our model doesn't fit the data well. And, since dispersion is all about the spread of points around the model, it makes sense that these two things are so closely related!
:::

Great - we can proceed with the Poisson regression, since we don't seem to have overdispersed or underdispersed data.

[Next chapter](glm-practical-negative-binomial.qmd) we'll look at what we would have done next, if we had run into problems with dispersion.

## Assessing significance

By testing the dispersion with a chi-square test, we have already essentially checked the goodness-of-fit of this model - it's good.

This leaves us with two things to check:

1.  Is the overall model better than the null model?
2.  Are any of the individual predictors significant?

and in this case, questions 2 and 3 are effectively asking the same thing because we only have a single predictor variable.

### Comparing against the null

We need to fit the null model, and then we can extract the null deviance and degrees of freedom to compare against our model.

::: {.panel-tabset group="language"}
## R

```{r}
glm_isl_null <- glm(species ~ 1,
                    data = islands, family = "poisson")

anova(glm_isl, glm_isl_null)
```

Since there's only one predictor, we actually could achieve the same effect here by just computing an analysis of deviance table:

```{r}
anova(glm_isl, test = "Chisq")
```

## Python

First, we fit a null model:

```{python}
model = smf.glm(formula = "species ~ 1",
                family = sm.families.Poisson(),
                data = islands)
                
glm_isl_null = model.fit()

glm_isl_null.df_resid
```

And now we can compare the two:

```{python}
# Calculate the likelihood ratio (i.e. the chi-square value)
lrstat = -2*(glm_isl_null.llf - glm_isl.llf)

# Calculate the associated p-value
pvalue = chi2.sf(lrstat, glm_isl_null.df_resid - glm_isl.df_resid)

print(lrstat, pvalue)
```
:::

This gives a reported p-value extremely close to zero, which is rather small.

Therefore, this model is significant over the null, and `species` does appear to be predicted by `area`.

## Exercises

### Seat belts {#sec-exr_seatbelts}

::::::::::: {.callout-exercise #ex-seatbelts}
{{< level 2 >}}

For this exercise we'll be using the data from `data/seatbelts.csv`.

The data tracks the number of drivers killed in road traffic accidents, before and after the seat belt law was introduced. The information on whether the law was in place is encoded in the `law` column as `0` (law not in place) or `1` (law in place).

The `year` variable is our predictor of interest.

In this exercise, you should:

1.  Visualise the data
2.  Create a poisson regression model and extract its equation
3.  Plot the regression model on top of the data
4.  Assess if the model is a decent predictor for the number of fatalities (& check assumptions)

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
seatbelts <- read_csv("data/seatbelts.csv")

head(seatbelts)
```

## Python

```{python}
seatbelts = pd.read_csv("data/seatbelts.csv")

seatbelts.head()
```
:::

::::::::: {.callout-answer collapse="true"}
#### Visualise the data

First we have a look at the data comparing no law versus law:

::: {.panel-tabset group="language"}
## R

We have to convert the `law` column to a factor, otherwise R will see it as numerical.

```{r}
#| label: fig-boxplot_seatbelt
#| fig-cap: Boxplot of driver casualties before and after seatbelt introduction
seatbelts %>% 
  ggplot(aes(as_factor(law), casualties)) +
   geom_boxplot() +
   geom_jitter()
```

The data are recorded by month and year, so we can also display the number of drivers killed by year:

```{r}
#| label: fig-scatter_seatbelt
#| fig-cap: Scatterplot of driver casualties across years
seatbelts %>% 
  ggplot(aes(year, casualties)) +
  geom_point()
```

## Python

We have to convert the `law` column to a factor, otherwise R will see it as numerical.

```{python}
#| results: hide
#| label: fig-boxplot_seatbelt_py
#| fig-cap: Boxplot of driver casualties before and after seatbelt introduction
(
  ggplot(seatbelts,
         aes(x = seatbelts.law.astype(object),
             y = "casualties")) +
     geom_boxplot() +
     geom_jitter()
)
```

The data are recorded by month and year, so we can also display the number of casualties by year:

```{python}
#| results: hide
#| label: fig-scatter_seatbelt_py
#| fig-cap: Scatterplot of driver casualties across years
(ggplot(seatbelts,
         aes(x = "year",
             y = "casualties")) +
     geom_point())
```
:::

The data look a bit weird. There's a bit of a wavy pattern across years. And although it looks like fatalities are lower after the law was implemented, there are many more observations when the law was *not* in place, which is going to make the data harder to interpret.

#### Constructing a model

::: {.panel-tabset group="language"}
## R

```{r}
glm_stb <- glm(casualties ~ year,
               data = seatbelts, family = "poisson")
```

## Python

```{python}
model = smf.glm(formula = "casualties ~ year",
                family = sm.families.Poisson(),
                data = seatbelts)

glm_stb = model.fit()
```
:::

#### Model equation

::: {.panel-tabset group="language"}
## R

```{r}
coefficients(glm_stb)
```

## Python

```{python}
print(glm_stb.params)
```
:::

The coefficients of the Poisson model equation should be placed in the following formula, in order to estimate the expected number of species as a function of island size:

$$ E(casualties) = \exp(`r round(unname(coefficients(glm_stb)[1]), 2)` - `r round(unname(coefficients(glm_stb)[2]), 3)` \times year) $$

#### Visualise model

We can just adapt the code used in the worked example earlier in the chapter:

::: {.panel-tabset group="language"}
## R

```{r}
#| message: false
#| warning: false
#| label: fig-poisson_seatbelt
#| fig-cap: Poisson regression of driver casualties across years
ggplot(seatbelts, aes(year, casualties)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, fullrange = TRUE, 
              method.args = list(family = poisson)) +
  xlim(1970, 1985)
```

## Python

```{python}
model = pd.DataFrame({'year': list(range(1968, 1985))})

model["pred"] = glm_stb.predict(model)

model.head()
```

```{python}
#| results: hide
#| label: fig-poisson_seatbelt_py
#| fig-cap: Poisson regression of driver casualties across years
(
  ggplot(seatbelts,
         aes(x = "year",
             y = "casualties")) +
     geom_point() +
     geom_line(model, aes(x = "year", y = "pred"), colour = "blue", size = 1)
)
```
:::

#### Assessing model quality & significance

Are the assumptions met?

|                                       |                              |
|---------------------------------------|------------------------------|
| Response distribution & link function | Yes, based on the info given |
| Independence                          | Yes, based on the info given |
| Influential points                    | Check with Cook's distance   |
| Dispersion                            | Check dispersion parameter   |

::: {.panel-tabset group="language"}
## R

We can assess outliers visually or by printing a list of high leverage points:

```{r}
check_outliers(glm_stb, threshold = list('cook'=0.5))

check_model(glm_stb, check = 'outliers')
```

It seems we're okay on the outlier front.

For dispersion, again we have some options of how to test it:

```{r}
check_overdispersion(glm_stb)

check_model(glm_stb, check = 'overdispersion')
```

Dispersion is *definitely* a problem here.

For completeness, we can also peek at the posterior predictive check:

```{r}
check_model(glm_stb, check = 'pp_check')
```

It doesn't seem to be doing the best job. Around 0 and in the 180+ range of the x-axis, it's under-predicting the counts; and it might be over-predicting a bit in the middle (120-140 ish).

## Python

We can assess outliers by printing a list of high leverage points:

```{python}
influence = glm_isl.get_influence()

cooks = influence.cooks_distance[0]
influential_points = np.where(cooks > 0.5)[0]

print("Influential points:", influential_points)
```

It seems we're okay on the outlier front.

For dispersion, let's calculate the parameter and p-value:

```{python}
pvalue = chi2.sf(glm_stb.deviance, glm_stb.df_resid)

print(glm_stb.deviance/glm_stb.df_resid, pvalue)
```

Dispersion is *definitely* a problem here.
:::

|                                       |                                |
|---------------------------------------|--------------------------------|
| Response distribution & link function | Yes, based on the info given   |
| Independence                          | Yes, based on the info given   |
| Influential points                    | Yes, no data points identified |
| Dispersion                            | **Clear overdispersion**       |

Given that we have a problem with overdispersion, we probably wouldn't proceed with significance testing here under normal circumstances. Instead, we'd want to fit a different GLM that can handle this overdispersion.

For completeness, however, since these are course materials rather than normal circumstances - let's check: is the model significant over the null?

::: {.panel-tabset group="language"}
## R

```{r}
glm_null <- glm(casualties ~ 1, 
                family = "poisson", 
                data = seatbelts)

anova(glm_stb, glm_null)
```

## Python

```{python}
model = smf.glm(formula = "casualties ~ 1",
                family = sm.families.Poisson(),
                data = seatbelts)

glm_stb_null = model.fit()
```

```{python}
lrstat = -2*(glm_stb_null.llf - glm_stb.llf)
pvalue = chi2.sf(lrstat, glm_stb_null.df_resid - glm_stb.df_resid)

print(lrstat, pvalue)
```
:::

What does the significant p-value here tell us?

Well, given the difference in degrees of freedom (1), the change in residual deviance between our model and the null model is unexpectedly large. So the model is doing *something* over and above what the null is doing.

#### Conclusions

Does this significant comparison against the null allow us to draw any conclusions about whether the seatbelt `law` affects the number of `casualties`?

No. Don't let it tempt you.

The model we've constructed here is not a good fit and doesn't meet all of the assumptions. We need to fit a different type of model that can cope with the overdispersion, which we'll look into next chapter.
:::::::::
:::::::::::

## Summary

::: callout-tip

#### Key points

-   Count data are bounded between 0 and $\infty$, with integer increases
-   This type of data can be modelled with a Poisson regression, using a log link function
-   Poisson regression makes all of the same assumptions as logistic regression, plus an assumption about dispersion
:::
