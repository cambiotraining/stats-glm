---
title: "Training and test data"
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# adjust and load as needed
source(file = "setup.R")

# required data
diabetes <- read_csv("data/diabetes.csv") %>% 
  mutate(test_result = factor(test_result))
```

## Model evaluation

So far we've constructed the logistic model and fed it some new data to make predictions to the possible outcome of a diabetes test, depending on the glucose level of a given patient. This gave us some diabetes test predictions and, importantly, the probabilities of whether the test could come back negative or positive.

The question we'd like to ask ourselves at this point: how reliable is the model?

To explore this, we need to take a step back.

### Split the data

When we created the model, we used *all* of the data. However, a good way of assessing a model fit is to actually split the data into two:

1.  a **training data set** that you use to fit your model
2.  a **test data set** to validate your model and measure model performance

Before we split the data, let's have a closer look at the data set. If we count how many diabetes test results are negative and positive, we see that these counts are not evenly split.

::: panel-tabset
## tidyverse

```{r}
diabetes %>% 
  count(test_result) %>% 
  mutate(prop = n/sum(n))
```

This can have some consequences if we start splitting our data into a training and test set. By splitting the data into two parts - where most of the data goes into your training set - you have data left afterwards that you can use to test how good the predictions of your model are. However, we need to make sure that the *proportion* of negative and positive diabetes test outcomes remains roughly the same.

The `rsample` package has a couple of useful functions that allow us to do just that and we can use the `strata` argument to keep these proportions more or less constant.

```{r}
# ensures random data split is reproducible
set.seed(123)

# split the data, basing the proportions on the diabetes test results
data_split <- initial_split(diabetes, strata = test_result)

# create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

We can check what the `initial_split()` function has done:

```{r}
# proportion of data allocated to the training set
nrow(train_data) / nrow(diabetes)

# proportion of diabetes test results for the training data set
train_data %>% 
  count(test_result) %>% 
  mutate(prop = n/sum(n))

# proportion of diabetes test results for the test data set
test_data %>% 
  count(test_result) %>% 
  mutate(prop = n/sum(n))
```

From the output we can see that around 75% of the data set has been used to create a training data set, with the remaining 25% kept as a test set.

Furthermore, the proportions of negative:positive are kept more or less constant.

### Create a recipe

```{r}
# Create a recipe
dia_rec <- 
  recipe(test_result ~ glucose, data = train_data)

# Look at the recipe summary
summary(dia_rec)
```

### Build a model specification

```{r}
dia_mod <- 
  logistic_reg() %>% 
  set_engine("glm")
```

### Use recipe as we train and test our model

```{r}
dia_wflow <- 
  workflow() %>% 
  add_model(dia_mod) %>% 
  add_recipe(dia_rec)

dia_wflow
```

Although it seems a bit of overkill, we now have a single function that can we can use to prepare the recipe and train the model from the resulting predictors:

```{r}
dia_fit <- 
  dia_wflow %>% 
  fit(data = train_data)
```

This creates an object called `dia_fit`, which contains the final recipe and fitted model objects. We can extract the model and recipe objects with several helper functions:

```{r}
dia_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

### Use trained workflow for predictions

So far, we have done the following:

1.  Built the model (`dia_mod`),
2.  Created a pre-processing recipe (`dia_rec`),
3.  Combined the model and recipe into a workflow (`dia_wflow`)
4.  Trained our workflow using the `fit()` function (`dia_fit`)

The results we generated above do not differ much from the values we obtained with the entire data set. However, these are based on 3/4 of the data (our training data set). Because of this, we still have our test data set available to apply this workflow to data the model has not yet seen.

```{r}
dia_aug <- 
augment(dia_fit, test_data)

dia_aug
```

### Evaluate the model

We can now evaluate the model. One way of doing this is by using the area under the ROC curve as a metric.

An [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (*receiver operating characteristic curve* - the name being a strange relic of WWII where developed for operators of military radar receivers) plots the true-positive rate (TPR) against the false-positive rate (FPR) at varying thresholds.

The true-positive rate is also known as *sensitivity*, whereas the false-positive rate is *1 - specificity*.

We've come across the true-positive rate before, which is also referred to as **power**.

::: callout-tip
A more useful way of remembering this is:

**True positive rate (TPR)**

$$ TPR = \frac{TP}{TP + FN} $$ with TP = true positives, FN = false negatives

**False positive rate**

$$ FPR = \frac{FP}{FP + FN} $$ with FP = false positives
:::

```{r}
dia_aug %>% 
  roc_curve(truth = test_result, .pred_0) %>% 
  autoplot()
```

The area under the ROC curve, which is known as the AUC provides an aggregate measure of performance across all possible classification thresholds.

It ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0. A model whose predictions are 100% correct has an AUC of 1.0.

```{r}
dia_aug %>% 
  roc_auc(truth = test_result, .pred_0)
```

In addition to the ROC curve and AUC we also have a whole range of model parameters associated with the fitted model. We're not going through all of them at this point, but one in particular should be familiar.

We extract these parameters as follows:

```{r}
dia_fit %>% glance()
```

Here we see the Akaike Information Criterion (AIC) as an output. Remember, the value of the AIC in itself is meaningless, but it's useful to compare relative to AICs of other models. We covered how to do this in the Power analysis session of the Core statistics course.

Here we see that the AIC for this model that uses the glucose level as a single predictor for the diabetes test result is 558.
:::

## Exercise - Diabetes predictors

Using the training and test `diabetes` data sets, investigate the relationship between `test_result` and both `glucose` and `diastolic`. Try to answer the following:

-   does adding `diastolic` to the model markedly improve the reliability of the predictions?
-   what do the AICs for the two models tell you?

::: {.callout-caution collapse="true"}
## Answer

::: panel-tabset
## tidyverse

```{r}
# Update the recipe
dia_rec <- 
  recipe(test_result ~ glucose + diastolic,
         data = train_data)

# Look at the recipe summary
summary(dia_rec)
```

Build the model, if needed (we have done this already and it stays the same):

```{r}
dia_mod <- 
  logistic_reg() %>% 
  set_engine("glm")
```

Create a workflow...

```{r}
dia_wflow <- 
  workflow() %>% 
  add_model(dia_mod) %>% 
  add_recipe(dia_rec)
```

... and fit the data:

```{r}
dia_fit <- 
  dia_wflow %>% 
  fit(data = train_data)
```

Extract the model parameters to have a look:

```{r}
dia_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

Apply the fitted model to the test data set:

```{r}
dia_aug <- 
augment(dia_fit, test_data)

dia_aug
```

Plot the ROC curve:

```{r}
dia_aug %>% 
  roc_curve(truth = test_result, .pred_0) %>% 
  autoplot()
```

And get the area under the ROC curve:

```{r}
dia_aug %>% 
  roc_auc(truth = test_result, .pred_0)
```

Another way to assess the model fit is to look at the Akaike Information Criterion (AIC).

```{r}
dia_fit %>% glance()
```

We get an AIC of 555, which is lower than the AIC of 558 that we got with just `glucose` as a predictor variable.
:::

### Conclusions

Adding the `diastolic` variable as a predictor to the model does not seem to have much of an effect on the model reliability, since the AUC is 0.761 with the extra parameter, versus 0.766 without.

The AIC on the other hand suggests that the additive model we've analysed here has a better fit than the original model (AIC of 555 vs 558).

### Food for thought

Perhaps there is an *interaction* between `glucose` and `diastolic`, which would be interesting to investigate.
:::
