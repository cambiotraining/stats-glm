{"title":"Binary response","markdown":{"yaml":{"title":"Binary response","format":"html","editor":"visual"},"headingText":"adjust and load as needed","containsRefs":false,"markdown":"\n\n```{r, echo=FALSE}\nsource(file = \"setup.R\")\n```\n\n::: callout-note\n## Aims & objectives\n\n-   How do we analyse data with a binary outcome?\n-   Can we test if our model is any good?\n-   Be able to perform a logistic regression with a binary outcome\n-   Predict outcomes of new data, based on a defined model\n:::\n\n## Libraries and functions\n\n::: panel-tabset\n## tidyverse\n\n| Library          | Description                                                                            |\n|:-----------------------------------|:-----------------------------------|\n| `tidyverse`      | A collection of R packages designed for data science                                   |\n| `tidymodels`     | A collection of packages for modelling and machine learning using tidyverse principles |\n| `palmerpenguins` | Package that contains data on penguins                                                 |\n:::\n\n## Datasets\n\n::: panel-tabset\n## Diabetes\n\nThe example in this section uses the following data set:\n\n`data/diabetes.csv`\n\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose test and the diastolic blood pressure for each of 767 women. The variables are called `test_result`, `glucose` and `diastolic`.\n:::\n\n## Visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: panel-tabset\n## tidyverse\n\nFirst, we load and inspect the data:\n\n```{r, message=FALSE, warning=FALSE}\ndiabetes <- read_csv(\"data/diabetes.csv\")\n```\n\nLooking at the data, we can see that the `test_result` column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\n\nWe'll deal with that issue later.\n\nWe can plot the data, by outcome:\n\n```{r}\ndiabetes %>% \n  ggplot(aes(x = factor(test_result), y = glucose)) +\n  geom_boxplot()\n```\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\n\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n```{r}\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_point()\n```\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea...\n\n```{r, message=FALSE}\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\nOf course this is rubbish - we can't have test results outside the range of [0, 1].\n\nBut for the sake of exploration, let's look at the assumptions:\n\n```{r}\ndiabetes %>% \n  lm(test_result ~ glucose, data = .) %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\nThey're ~~pretty~~ extremely bad.\n\n* The response is not linear (Residual Plot, binary response plot, common sense).\n* The residuals are not distributed normally (Q-Q Plot)\n* The variance is not homogeneous across the predicted values (Location-Scale Plot)\n* But - there is always a silver lining - we don't have influential data points.\n\n::: {.callout-note}\n## Viewing residuals\n\nAnother way of viewing the residuals (apart from the Q-Q plot) is as a dot-plot. The `ggdist` and `distributional` packages are extremely useful for this kind of stuff.\n\nWhat I'm doing here is:\n\n* define the model\n* create a normal distribution with $\\mu = 0$ and $\\sigma = 0.415$ (I've calculated these from the residuals with `rstatix::get_summary_stats`)\n* plot the residuals\n\n```{r}\ndiabetes %>% \n  lm(test_result ~ glucose, data = .) %>%\n  resid() %>%\n  as_tibble() %>%\n  # rstatix::get_summary_stats()\n  ggplot(aes(x = value)) +\n  stat_dist_halfeye(aes(dist = dist_normal(0,0.415)),\n                    orientation = \"horizontal\") +\n  stat_dotsinterval(aes(x = value),\n                    orientation = \"horizontal\",\n                    fill = \"firebrick\", scale = 1) +\n  labs(title = \"Linear model (diabetes)\", y = \"probability\", x = NULL)\n```\n\nThis again tells us that the residuals are really not normally distributed.\n:::\n:::\n\n\n## Model building\n\nSo we've established that we shouldn't be using a standard linear regression model. From the introduction we've learned that we can use a binomial model instead. We are specifically using a _logistic model_, which is a binomial model with a logistic link function.\n\nThere are different ways to construct a logistic model.\n\n::: panel-tabset\n## tidyverse\n\nIn `tidymodels` we have access to a very useful package: `parsnip`, which provides a common syntax for a whole range of modelling libraries. This means that the syntax will stay the same as you do different kind of model comparisons. So, the learning curve might be a bit steeper to start with, but this will pay dividend in the long-term (just like when you started using R!).\n\nFirst, we need to load `tidymodels` (install it first, if needed):\n\n```{r, eval=FALSE}\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\n```\n\nThe workflow in `parsnip` is a bit different to what we're used to so far. Up until now, we've directly used the relevant model functions to analyse our data, for example using the `lm()` function to create linear models.\n\nUsing `parsnip` we approach things in a more systematic manner. At first this might seem unnecessarily verbose, but there are clear advantages to approaching your analysis in a systematic way. For example, it will be straightforward to implement other types of models using the same workflow, which you'll definitely find useful when moving on to more difficult modelling tasks.\n\nUsing `tidymodels` we specify a model in three steps:\n\n1.  **Specify the type of model based on its mathematical structure** (e.g., linear regression, logistic regression, poisson regression etc).\n\nFor example:\n\n-   `linear_reg()` for linear regression\n-   `logistic_reg()` for logistic regression\n-   `poisson_reg()` for poisson regression (more on this later)\n\n2.  **When required, declare the mode of the model.** The mode reflects the type of prediction outcome. For numeric outcomes, the mode is *regression*; for qualitative outcomes, it is *classification.* If a model can only create one type of model, such as logistic regression, the mode is already set to, in this case, `mode = \"classification\"`.\n\n3.  **Specify the engine for fitting the model.** This usually is the software package or library that should be used.\n\nFor example,\n\n-   `\"lm\"` for linear models\n-   `\"glm\"` for generalised linear models\n-   `\"stan\"` for Bayesian inference\n\nYou can find out which engines can be used with the `show_engines()` function. The command `show_engines(\"logistic_reg\")` will give you the available engines for the `logistic_reg()` function.\n\nSo, we can create the model as follows:\n\n```{r}\ndia_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\n```\n\nNote that we are not actually specifying any of the variables just yet! All we've done is tell R what kind of model we're planning to use. If we want to see how `parsnip` converts this code to the package syntax, we can check this with `translate()`:\n\n```{r}\ndia_mod %>% translate()\n```\n\nThis shows that we have a logistic regression model, where the outcome is going to be a **classification** (in our case, that's a positive (1) or negative test (0) result).\n\nThe model fit template tells us that we'll be using the `glm()` function from the `stats` package (`stats::glm`). This function has several arguments:\n\n1.  a `formula`, which we'll specify later\n2.  `data`, which we'll provide in a bit\n3.  `weights`, if we want to add prior weights to our variable - we don't have to concern ourselves with this - and\n4.  a `family` argument, which is already set to `binomial`\n\n::: callout-important\n## The `family` argument\n\nThe `family` argument gives us a description of the error distribution and link function that will be used in the model. For the `diabetes` data set we are looking at a binary (0 \\| 1) outcome - which we can model using a *binomial* model.\n\nIf we'd want to specify it manually, then we'd use\n\n`set_engine(\"glm\", family = stats::binomial(link = \"logit\"))`\n\nwhich sets the family to binomial, using a logit link function.\n:::\n\nNow we've specified what kind of model we're planning to use, we can fit our data to it, using the `fit()` function:\n\n```{r}\ndia_fit <- dia_mod %>% \n  fit(factor(test_result) ~ glucose,\n      data = diabetes)\n```\n\nWe can look at the output directly, but I prefer to tidy the data up using the `tidy()` function from `broom` package:\n\n```{r}\ndia_fit %>% tidy()\n```\n\nThe `estimate` column gives you the coefficients of the logistic model equation. We could use these to calculate the probability of having a positive diabetes test, for any given glucose level, using the following equation:\n\n$$ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} $$\n\n::: {.callout-note collapse=\"true\"}\n## Expanded explanation\n\nWe started with:\n\n-   Our linear predictor equation $$logit(p) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_p x_p$$\n\n-   and link function $$logit(p) = log(\\frac{p}{1 - p})$$\n\nOur coefficients are as follows:\n\n| coefficient | value                                                                       |\n|:-----------------------------------|:-----------------------------------|\n| $\\beta_0$   | `r dia_fit %>% tidy() %>% filter(term == \"(Intercept)\") %>% pull(estimate)` |\n| $\\beta_1$   | `r dia_fit %>% tidy() %>% filter(term == \"glucose\") %>% pull(estimate)`     |\n\nWhich means that we can write the linear predictor equation as follows:\n\n$$logit(p) = -5.61 + 0.04 \\times glucose$$ We still have to take into account our link function. Combining the two equations gives us:\n\n$$log(\\frac{p}{1 - p}) = -5.61 + 0.04 \\times glucose$$\n\nTo get our $p$ (the probability of a mouse surviving our treatment), we need to exponentiate our equation:\n\n$$\\frac{p}{1 - p} = \\exp{(-5.61 + 0.04 \\times glucose)}$$\n\nleading to...\n\n::: {style=\"background-color: #e0e0e0; padding: 5px\"}\n$$p = \\frac{\\exp{(-5.61 + 0.04 \\times glucose)}}{1 + \\exp{(-5.61 + 0.04 \\times glucose)}}$$\n:::\n:::\n:::\n\nBut of course we're not going to do it that way. We'll let R deal with that in the next section.\n\nThe `std.error` column gives you the error associated with the coefficients and the `statistic` column tells you the Z-statistic value.\n\nThe values in `p.value` merely show whether that particular coefficient is significantly different from zero. This is similar to the p-values obtained in the summary output of a linear model, and as before, for continuous predictors these p-values can be used as a rough guide as to whether that predictor is important (so in this case glucose appears to be significant). However, these p-values aren't great when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a p-value for each level rather than for the predictor as a whole).\n\n## Model visualisation\n\nIt'll be nice to have a better sense of what this model actually looks like. We have an equation for the $p$ value, giving us an outcome probability. But I'd like to _see_ that!\n\nWe could visualise the model in different ways.\n\n:::panel-tabset\n## tidyverse\n\n1. We could create a dummy data set a range of values covering our interval of interest. In this example we could take `glucose` levels between the minimum and maximum value range in increments of 1. Next we ask the model to give us the predicted probability, using the `augment()` function.\n\nWe then plot this.\n\n```{r}\ndummy_data <- tibble(glucose = seq(min(diabetes$glucose),\n                     max(diabetes$glucose),\n                     by = 1))\n\ndia_fit %>% augment(new_data = dummy_data) %>% \n  ggplot(aes(glucose, .pred_1)) +\n  geom_point(colour = \"firebrick\") +\n  geom_point(data = diabetes, aes(glucose, test_result))\n```\n\n2. We can also fit a smoothed line to our data.\n\n```{r}\nggplot(diabetes, aes(glucose, test_result)) + \n  geom_point() +\n  geom_smooth(method = \"glm\",\n              formula = y ~ (x),\n              method.args = list(family = \"binomial\"))\n```\n\nThe nice thing about plotting the model like this with `geom_smooth()` is that it also includes the uncertainty of the model parameters as the standard error of the mean (indicated by the shaded grey area).\n:::\n\n## Model predictions\n\nWhat if we got some new glucose level data and we wanted to predict the probability of those people having diabetes?\n\nWe could use the existing model and feed it some data, using the `augment()` function:\n\n::: panel-tabset\n## tidyverse\n\n```{r}\n# create a dummy data set using some hypothetical glucose measurements\ndiabetes_newdata <- tibble(glucose = c(188, 122, 83, 76, 144))\n\n# predict if the patients have diabetes or not\naugment(dia_fit,\n        new_data = diabetes_newdata)\n```\n\nAlthough you are able to get the predicted outcomes (in `.pred_class`), I would like to stress that this is not the point of running the model. It is important to realise that the model (as with all statistical models) creates a predicted outcome based on certain *probabilities*. It is therefore much more informative to look at how probable these predicted outcomes are. They are encoded in `.pred_0` and `.pred_1`.\n\nFor the first value this means that there is a 14% chance that the diabetes test will return a negative result and around 86% chance that it will return a positive result.\n:::\n\n## Exercise: Penguins\n\nTo practice this a bit more, we'll be using a data set about penguins. The data are from the `palmerpenguins` package, so load that if needed. The data set contains information on penguins at the Palmer Station on Antarctica. Chilly.\n\n::: {.callout-tip}\nThere is also a built-in package called `penguins` in R. Which can cause some issues sometimes. If you're having problems creating some of the graphs, then run the following command to ensure that the correct `penguins` data set is loaded:\n\n```{r}\ndata(\"penguins\", package = \"palmerpenguins\")\n```\n:::\n\nHave a look at the plot below, where we are comparing the bill length (`bill_length_mm`) of three species of penguins (`species`) against flipper length (`flipper_length_mm`).\n\nWe are also colouring the data based on sex (`sex`).\n\n```{r, echo=FALSE}\npenguins %>%\n  filter(!is.na(sex)) %>%\n  ggplot(aes(flipper_length_mm, bill_length_mm,\n             color = sex)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(facets = vars(species))\n```\n\nIt looks like female penguins are smaller with different sized bills and it would be interesting (yes, it would!) to investigate this further.\n\nI would like you to do the following:\n\n1.  load the data into an object called `penguins`\n2.  create a logistic model and fit the data to it, using `sex` as a classifier\n3.  is bill length an important indicator of sex?\n\n::: {.callout-caution collapse=\"true\"}\n## Answer\n\n::: panel-tabset\n## tidyverse\n\nFirst, we load the data:\n\n```{r}\ndata(\"penguins\", package = \"palmerpenguins\")\n```\n\nWe already have a reasonably good idea of what we're looking at, but it can never hurt to understand your data better, so:\n\n```{r}\nhead(penguins)\n```\n\nThis shows that there are a few other columns in our data set, namely `island`, indicating the island where the penguins are residing and `bill_depth_mm` which records the bill depth.\n\nWe also notice that there are some missing values. It would be good to get rid of these, at least for the rows where there sex isn't scored:\n\n```{r}\npenguins <- penguins %>% \n  filter(!is.na(sex))\n```\n\nNext, we specify the type of model. Notice that it can be useful to use a prefix in the naming of these objects to indicate which data set your model belongs to. Here we're using *pgn* to denote penguins.\n\n```{r}\npgn_mod <- logistic_reg() %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"glm\")\n```\n\nRemember, that setting the model *specification* does not yet define the model itself. We do that as follows:\n\n```{r}\npgn_fit <- pgn_mod %>% \n  fit(sex ~ bill_length_mm,\n      data = penguins)\n```\n\nOnce we've fitted the data to the model, we can have a look at the model parameters:\n\n```{r}\npgn_fit %>% tidy()\n```\n\nThe model parameters tell us that both the intercept and the coefficient for `bill_length_mm` are significantly different from zero. So it seems that bill length is an important predictor of the sex of these penguins. Who knew?!\n:::\n:::\n\n## Exercise: Diabetes multiple predictors\nSo far we've only looked at the probability of having a positive diabetes test in the context of the value of the glucose tolerance test.\n\nHowever, there is an extra variable in the data set, `diastolic`. This records the diastolic blood pressure of the patients.\n\nWhat I'd like you to do is the following:\n\n* create a new model fit (similar to `dia_fit`) that takes the `diastolic` pressure into account\n* have a look at the model output using the `glance()`function\n* focussing on the `AIC`, is the new model better supported than the original one?\n* repeat the analysis by adding a `glucose:diastolic` interaction\n\n::: {.callout-caution collapse=\"true\"}\n## Answer\n\n::: panel-tabset\n## tidyverse\n:::\n\nFirst, let's get the AIC value from the _original_ model:\n\n```{r}\ndia_mod %>% \n  fit(factor(test_result) ~ glucose,\n      data = diabetes) %>% \n  glance()\n```\n\nThat gives us an AIC value of `r dia_mod %>% fit(factor(test_result) ~ glucose, data = diabetes) %>% glance() %>% select(AIC) %>% pull()`.\n\nExcellent. Now let's add the `diastolic` predictor variable to the model:\n\n```{r}\ndia_mod %>% \n  fit(factor(test_result) ~ glucose + diastolic,\n      data = diabetes) %>% \n  glance()\n```\n\nThis gives us an AIC value of That gives us an AIC value of `r dia_mod %>% fit(factor(test_result) ~ glucose + diastolic, data = diabetes) %>% glance() %>% select(AIC) %>% pull()`.\n\nFinally, we look also add the `glucose:diastolic` interaction term:\n\n```{r}\ndia_mod %>% \n  fit(factor(test_result) ~ glucose * diastolic,\n      data = diabetes) %>% \n  glance()\n```\n\nThis gives us an AIC of That gives us an AIC value of `r dia_mod %>% fit(factor(test_result) ~ glucose * diastolic, data = diabetes) %>% glance() %>% select(AIC) %>% pull()`.\n\n### Conclusions\nThe original model, with `glucose` as a single predictor, has more or less the same AIC as the model with `glucose * diastolic` (all the main effects and the interaction).\n\nThis suggests that the interaction model only complicates the model, but does not actually make the model any better.\n\nThe multiple predictor model, where we add `diastolic` as a predictor has a slightly lower AIC than the original model. This suggests that the `diastolic` variable makes the model slightly better. It's not a big difference (an AIC difference of around 2), so in this case I personally would go for simplicity and work with the original model.\n\n::: {.callout-note icon=false}\nIf we would have wanted to see if the interaction was statistically significant, we would have done the following:\n\n```{r}\ndia_mod %>% \n  fit(factor(test_result) ~ glucose * diastolic,\n      data = diabetes) %>%\n  extract_fit_engine() %>% \n  tidy()\n```\n\nIt's a bit annoying that you have to extract the model first, using the `extract_fit_engine()` function, before you can pass it on to the `tidy()` (or `summary()`) function. But it is what it is.\n\nWe can see that the interaction is not significant, nor is the `diastolic` main effect. The `glucose` main effect is, however!\n\n:::\n\n:::\n## Key points\n\n::: callout-note\n- We use a logistic regression to model a binary response\n- We can feed new observations into the model and get probabilities for the outcome\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"glm-practical-logistic-binary.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.0.8","editor":"visual","theme":"cosmo","title":"Binary response"},"extensions":{"book":{"multiFile":true}}}}}