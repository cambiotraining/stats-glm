{
  "hash": "0477f63484df996022c27c619b5dec38",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Significance testing\"\nlightbox: true\n---\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n-   Understand the concept of statistical deviance\n-   Use likelihood ratio tests to perform significance testing for:\n-   An entire model (versus the null model)\n-   Individual predictor variables\n:::\n\n## Context\nUp until now we've focussed on creating appropriate models for non-continuous data and making model predictions. In this section we're going to focus on statistical significance testing.\n\nGeneralised linear models are fitted a little differently to standard linear models - namely, using maximum likelihood estimation instead of ordinary least squares for estimating the model coefficients.\n\nAs a result, we can no longer use F-tests for significance, or interpret $R^2$ values in quite the same way. This section will introduce likelihood ratio tests, a method for extracting p-values for GLMs.\n\n## Libraries and functions\n\n:::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(performance)\n\n# You will likely need to install this package first\nlibrary(lmtest)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\nimport numpy as np\n```\n:::\n\n:::\n::::\n\n## Deviance {#sec-mat_deviance}\n\nSeveral of the tests and metrics we'll discuss below are based heavily on deviance. So, what is deviance, and where does it come from?\n\nHere's a few key definitions:\n\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Maximum likelihood estimation** | This is the method by which we fit the GLM (i.e., find the values for the beta coefficients). As the name suggests, we are trying to find the beta coefficients that maximise the likelihood of the dataset/sample.                                                             |\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Likelihood**                    | In this context, \"likelihood\" refers to the joint probability of all of the data points in the sample. In other words, how likely is it that you would sample a set of data points like these, if they were being drawn from an underlying population where your model is true? |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | Each candidate model fitted to a dataset will have its own unique likelihood.                                                                                                                                                                                                   |\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Saturated (perfect) model**     | For each dataset, there is a \"saturated\", or perfect, model. This model has the same number of parameters in it as there are data points, meaning the data are fitted exactly - as if connecting the dots between them.                                                         |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | This model has the largest possible likelihood of any model fitted to the dataset.                                                                                                                                                                                              |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | Of course, we don't actually use the saturated model for drawing real conclusions, but we can use it as a baseline for comparison.                                                                                                                                              |\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Deviance\\                       | Each candidate model is compared back to the saturated model to figure out its deviance.                                                                                                                                                                                        |\n| (residual deviance)**             |                                                                                                                                                                                                                                                                                 |\n|                                   | Deviance is defined as the difference between the log-likelihood of your fitted model and the log-likelihood of the saturated model (multiplied by 2).                                                                                                                          |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | Because deviance is all about capturing the discrepancy between fitted and actual values, it's performing a similar function to the residual sum of squares (RSS) in a standard linear model. In fact, the RSS is really just a specific type of deviance.                      |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | Sometimes, the deviance of a candidate (non-null) model is referred to more fully as \"residual deviance\".                                                                                                                                                                       |\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| **Null deviance**                 | One of the models that we can compare against the saturated model is the null model (a model with no predictors). This gives us the deviance value for the null model.                                                                                                          |\n|                                   |                                                                                                                                                                                                                                                                                 |\n|                                   | This is the greatest deviance of any possible model that could be fitted to the data, because it explains zero variance in the response variable.                                                                                                                               |\n+-----------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n![Different models and their deviances](images/LRT_schematic.png){width=\"70%\" #fig-modeldeviances}\n\n## Revisiting the diabetes dataset\n\nAs a worked example, we'll use a logistic regression fitted to the `diabetes` dataset that we saw in a previous section.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read_csv(\"data/diabetes.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n\ndiabetes_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   glucose  diastolic  test_result\n0      148         72            1\n1       85         66            0\n2      183         64            1\n3       89         66            0\n4      137         40            1\n```\n\n\n:::\n:::\n\n:::\n\nAs a reminder, this dataset contains three variables:\n\n-   `test_result`, binary results of a diabetes test result (1 for positive, 0 for negative)\n-   `glucose`, the results of a glucose tolerance test\n-   `diastolic` blood pressure\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_dia <- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n```\n:::\n\n:::\n\n## What are the p-values in the summary?\n\nYou might have noticed that when you use `summary` to see the model output, it comes with some p-values automatically.\n\nWhat are they? Can you use/interpret them?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_dia)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_dia_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      724\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -374.00\nDate:                Thu, 24 Jul 2025   Deviance:                       748.01\nTime:                        08:50:50   Pearson chi2:                     720.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.2282\nCovariance Type:            nonrobust                                         \n=====================================================================================\n                        coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nIntercept            -8.5711      2.703     -3.171      0.002     -13.869      -3.273\nglucose               0.0547      0.021      2.614      0.009       0.014       0.096\ndiastolic             0.0424      0.036      1.165      0.244      -0.029       0.114\nglucose:diastolic    -0.0002      0.000     -0.796      0.426      -0.001       0.000\n=====================================================================================\n```\n\n\n:::\n:::\n\n:::\n\nEach individual parameter, or coefficient, has its own z-value and associated p-value. In each case, a hypothesis test has been performed - these are formally called **Wald tests**.\n\nThe null hypothesis for these Wald tests is that the value of the coefficient = 0. The idea is that if a coefficient isn't significantly different from 0, then that parameter isn't useful and could be dropped from the model.\n\nThese tests are the equivalent of the t-tests that are calculated as part of the `summary` output for standard linear models.\n\n::: callout-warning\n### Why can't we just use these p-values?\n\nIn some cases, you can. However, there are a few cases where they don't give you all the info you need.\n\nFirstly: they don't tell you about the significance of the model as a whole (versus the null model).\n\nSecondly: for categorical predictors, you will get a separate Wald p-value for each non-reference group (compared back to the reference group). This is *not* the same as a p-value for the categorical predictor as a whole. The Wald p-values can also be heavily affected by which group was chosen as the reference.\n:::\n\nIt's typically preferable to use a likelihood ratio test instead.\n\n## Likelihood ratio tests (LRTs)\n\nWhen we were assessing the significance of standard linear models, we were able to use the F-statistic to determine:\n\n-   the significance of the model versus a null model, and\n-   the significance of individual predictors.\n\nWe can't use these F-tests for GLMs, but we can use LRTs in a very similar way, to calculate p-values for both the model as a whole, and for individual variables. This work because, in general, if you have a test statistic and you know the distribution of that test statistic, then you can use this to calculate a p-value.\n\nThese tests are all built on the idea of deviance, or the likelihood ratio, as discussed above on this page. We can compare any two models fitted to the same dataset by looking at the difference in their deviances, also known as the difference in their log-likelihoods, or more simply as a likelihood ratio.\n\nHelpfully, this likelihood ratio approximately follows a chi-square distribution, which we can capitalise on to calculate a p-value. All we need is the number of degrees of freedom, which is equal to the difference in the number of parameters of the two models you're comparing.\n\n::: callout-warning\nImportantly, we are only able to use this sort of test when one of the two models that we are comparing is a \"simpler\" version of the other, i.e., one model has a subset of the parameters of the other model.\n\nSo while we could perform an LRT just fine between these two models: `Y ~ A + B + C` and `Y ~ A + B + C + D`, or between any model and the null (`Y ~ 1`), we would not be able to use this test to compare `Y ~ A + B + C` and `Y ~ A + B + D`.\n:::\n\n### Testing the model versus the null\n\nSince LRTs involve making a comparison between two models, we must first decide which models we're comparing, and check that one model is a \"subset\" of the other.\n\nLet's use an example from a previous section of the course, where we fitted a logistic regression to the `diabetes` dataset.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without predictors, respectively).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_dia <- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nglm_null <- glm(test_result ~ 1, \n                family = binomial, \n                data = diabetes)\n```\n:::\n\n\nThen, we use the `lrtest` function from the `lmtest` package to perform the test itself; we include both the models that we want to compare, listing them one after another.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrtest(glm_dia, glm_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ 1\n  #Df LogLik Df  Chisq Pr(>Chisq)    \n1   4 -374.0                         \n2   1 -468.3 -3 188.59  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nWe can see from the output that our chi-square statistic is significant, with a really small p-value. This tells us that, for the difference in degrees of freedom (here, that's 3), the change in deviance is actually quite big. (In this case, you can use `summary(glm_dia)` to see those deviances - 936 versus 748!)\n\nIn other words, our model is better than the null.\n\n## Python\n\nThe first step is to create the two models that we want to compare: our original model, and the null model (with and without our predictor, respectively).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\nmodel = smf.glm(formula = \"test_result ~ 1\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_null_py = model.fit()\n```\n:::\n\n\nUnlike in R, there isn't a nice neat function for extracting the $\\chi^2$ value, so we have to do a little bit of work by hand.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# calculate the likelihood ratio (i.e. the chi-square value)\nlrstat = -2*(glm_null_py.llf - glm_dia_py.llf)\n\n# calculate the associated p-value\npvalue = chi2.sf(lrstat, glm_null_py.df_resid - glm_dia_py.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n188.5931483744455 1.228870036004379e-40\n```\n\n\n:::\n:::\n\n\nThis gives us the likelihood ratio, based on the log-likelihoods that we've extracted directly from the models, which approximates a chi-square distribution.\n\nWe've also calculated the associated p-value, by providing the difference in degrees of freedom between the two models (in this case, that's simply 1, but for more complicated models it's easier to extract the degrees of freedom directly from the model as we've done here).\n\nHere, we have a large chi-square statistic and a small p-value. This tells us that, for the difference in degrees of freedom (here, that's 1), the change in deviance is actually quite big. (In this case, you can use `glm_dia_py.summary()` to see those deviances - 936 versus 748!)\n\nIn other words, our model is better than the null.\n:::\n\n### Testing individual predictors\n\nAs well as testing the overall model versus the null, we might want to test particular predictors to determine whether they are individually significant.\n\nThe way to achieve this is essentially to perform a series of \"targeted\" likelihood ratio tests. In each LRT, we'll compare two models that are almost identical - one with, and one without, our variable of interest in each case.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe first step is to construct a new model that doesn't contain our predictor of interest. Let's test the `glucose:diastolic` interaction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_dia_add <- glm(test_result ~ glucose + diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n```\n:::\n\n\nNow, we can use the `lrtest` function (or the `anova` function) to compare the models with and without the interaction:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrtest(glm_dia, glm_dia_add)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio test\n\nModel 1: test_result ~ glucose * diastolic\nModel 2: test_result ~ glucose + diastolic\n  #Df  LogLik Df  Chisq Pr(>Chisq)\n1   4 -374.00                     \n2   3 -374.32 -1 0.6288     0.4278\n```\n\n\n:::\n:::\n\n\nThis tells us that our interaction `glucose:diastolic` isn't significant - our more complex model doesn't have a meaningful reduction in deviance.\n\nThis might, however, seem like a slightly clunky way to test each individual predictor. Luckily, we can also use our trusty `anova` function with an extra argument to tell us about individual predictors.\n\nBy specifying that we want to use a chi-squared test, we are able to construct an analysis of deviance table (as opposed to an analysis of variance table) that will perform the likelihood ratio tests for us for each predictor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(glm_dia, test=\"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: test_result\n\nTerms added sequentially (first to last)\n\n                  Df Deviance Resid. Df Resid. Dev Pr(>Chi)    \nNULL                                727     936.60             \nglucose            1  184.401       726     752.20  < 2e-16 ***\ndiastolic          1    3.564       725     748.64  0.05905 .  \nglucose:diastolic  1    0.629       724     748.01  0.42779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nYou'll spot that the p-values we get from the analysis of deviance table match the p-values you could calculate yourself using `lrtest`; this is just more efficient when you have a complex model!\n\n## Python\n\nThe first step is to construct a new model that doesn't contain our predictor of interest. Let's test the `glucose:diastolic` interaction.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"test_result ~ glucose + diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_add_py = model.fit()\n```\n:::\n\n\nWe'll then use the same code we used above, to compare the models with and without the interaction:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlrstat = -2*(glm_dia_add_py.llf - glm_dia_py.llf)\n\npvalue = chi2.sf(lrstat, glm_dia_py.df_model - glm_dia_add_py.df_model)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.6288201373599804 0.42778842576800746\n```\n\n\n:::\n:::\n\n\nThis tells us that our interaction `glucose:diastolic` isn't significant - our more complex model doesn't have a meaningful reduction in deviance.\n:::\n\n## Exercises\n\n### Predicting failure {#sec-exr_failure}\n\n::::: {.callout-exercise #ex-predict_failure}\n#### Predicting failure\n\n{{< level 2 >}}\n\nIn the [previous chapter](glm-08-proportional-response.qmd), we used the `challenger.csv` dataset as a worked example.\n\nOur research question was: should NASA have cancelled the Challenger launch, based on the data they had about o-rings in previous launches?\n\nLet's try to come up with an interpretation from these data, with the help of a likelihood ratio test.\n\nYou should:\n\n1.  Refit the model (if it's not still in your environment from last chapter)\n2.  Fit a null model (no predictors)\n3.  Perform a likelihood ratio test to compare the model to the null model\n4.  Decide what you think the answer is to the research question\n\n:::: {.callout-answer collapse=\"true\"}\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nLet's read in our data and mutate it to contain the relevant variables (this is borrowed from the last chapter):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchallenger <- read_csv(\"data/challenger.csv\") |>\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\nWe create our logistic model like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_chl <- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger)\n```\n:::\n\n\nWe can get the model parameters as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_chl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\nAnd let's visualise the model, just to make sure it looks sensible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(challenger, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](glm-09-significance-testing_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n#### Comparing against the null\n\nThe next question we can ask is: is our model any better than the null model?\n\nFirst, we define the null model; then we use `lrtest` to compare them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_chl_null <- glm(cbind(damage, intact) ~ 1,\n                family = binomial,\n                data = challenger)\n\nlrtest(glm_chl, glm_chl_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio test\n\nModel 1: cbind(damage, intact) ~ temp\nModel 2: cbind(damage, intact) ~ 1\n  #Df  LogLik Df  Chisq Pr(>Chisq)    \n1   2 -14.837                         \n2   1 -25.830 -1 21.985  2.747e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nWith a very small p-value (and a large chi-square statistic), it would seem that the model is indeed significantly better than the null.\n\nSince there's only one predictor variable, this is pretty much equivalent to saying that `temp` does predict the proportion of o-rings that are damaged.\n\n## Python\n\nWe need to make sure we've read in our data, and mutated it to contain the relevant variables (this is borrowed from the last chapter):\n\n\n::: {.cell}\n\n:::\n\n\nOur logistic regression is fitted like so:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_py = model.fit()\n```\n:::\n\n\nWe can get the model parameters as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_chl_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -14.837\nDate:                  Thu, 24 Jul 2025   Deviance:                       16.912\nTime:                          08:50:50   Pearson chi2:                     28.1\nNo. Iterations:                       7   Pseudo R-squ. (CS):             0.6155\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.6630      3.296      3.538      0.000       5.202      18.124\ntemp          -0.2162      0.053     -4.066      0.000      -0.320      -0.112\n==============================================================================\n```\n\n\n:::\n:::\n\n\nGenerate new model data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_py.predict(model)\n\nmodel.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   temp      pred\n0    25  0.998087\n1    26  0.997626\n2    27  0.997055\n3    28  0.996347\n4    29  0.995469\n```\n\n\n:::\n:::\n\n\nAnd let's visualise the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot() +\n   geom_point(challenger_py, aes(x = \"temp\", y = \"prop_damaged\")) +\n   geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1))\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-09-significance-testing_files/figure-html/unnamed-chunk-29-1.png){width=614}\n:::\n:::\n\n\n#### Comparing against the null\n\nThe next question we can ask is: is our model any better than the null model?\n\nFirst we need to define the null model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.glm(formula = \"damage + intact ~ 1\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_null_py = model.fit()\n\nprint(glm_chl_null_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       22\nModel Family:                  Binomial   Df Model:                            0\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -25.830\nDate:                  Thu, 24 Jul 2025   Deviance:                       38.898\nTime:                          08:50:51   Pearson chi2:                     58.5\nNo. Iterations:                       5   Pseudo R-squ. (CS):              0.000\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.4463      0.314     -7.783      0.000      -3.062      -1.830\n==============================================================================\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlrstat = -2*(glm_chl_null_py.llf - glm_chl_py.llf)\n\npvalue = chi2.sf(lrstat, glm_chl_null_py.df_resid - glm_chl_py.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n21.985381067707717 2.747351270353041e-06\n```\n\n\n:::\n:::\n\n\nWith a very small p-value (and a large chi-square statistic), it would seem that the model is indeed significantly better than the null.\n\nSince there's only one predictor variable, this is pretty much equivalent to saying that `temp` does predict the proportion of o-rings that are damaged.\n:::\n\nSo, could NASA have predicted what happened?\n\nProbably, yes. They certainly should have listened to the engineers who were raising concerns based on these data. But that's the subject of many documentaries, if you're interested in the topic, so we won't get into it here...\n::::\n:::::\n\n### Predicting failure (with a tweak) {#sec-exr_failure-tweak}\n\n::::: {.callout-exercise #ex-failure_tweak}\n#### Predicting failure (with a tweak)\n{{< level 3 >}}\n\nIn the `challenger` dataset, the data point at 53 degrees Fahrenheit is quite influential.\n\nWould the conclusions from the previous exercise still hold without that point?\n\nYou should:\n\n1.  Fit a model without this data point\n2.  Visualise the new model\n3.  Determine whether there is a significant link between launch temperature and o-ring failure in the new model\n\n:::: {.callout-answer collapse=\"true\"}\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, we need to remove the influential data point:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchallenger_new <- challenger |> filter(temp != 53)\n```\n:::\n\n\nNow we can create a new generalised linear model, based on these data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_chl_new <- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger_new)\n```\n:::\n\n\nWe can get the model parameters as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_chl_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger_new)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  5.68223    4.43138   1.282   0.1997  \ntemp        -0.12817    0.06697  -1.914   0.0556 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.375  on 21  degrees of freedom\nResidual deviance: 12.633  on 20  degrees of freedom\nAIC: 27.572\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nAnd let's visualise the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(challenger_new, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85) +\n  # add a vertical line at 53 F temperature\n  geom_vline(xintercept = 53, linetype = \"dashed\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](glm-09-significance-testing_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\n\n#### Comparing against the null\n\nSo is our new model any better than the null?\n\nWe need to construct a new null model - we can't use the one from the previous exercise, because it was fitted to a different dataset that had an extra observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_chl_null_new <- glm(cbind(damage, intact) ~ 1,\n                family = binomial,\n                data = challenger_new)\n\nlrtest(glm_chl_new, glm_chl_null_new)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio test\n\nModel 1: cbind(damage, intact) ~ temp\nModel 2: cbind(damage, intact) ~ 1\n  #Df  LogLik Df  Chisq Pr(>Chisq)  \n1   2 -11.786                       \n2   1 -13.657 -1 3.7421    0.05306 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nThe model is not significantly better than the null in this case, with a p-value here of just over 0.05.\n\n## Python\n\nFirst, we need to remove the influential data point:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchallenger_new_py = challenger_py.query(\"temp != 53\")\n```\n:::\n\n\nWe can create a new generalised linear model, based on these data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_py = model.fit()\n```\n:::\n\n\nWe can get the model parameters as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_chl_new_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       20\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -11.786\nDate:                  Thu, 24 Jul 2025   Deviance:                       12.633\nTime:                          08:50:51   Pearson chi2:                     16.6\nNo. Iterations:                       6   Pseudo R-squ. (CS):             0.1564\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.6822      4.431      1.282      0.200      -3.003      14.368\ntemp          -0.1282      0.067     -1.914      0.056      -0.259       0.003\n==============================================================================\n```\n\n\n:::\n:::\n\n\nGenerate new model data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_new_py.predict(model)\n\nmodel.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   temp      pred\n0    25  0.922585\n1    26  0.912920\n2    27  0.902177\n3    28  0.890269\n4    29  0.877107\n```\n\n\n:::\n:::\n\n\nAnd let's visualise the model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot() +\n   geom_point(challenger_new_py, aes(x = \"temp\", y = \"prop_damaged\")) +\n   geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1) +\n   # add a vertical line at 53 F temperature\n   geom_vline(xintercept = 53, linetype = \"dashed\"))\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-09-significance-testing_files/figure-html/unnamed-chunk-41-1.png){width=614}\n:::\n:::\n\n\nThe prediction proportion of damaged o-rings is markedly less than what was observed.\n\n#### Comparing against the null\n\nSo is our new model any better than the null?\n\nWe need to construct a new null model - we can't use the one from the previous exercise, because it was fitted to a different dataset that had an extra observation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.glm(formula = \"damage + intact ~ 1\",\n                family = sm.families.Binomial(),\n                data = challenger_new_py)\n# and get the fitted parameters of the model\nglm_chl_new_null_py = model.fit()\n\nprint(glm_chl_new_null_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   22\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            0\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -13.657\nDate:                  Thu, 24 Jul 2025   Deviance:                       16.375\nTime:                          08:50:51   Pearson chi2:                     16.8\nNo. Iterations:                       6   Pseudo R-squ. (CS):         -2.220e-16\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.0445      0.418     -7.286      0.000      -3.864      -2.226\n==============================================================================\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlrstat = -2*(glm_chl_new_null_py.llf - glm_chl_new_py.llf)\n\npvalue = chi2.sf(lrstat, glm_chl_new_null_py.df_resid - glm_chl_new_py.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n3.7421161935342973 0.053057208274015694\n```\n\n\n:::\n:::\n\n\nThe model is not significantly better than the null in this case, with a p-value here of just over 0.05.\n:::\n\nSo, could NASA have predicted what happened? This model is not significantly different from the null, i.e., temperature is not a significant predictor.\n\nHowever, note that it’s only marginally non-significant, and this is with a data point removed.\n\nIt is possible that if more data points were available that followed a similar trend, the story might be different). Even if we did use our non-significant model to make a prediction, it doesn’t give us a value anywhere near 5 failures for a temperature of 53 degrees Fahrenheit. So overall, based on the model we’ve fitted with these data, there was no clear indication that a temperature just a few degrees cooler than previous missions could have been so disastrous for the Challenger.\n::::\n:::::\n\n### Revisiting rats and levers {#sec-exr_levers-again}\n\n:::::::: {.callout-exercise #ex-levers_again}\n#### Revisiting rats and levers\n{{< level 2 >}}\n\nLast chapter, we fitted a model to the `levers.csv` dataset in [Exercise @sec-exr_levers].\n\nNow, let's test significance.\n\nIn this exercise, you should:\n\n1.  Fit a model with the predictors `~ stress_type * sex + rat_age`\n2.  Assess whether this model is significant over the null model\n3.  Assess whether any of the 4 individual predictors (including the interaction) are significant\n\n::::::: {.callout-answer collapse=\"true\"}\n#### Fit the model\n\nBefore we fit the model, we need to:\n\n-   Read the data in\n-   Mutate to create an `incorrect_presses` variable\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevers <- read_csv(\"data/levers.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 62 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): stress_type, sex\ndbl (5): rat_id, rat_age, trials, correct_presses, prop_correct\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nlevers <- levers |>\n  mutate(incorrect_presses = trials - correct_presses)\n\nglm_lev <- glm(cbind(correct_presses, incorrect_presses) ~ stress_type * sex + rat_age,\n               family = binomial,\n               data = levers)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevers = pd.read_csv(\"data/levers.csv\")\n\nlevers['incorrect_presses'] = levers['trials'] - levers['correct_presses']\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev = model.fit()\n```\n:::\n\n:::\n\n#### Compare to the null\n\nWe also need to fit a null model to these data before we can do any comparisons.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_lev_null <- glm(cbind(correct_presses, incorrect_presses) ~ 1,\n                    family = binomial,\n                    data = levers)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ 1\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev_null = model.fit()\n```\n:::\n\n:::\n\nNow, we run our likelihood ratio test comparing the two models.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(glm_lev, glm_lev_null)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: cbind(correct_presses, incorrect_presses) ~ stress_type * sex + \n    rat_age\nModel 2: cbind(correct_presses, incorrect_presses) ~ 1\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)    \n1        57     60.331                         \n2        61    120.646 -4  -60.315 2.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlrstat = -2*(glm_lev_null.llf - glm_lev.llf)\n\npvalue = chi2.sf(lrstat, glm_lev_null.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n60.31519356442419 2.490497342946823e-12\n```\n\n\n:::\n:::\n\n:::\n\nThis is pretty significant, suggesting that our model is quite a bit better than the null.\n\n#### Test individual predictors\n\nNow, let's test individual predictors.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThis is extremely easy to do in R. We produce an analysis of deviance table with `anova`, using the chi-square statistic for our likelihood ratios.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(glm_lev, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(correct_presses, incorrect_presses)\n\nTerms added sequentially (first to last)\n\n                Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                               61    120.646              \nstress_type      1    8.681        60    111.965 0.0032150 ** \nsex              1    5.389        59    106.576 0.0202584 *  \nrat_age          1   12.216        58     94.359 0.0004738 ***\nstress_type:sex  1   34.028        57     60.331 5.432e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n## Python\n\nLet's build two new candidate models: one with `rat_age` removed, and one with the `stress:sex` interaction removed.\n\n(If the interaction isn't significant, then we'll push on and look at the main effects of `stress` and `sex`, if needed.)\n\n#### The age effect\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex\",\n                family = sm.families.Binomial(),\n                data = levers)\nglm_lev_dropage = model.fit()\n\nlrstat = -2*(glm_lev_dropage.llf - glm_lev.llf)\npvalue = chi2.sf(lrstat, glm_lev_dropage.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n17.8705812026094 2.364481283464226e-05\n```\n\n\n:::\n:::\n\n\n#### The stress:sex interaction\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type + sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\nglm_lev_dropint = model.fit()\n\nlrstat = -2*(glm_lev_dropint.llf - glm_lev.llf)\npvalue = chi2.sf(lrstat, glm_lev_dropint.df_resid - glm_lev.df_resid)\n\nprint(lrstat, pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n34.02832899366183 5.431548556703029e-09\n```\n\n\n:::\n:::\n\n\nSince the interaction is significant, we don't really need the specifics of the main effects (it becomes hard to interpret!)\n:::\n:::::::\n::::::::\n\n## Summary\n\nLikelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and [goodness-of-fit](glm-10-goodness-of-fit.qmd). To determine the quality of a model and draw conclusions from it, it's important to assess both of these things.\n\n::: callout-tip\n#### Key points\n\n-   Deviance is the difference between predicted and actual values, and is calculated by comparing a model's log-likelihood to that of the perfect \"saturated\" model\n-   Using deviance, likelihood ratio tests can be used in lieu of F-tests for generalised linear models\n-   This is distinct from (and often better than) using the Wald p-values that are reported automatically in the model summary\n:::\n",
    "supporting": [
      "glm-09-significance-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}