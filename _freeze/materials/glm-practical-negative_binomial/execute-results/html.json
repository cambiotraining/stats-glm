{
  "hash": "fbe42cb192ce7ba612840e88149e1db3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Negative binomial\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n## Learning outcomes\n\n**Questions**\n\n- \n\n**Objectives**\n\n- \n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fits a negative binomial model\nMASS::glm.nb()\n```\n:::\n\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A maths library\nimport math\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n```\n:::\n\n\n### Functions\n\n:::\n:::\n\nNegative Binomial Models are also used for count data, but these models don’t\nrequire that the variance of the data exactly matches the mean of the data, and so they can be used in situations where your data exhibit overdispersion.\n\nThe example in this section use the following data set:\n\n`data/galapagos.csv`\n\nThere are 30 Galapagos islands and 4 variables in the data. The relationship between the number of plant species (`species`) and several geographic variables is of interest.\n\n* `endemics` – the number of endemic species\n* `area` – the area of the island km<sup>2</sup>\n* `elevation` – the highest elevation of the island (m).\n* `nearest` – the distance from the nearest island (km)\n\n## Load and visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalapagos <- read_csv(\"data/galapagos.csv\")\n```\n:::\n\n\nLet's have a glimpse at the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalapagos\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 × 5\n   species endemics  area elevation nearest\n     <dbl>    <dbl> <dbl>     <dbl>   <dbl>\n 1      58       23 25.1        346     0.6\n 2      31       21  1.24       109     0.6\n 3       3        3  0.21       114     2.8\n 4      25        9  0.1         46     1.9\n 5       2        1  0.05        77     1.9\n 6      18       11  0.34       119     8  \n 7      24        0  0.08        93     6  \n 8      10        7  2.33       168    34.1\n 9       8        4  0.03        71     0.4\n10       2        2  0.18       112     2.6\n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngalapagos_py = pd.read_csv(\"data/galapagos.csv\")\n```\n:::\n\n\nLet's have a glimpse at the data:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ngalapagos_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   species  endemics   area  elevation  nearest\n0       58        23  25.09        346      0.6\n1       31        21   1.24        109      0.6\n2        3         3   0.21        114      2.8\n3       25         9   0.10         46      1.9\n4        2         1   0.05         77      1.9\n```\n\n\n:::\n:::\n\n\n:::\n\nWe can plot the data:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngalapagos %>% \n  pairs(lower.panel = NULL)\n```\n\n::: {.cell-output-display}\n![](glm-practical-negative_binomial_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nIt looks as though `endemics` and `elevation` might be related to `species`, but\n`area` and `nearest` are harder to work out.\n\nGiven that the response variable, `species`, is a count variable we try to construct a Poisson regression. We decide that there is no biological reason to look for interaction between the various predictor variables and so we don’t construct a model with any interactions. Remember that this may or may not be a sensible thing to do in general.\n\n## Constructing a model\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_gal <- glm(species ~ area + endemics + elevation + nearest,\n               data = galapagos, family = \"poisson\")\n```\n:::\n\n\nand we look at the model summary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_gal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = species ~ area + endemics + elevation + nearest, \n    family = \"poisson\", data = galapagos)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.794e+00  5.332e-02  52.399  < 2e-16 ***\narea        -1.266e-04  2.559e-05  -4.947 7.53e-07 ***\nendemics     3.325e-02  9.164e-04  36.283  < 2e-16 ***\nelevation    3.799e-04  9.432e-05   4.028 5.63e-05 ***\nnearest      9.049e-03  1.327e-03   6.819 9.18e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  315.88  on 25  degrees of freedom\nAIC: 486.71\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n## Python\n\n:::\n\nNow, this time, before we start looking at interpreting the model coefficients were going to jump straight into assessing whether the model is well-specified (spoiler alert: we do this because I already know that it isn't...).\n\nThe residual deviance is 315.88, but we only have 25 degrees of freedom in the model. This means that we are dealing with a lot of overdispersion. The estimated amount of overdispersion, $\\theta$, is given by dividing these two numbers together, so we\nhave $\\theta$ = 315.88 / 25 = 12.64. This is definitely nowhere near close to 1.\n\nWe can formally check this with our trusty \"Is the model well-posed\" probability\nvalue:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(315.88, 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Python\n:::\n\nThis gives a big fat 0, so no, there are definitely things wrong with our model and we can’t really trust anything that’s being spat out at this stage.\nSo, with that conclusion, we won’t bother looking at the analysis of deviance table or asking whether the model is better than the null model. Instead we need to find a better fitting model...\n\nFor count response data options are limited, but the main alternative to a Poisson model is something called a negative binomial model. \n\n## Negative binomial model\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nTo specify a negative binomial model, we use the `MASS` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_gal <- glm.nb(species ~ area + endemics + elevation + nearest,\n               data = galapagos)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nb_gal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm.nb(formula = species ~ area + endemics + elevation + nearest, \n    data = galapagos, init.theta = 2.987830946, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.4870922  0.1864426  13.340  < 2e-16 ***\narea        -0.0002911  0.0001941  -1.500    0.134    \nendemics     0.0457287  0.0065890   6.940 3.92e-12 ***\nelevation    0.0003053  0.0005137   0.594    0.552    \nnearest      0.0040316  0.0079105   0.510    0.610    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.9878) family taken to be 1)\n\n    Null deviance: 151.446  on 29  degrees of freedom\nResidual deviance:  33.395  on 25  degrees of freedom\nAIC: 286.06\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.988 \n          Std. Err.:  0.898 \n\n 2 x log-likelihood:  -274.060 \n```\n\n\n:::\n:::\n\n\nThis output is very similar to the other GLM outputs that we’ve seen but with some additional information at the bottom regarding the dispersion parameter that the negative binomial model has used, which it calls Theta (2.988). This is just for information rather than anything to worry about.\n\nAs before, the main numbers to extract from the output are the numbers underneath `Estimate` in the `Coefficients` table:\n\n```\nCoefficients:\n              Estimate\n(Intercept)  2.4870922\narea        -0.0002911\nendemics     0.0457287\nelevation    0.0003053\nnearest      0.0040316\n```\n\n## Python\n:::\n\nThese are the coefficients of the Negative Binomial model equation and need to be placed in the following formula in order to estimate the expected number of species as a function of the other variables.:\n\n$E(species) = \\exp(2.49 + 0.046 \\times endemics - 0.0003 \\times elevation + 0.004 \\times nearest)$\n\nThe Negative Binomial model has the same form for its line of best fit as the Poisson model, but the underlying probability distribution is different.\n\n### Assessing significance\n\nWe can ask the same three questions we asked before.\n\n1. Is the model well-specified?\n2. Is the overall model better than the null model?\n3. Are any of the individual predictors significant?\n\nTo assess whether the model is any good we’ll use the residual deviance and the\nresidual degrees of freedom.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(33.395, 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1214851\n```\n\n\n:::\n:::\n\nInstead of manually typing in the values, which is of course prone to errors, we can also extract them directly from the model object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(nb_gal$deviance, nb_gal$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1214756\n```\n\n\n:::\n:::\n\n\n## Python\n:::\n\nThis gives a probability of 0.121. Whilst this isn’t brilliant, it is still much better than the model we had before, and now that we’ve taken account of the overdispersion issue, the fact that this probability is a bit small is probably down to the fact that the predictor variables we have in the model might not be enough to fully explain the number of `species` on each of the Galapagos islands. \nHowever, since we don’t have any other data to play with there’s nothing we can do about that right now.\n\n\nSecondly, to assess whether the overall model, with all four terms, is better than the null model we’ll look at the difference in deviances and the difference in degrees of freedom:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(151.446 - 33.395, 29 - 25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nOr extracting them directly from the model object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(nb_gal$null.deviance - nb_gal$deviance,\n           nb_gal$df.null - nb_gal$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Python\n:::\n\nThis gives a reported p-value of 0, which is pretty damn small. So, yes, this model is better than nothing at all and at least some of our predictors are related to the response variable in a meaningful fashion.\n\nFinally, we’ll construct an analysis of deviance table to look at the individual\npredictors:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(nb_gal, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in anova.negbin(nb_gal, test = \"Chisq\"): tests made without\nre-estimating 'theta'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: algorithm did not converge\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: Negative Binomial(2.9878), link: log\n\nResponse: species\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                         29    151.446              \narea       1   33.873        28    117.574 5.884e-09 ***\nendemics   1   83.453        27     34.121 < 2.2e-16 ***\nelevation  1    0.468        26     33.653    0.4939    \nnearest    1    0.257        25     33.395    0.6121    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nYou might get a warning message about theta not being recalculated but this isn’t\nsomething to worry about.\n\n## Python\n:::\n\nWe can now see that it looks like three of our predictor variables aren’t actually significant predictors at all, and that only the number of Endemic species on each island is a significant predictor of the number of plant species on each Galapagos island. We can check this further using backward stepwise elimination.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(nb_gal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=284.06\nspecies ~ area + endemics + elevation + nearest\n\n            Df Deviance    AIC\n- nearest    1   33.653 282.32\n- elevation  1   33.831 282.50\n<none>           33.395 284.06\n- area       1   35.543 284.21\n- endemics   1   70.764 319.43\n\nStep:  AIC=282.32\nspecies ~ area + endemics + elevation\n\n            Df Deviance    AIC\n- elevation  1   33.814 280.78\n<none>           33.351 282.32\n- area       1   35.795 282.76\n- endemics   1   70.183 317.15\n\nStep:  AIC=280.77\nspecies ~ area + endemics\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: algorithm did not converge\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Df Deviance    AIC\n- area      1   35.179 280.72\n<none>          33.232 280.77\n- endemics  1  114.180 359.72\n\nStep:  AIC=280.66\nspecies ~ endemics\n\n           Df Deviance    AIC\n<none>          33.204 280.66\n- endemics  1  137.826 383.28\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm.nb(formula = species ~ endemics, data = galapagos, init.theta = 2.695721184, \n    link = log)\n\nCoefficients:\n(Intercept)     endemics  \n    2.63124      0.04378  \n\nDegrees of Freedom: 29 Total (i.e. Null);  28 Residual\nNull Deviance:\t    137.8 \nResidual Deviance: 33.2 \tAIC: 282.7\n```\n\n\n:::\n:::\n\n\n## Python\n:::\n\nThis will confirm that only `endemics` is an appropriate predictor. (You may have to look back at the linear model handout to remember how the step function works.)\n\nOur new best model only contains `endemics` as a predictor, so we should fit this\nmodel and check that it still is an adequate model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_gal_min <- glm.nb(species ~ endemics, data = galapagos)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(nb_gal_min)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm.nb(formula = species ~ endemics, data = galapagos, init.theta = 2.695721175, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 2.631244   0.164242   16.02   <2e-16 ***\nendemics    0.043785   0.004233   10.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.6957) family taken to be 1)\n\n    Null deviance: 137.826  on 29  degrees of freedom\nResidual deviance:  33.204  on 28  degrees of freedom\nAIC: 282.66\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.696 \n          Std. Err.:  0.789 \n\n 2 x log-likelihood:  -276.662 \n```\n\n\n:::\n:::\n\n\nIf we look at the deviance residuals (33.204) and the residual degrees of freedom (28), we can use the `pchisq()` function to get an overall assessment of whether this model is well-specified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pchisq(nb_gal_min$deviance, nb_gal_min$df.residual)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2283267\n```\n\n\n:::\n:::\n\n\nAnd we get a probability of 0.228 which is better than before, not amazing, but\nadequate for what we have. Woohoo!\n\n## Python\n:::\n\nThe model equation for a negative binomial curve is the same as for a Poisson\nmodel and so, lifting the coefficients from the summary output we have the following relationship in our model:\n\n$E(species) = \\exp(2.63 + 0.044 \\times endemics$\n\n## Exercises\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n-   Negative binomial regression relaxes the assumption made by Poisson regressions that the variance is equal to the mean.\n:::\n",
    "supporting": [
      "glm-practical-negative_binomial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}