{
  "hash": "535e106d186ff7e5f52b654ff8da0855",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Goodness-of-fit\"\nlightbox: true\n---\n\nGoodness-of-fit is all about how well a model fits the data, and typically involves summarising the discrepancy between the actual data points, and the fitted/predicted values that the model produces.\n\nThough closely linked, it's important to realise that goodness-of-fit and significance don't come hand-in-hand automatically: we might find a model that is significantly better than the null, but is still overall pretty rubbish at matching the data. So, to understand the quality of our model better, we should ideally perform both types of test.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n-   Understand the difference between significance and goodness-of-fit\n-   Know how to use at least two methods to evaluate the quality of a model fit\n-   Know how to use AIC values to perform model comparison\n:::\n\n## Libraries and functions\n\n:::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"lmtest\")\nlibrary(lmtest)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import *\n```\n:::\n\n:::\n::::\n\n## Data and model\n\nWe'll continue using the data and model for the `diabetes` dataset, which were defined as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read_csv(\"data/diabetes.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 728 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): glucose, diastolic, test_result\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_dia <- glm(test_result ~ glucose * diastolic,\n                  family = \"binomial\",\n                  data = diabetes)\n\nglm_null <- glm(test_result ~ 1, \n                family = binomial, \n                data = diabetes)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"test_result ~ glucose * diastolic\", \n                family = sm.families.Binomial(), \n                data = diabetes_py)\n                \nglm_dia_py = model.fit()\n\nmodel = smf.glm(formula = \"test_result ~ 1\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n\nglm_null_py = model.fit()\n```\n:::\n\n:::\n\n## Chi-square tests\n\nLast chapter, we talked about [deviance](@sec-mat_deviance) and chi-square tests, to assess significance.\n\nWe can use these in a very similar way to assess the goodness-of-fit of a model.\n\nWhen we compared our model against the null (last chapter), we tested the null hypothesis that the candidate model and the null model had the same deviance.\n\nNow, however, we will test the null hypothesis that the fitted model and the saturated (perfect) model have the same deviance, i.e., that they both fit the data equally well.\n\n![Using deviance to assess goodness-of-fit](images/chisq_gof.png){width=\"70%\"}\n\nIn most hypothesis tests, we want to reject the null hypothesis, but in this case, we'd like it to be **true**.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nRunning a goodness-of-fit chi-square test in R can be done using the `pchisq` function. We need to include two arguments: 1) the residual deviance, and 2) the residual degrees of freedom. Both of these can be found in the `summary` output, but you can use the `$` syntax to call these properties directly like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npchisq(glm_dia$deviance, glm_dia$df.residual, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2605931\n```\n\n\n:::\n:::\n\n\n## Python\n\nThe syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npvalue = chi2.sf(glm_dia_py.deviance, glm_dia_py.df_resid)\n\nprint(pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.26059314630406843\n```\n\n\n:::\n:::\n\n:::\n\nYou can think about this p-value, roughly, as \"the probability that this model is good\". We're not below our significance threshold, which means that we're not rejecting our null hypothesis (which is a good thing) - but it's also not a huge probability. This suggests that there's probably other variables we could measure and include in a future experiment, to give a better overall model.\n\n## AIC values\n\nYou might remember AIC values from standard linear modelling. AIC values are useful, because they tell us about overall model quality, factoring in both goodness-of-fit and model complexity.\n\nOne of the best things about the Akaike information criterion (AIC) is that it isn't specific to linear models - it works for models fitted with maximum likelihood estimation.\n\nIn fact, if you look at the formula for AIC, you'll see why:\n\n$$\nAIC = 2k - 2ln(\\hat{L})\n$$\n\nwhere $k$ represents the number of parameters in the model, and $\\hat{L}$ is the maximised likelihood function. In other words, the two parts of the equation represent the complexity of the model, versus the log-likelihood.\n\nThis means that AIC can be used for model comparison for GLMs in precisely the same way as it's used for linear models: lower AIC indicates a better-quality model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe AIC value is given as standard, near the bottom of the `summary` output (just below the deviance values). You can also print it directly using the `$` syntax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_dia)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = test_result ~ glucose * diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)   \n(Intercept)       -8.5710565  2.7032318  -3.171  0.00152 **\nglucose            0.0547050  0.0209256   2.614  0.00894 **\ndiastolic          0.0423651  0.0363681   1.165  0.24406   \nglucose:diastolic -0.0002221  0.0002790  -0.796  0.42590   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.60  on 727  degrees of freedom\nResidual deviance: 748.01  on 724  degrees of freedom\nAIC: 756.01\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\nglm_dia$aic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 756.0069\n```\n\n\n:::\n:::\n\n\nIn even better news for R users, the `step` function works for GLMs just as it does for linear models, so long as you include the `test = LRT` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(glm_dia, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=756.01\ntest_result ~ glucose * diastolic\n\n                    Df Deviance    AIC     LRT Pr(>Chi)\n- glucose:diastolic  1   748.64 754.64 0.62882   0.4278\n<none>                   748.01 756.01                 \n\nStep:  AIC=754.64\ntest_result ~ glucose + diastolic\n\n            Df Deviance    AIC     LRT Pr(>Chi)    \n<none>           748.64 754.64                     \n- diastolic  1   752.20 756.20   3.564  0.05905 .  \n- glucose    1   915.52 919.52 166.884  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = test_result ~ glucose + diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n(Intercept)      glucose    diastolic  \n   -6.49941      0.03836      0.01407  \n\nDegrees of Freedom: 727 Total (i.e. Null);  725 Residual\nNull Deviance:\t    936.6 \nResidual Deviance: 748.6 \tAIC: 754.6\n```\n\n\n:::\n:::\n\n\n## Python\n\nThe AIC value isn't printed as standard with the model summary, but you can access it easily like so:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_dia_py.aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n756.0068586069744\n```\n\n\n:::\n:::\n\n:::\n\n## Pseudo $R^2$\n\n#### Refresher on $R^2$\n\nIn linear modelling, we could extract and interpret $R^2$ values that summarised our model. $R^2$ in linear modelling represents a few different things:\n\n-   The proportion of variance in the response variable, that's explained by the model (i.e., jointly by the predictors)\n-   The improvement of the model over the null model\n-   The square of the Pearson's correlation coefficient\n\nThe first one on that list is the interpretation we usually use it for, in linear modelling.\n\n#### What is a \"pseudo $R^2$\"?\n\nIt's not possible to calculate $R^2$ for a GLM like you can for a linear model.\n\nHowever, because people are fond of using $R^2$, statisticians have developed alternatives that can be used instead.\n\nThere is no single value that can replace $R^2$ and have all the same interpretations, so several different metrics have been proposed. Depending how they're calculated, they all have different interpretations.\n\nThere are many. Some of the most popular are McFadden's, Nagelkerke's, Cox & Snell's, and Tjur's. [This post](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-pseudo-r-squareds/) does a nice job of discussing some of them and providing some comparisons.\n\n#### Should you use pseudo $R^2$?\n\n[**We recommend not to use pseudo** $R^2$**.**]{style=\"color:orange;\"}\n\n(Unless you are very statistically-minded and prepared to wade through a lot of mathematical explanation...)\n\nThis is for a few reasons:\n\n-   It's too easy to fall into the bad habit of treating it like regular $R^2$, and making bad interpretations\n-   Even if you've done a good job, your readers might make their own bad interpretations\n-   Figuring out which version to use, and what they all mean, is a minefield\n-   It doesn't really tell you anything that a chi-square test and/or AIC can't tell you\n\nThe main reason we've mentioned it here is because you are likely to come across pseudo $R^2$ when reading research papers that use GLMs - we want you to know what they are!\n\n## Exercises\n\n### Revisiting aphids {#sec-exr_aphids-revisited}\n\n::: {.callout-exercise #ex-aphids_revisited}\n{{< level 2 >}}\n\nBack in [Exercise @sec-exr_aphids], we fitted a logistic model to the `aphids` dataset - the code is included below in case you need to run it again.\n\nNow, let's assess the goodness-of-fit of that model.\n\nYou should:\n\n1.  Compute a chi-square goodness-of-fit test for the full model (`~ buds + cultivar`)\n2.  Calculate the AIC value for the full model\n3.  Use backwards stepwise elimination to determine whether dropping the `buds` and/or `cultivar` predictors improves the goodness-of-fit\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\naphids <- read_csv(\"data/aphids.csv\")\n\nglm_aphids <- glm(aphids_present ~ buds + cultivar,\n                  family = binomial,\n                  data = aphids)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\naphids = pd.read_csv(\"data/aphids.csv\")\n\nmodel = smf.glm(formula = \"aphids_present ~ buds + cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids = model.fit()\n```\n:::\n\n:::\n\n:::: {.callout-answer collapse=\"true\"}\n#### Chi-square goodness-of-fit\n\nThis is a simple one-function task:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npchisq(glm_aphids$deviance, glm_aphids$df.residual, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1666178\n```\n\n\n:::\n:::\n\n\n## Python\n\nThe syntax is very similar to the LRT we ran above, but now instead of including information about both our candidate model and the null, we instead just need 1) the residual deviance, and 2) the residual degrees of freedom:\n\n\n::: {.cell}\n\n```{.python .cell-code}\npvalue = chi2.sf(glm_aphids.deviance, glm_aphids.df_resid)\n\nprint(pvalue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.16661777677427902\n```\n\n\n:::\n:::\n\n:::\n\n#### Extract AIC for full model\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe can access the AIC either in the model summary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_aphids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = aphids_present ~ buds + cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)   \n(Intercept)     -2.0687     0.7483  -2.765  0.00570 **\nbuds             0.2067     0.1262   1.638  0.10149   \ncultivarmozart   1.9621     0.6439   3.047  0.00231 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.666  on 51  degrees of freedom\nAIC: 66.666\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nor directly using the `$` syntax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_aphids$aic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 66.66602\n```\n\n\n:::\n:::\n\n\n## Python\n\nThe AIC value is an \"attribute\" of the model object, which we can access like so:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_aphids.aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n66.66602340347231\n```\n\n\n:::\n:::\n\n:::\n\n#### Backwards stepwise elimination\n\nLast but not least, let's see if dropping either or both of the predictors improves the model quality. (Spoiler: it probably won't!)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe use the convenient `step` function for this - don't forget the `test = LRT` argument, though.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep(glm_dia, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=756.01\ntest_result ~ glucose * diastolic\n\n                    Df Deviance    AIC     LRT Pr(>Chi)\n- glucose:diastolic  1   748.64 754.64 0.62882   0.4278\n<none>                   748.01 756.01                 \n\nStep:  AIC=754.64\ntest_result ~ glucose + diastolic\n\n            Df Deviance    AIC     LRT Pr(>Chi)    \n<none>           748.64 754.64                     \n- diastolic  1   752.20 756.20   3.564  0.05905 .  \n- glucose    1   915.52 919.52 166.884  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:  glm(formula = test_result ~ glucose + diastolic, family = \"binomial\", \n    data = diabetes)\n\nCoefficients:\n(Intercept)      glucose    diastolic  \n   -6.49941      0.03836      0.01407  \n\nDegrees of Freedom: 727 Total (i.e. Null);  725 Residual\nNull Deviance:\t    936.6 \nResidual Deviance: 748.6 \tAIC: 754.6\n```\n\n\n:::\n:::\n\n\nSince neither of our reduced models improve on the AIC versus our original model, we don't drop either predictor, and the process stops there.\n\n## Python\n\nWe need to build two new candidate models. In each case, we drop just one variable.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Dropping buds\nmodel = smf.glm(formula = \"aphids_present ~ cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids_dropbuds = model.fit()\n\n# Dropping cultivar\nmodel = smf.glm(formula = \"aphids_present ~ buds\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \nglm_aphids_dropcultivar = model.fit()\n```\n:::\n\n\nNow, we can look at the three AIC values next to each other, to determine which of these is the best option.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_aphids.aic,\n      glm_aphids_dropbuds.aic, \n      glm_aphids_dropcultivar.aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n66.66602340347231 67.51106908352895 75.19195097185758\n```\n\n\n:::\n:::\n\n\nSince neither of our reduced models improve on the AIC versus our original model, we don't drop either predictor, and the process stops there.\n:::\n\n::::\n:::\n\n## Summary\n\nLikelihood and deviance are very important in generalised linear models - not just for fitting the model via maximum likelihood estimation, but for assessing significance and goodness-of-fit. To determine the quality of a model and draw conclusions from it, it's important to assess both of these things.\n\n::: callout-tip\n#### Key points\n\n-   A chi-square goodness-of-fit test can also be performed using likelihood/deviance\n-   The Akaike information criterion is also based on likelihood, and can be used to compare the quality of GLMs fitted to the same dataset\n-   Other metrics that may be of use are Wald test p-values and pseudo $R^2$ values (if interpreted thoughtfully)\n:::\n",
    "supporting": [
      "glm-practical-goodness-of-fit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}