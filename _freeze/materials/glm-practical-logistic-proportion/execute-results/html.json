{
  "hash": "473ec928f8eefe54cd065d00ad207da7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Proportional response\"\nlightbox: true\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n## Learning outcomes\n\n-   Be able to analyse proportional response variables\n-   Be able to create a logistic model to test proportional response variables\n-   Be able to plot the data and fitted curve\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(ggResidpanel)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport math\nimport pandas as pd\nfrom plotnine import *\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import *\n```\n:::\n\n:::\n:::\n\n## The Challenger dataset\n\nThe example in this section uses the following data set:\n\n`data/challenger.csv`\n\nThese data, obtained from the [faraway package](https://www.rdocumentation.org/packages/faraway/versions/1.0.7) in R, contain information related to the explosion of the USA Space Shuttle Challenger on 28 January, 1986. An investigation after the disaster traced back to certain joints on one of the two solid booster rockets, each containing O-rings that ensured no exhaust gases could escape from the booster.\n\nThe night before the launch was unusually cold, with temperatures below freezing. The final report suggested that the cold snap during the night made the o-rings stiff, and unable to adjust to changes in pressure. As a result, exhaust gases leaked away from the solid booster rockets, causing one of them to break loose and rupture the main fuel tank, leading to the final explosion.\n\nThe question we're trying to answer in this session is: based on the data from the previous flights, would it have been possible to predict the failure of most o-rings on the Challenger flight?\n\n## Load and visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchallenger <- read_csv(\"data/challenger.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 23 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (2): temp, damage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchallenger_py = pd.read_csv(\"data/challenger.csv\")\n```\n:::\n\n:::\n\nThe data set contains several columns:\n\n1.  `temp`, the launch temperature in degrees Fahrenheit\n2.  `damage`, the number of o-rings that showed erosion\n\nBefore we have a further look at the data, let's calculate the proportion of damaged o-rings (`prop_damaged`) and the total number of o-rings (`total`) and update our data set.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchallenger <-\nchallenger %>%\n  mutate(total = 6,                     # total number of o-rings\n         intact = 6 - damage,           # number of undamaged o-rings\n         prop_damaged = damage / total) # proportion damaged o-rings\n\nchallenger\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 23 × 5\n    temp damage total intact prop_damaged\n   <dbl>  <dbl> <dbl>  <dbl>        <dbl>\n 1    53      5     6      1        0.833\n 2    57      1     6      5        0.167\n 3    58      1     6      5        0.167\n 4    63      1     6      5        0.167\n 5    66      0     6      6        0    \n 6    67      0     6      6        0    \n 7    67      0     6      6        0    \n 8    67      0     6      6        0    \n 9    68      0     6      6        0    \n10    69      0     6      6        0    \n# ℹ 13 more rows\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchallenger_py['total'] = 6\nchallenger_py['intact'] = challenger_py['total'] - challenger_py['damage']\nchallenger_py['prop_damaged'] = challenger_py['damage'] / challenger_py['total']\n```\n:::\n\n\n:::\n\nPlotting the proportion of damaged o-rings against the launch temperature shows the following picture:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(challenger, aes(x = temp, y = prop_damaged)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point())\n     \np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-11-1.png){width=614}\n:::\n:::\n\n\n:::\n\nThe point on the left is the data point corresponding to the coldest flight experienced before the disaster, where five damaged o-rings were found. Fortunately, this did not result in a disaster.\n\nHere we'll explore if we could have reasonably predicted the failure of both o-rings on the Challenger flight, where the launch temperature was 31 degrees Fahrenheit.\n\n## Creating a suitable model\n\nWe only have 23 data points in total. So we're building a model on not that much data - we should keep this in mind when we draw our conclusions!\n\nWe are using a logistic regression for a proportion response in this case, since we're interested in the proportion of o-rings that are damaged.\n\nWe can define this as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_chl <- glm(cbind(damage, intact) ~ temp,\n               family = binomial,\n               data = challenger)\n```\n:::\n\n\nDefining the relationship for proportion responses is a bit annoying, where you have to give the `glm` model a two-column matrix to specify the response variable.\n\nHere, the first column corresponds to the number of damaged o-rings, whereas the second column refers to the number of intact o-rings. We use the `cbind()` function to bind these two together into a matrix.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a generalised linear model\nmodel = smf.glm(formula = \"damage + intact ~ temp\",\n                family = sm.families.Binomial(),\n                data = challenger_py)\n# and get the fitted parameters of the model\nglm_chl_py = model.fit()\n```\n:::\n\n\n:::\n\nIf we're using the original observations, we need to supply both the number of damaged o-rings *and* the number of intact ones.\n\n::: {.callout-warning}\n### Why don't we use `prop_damaged` as our response variable?\n\nYou might have noticed that when we write our model formula, we are specifically writing `cbind(damage, intact)` (R) or `damage + intact` (Python), instead of the `prop_damaged` variable we used for plotting.\n\nThis is very deliberate - it's **absolutely essential** we do this, in order to fit the correct model.\n\nWhen modelling our data, we need to provide the function with both the number of successes (damaged o-rings) and the number of failures (intact o-rings) - and therefore, implicitly, the total number of o-rings - in order to properly model our proportional variable.\n\nIf we provide it with just the `prop_damaged`, then R/Python will incorrectly think our response variable is **fractional**.\n\nA fractional variable is not constructed from a series of trials with successes/failures. It doesn't have trials - there's just a single value somewhere between 0 and 1 (or a percentage). An example of a fractional response variable might be what percentage of rocket fuel was used on a launch. We would not model this response variable with a logistic regression.\n\n![Fractional vs proportional variables](images/fractional-vs-proportional.png){width=40%}\n:::\n\n## Model output\n\nThat's the easy part done! The trickier part is interpreting the output. First of all, we'll get some summary information.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nNext, we can have a closer look at the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_chl)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(damage, intact) ~ temp, family = binomial, \n    data = challenger)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 11.66299    3.29626   3.538 0.000403 ***\ntemp        -0.21623    0.05318  -4.066 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38.898  on 22  degrees of freedom\nResidual deviance: 16.912  on 21  degrees of freedom\nAIC: 33.675\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\nWe can see that the p-values of the `intercept` and `temp` are significant. We can also use the intercept and `temp` coefficients to construct the logistic equation, which we can use to sketch the logistic curve.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_chl_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Generalized Linear Model Regression Results                   \n================================================================================\nDep. Variable:     ['damage', 'intact']   No. Observations:                   23\nModel:                              GLM   Df Residuals:                       21\nModel Family:                  Binomial   Df Model:                            1\nLink Function:                    Logit   Scale:                          1.0000\nMethod:                            IRLS   Log-Likelihood:                -14.837\nDate:                  Mon, 21 Jul 2025   Deviance:                       16.912\nTime:                          17:00:02   Pearson chi2:                     28.1\nNo. Iterations:                       7   Pseudo R-squ. (CS):             0.6155\nCovariance Type:              nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.6630      3.296      3.538      0.000       5.202      18.124\ntemp          -0.2162      0.053     -4.066      0.000      -0.320      -0.112\n==============================================================================\n```\n\n\n:::\n:::\n\n:::\n\n$$E(prop \\ failed\\ orings) = \\frac{\\exp{(11.66 -  0.22 \\times temp)}}{1 + \\exp{(11.66 -  0.22 \\times temp)}}$$\n\nLet's see how well our model would have performed if we would have fed it the data from the ill-fated Challenger launch.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(challenger, aes(temp, prop_damaged)) +\n  geom_point() +\n  geom_smooth(method = \"glm\", se = FALSE, fullrange = TRUE, \n              method.args = list(family = binomial)) +\n  xlim(25,85)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in eval(family$initialize): non-integer #successes in a binomial glm!\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nWe can get the predicted values for the model as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nchallenger_py['predicted_values'] = glm_chl_py.predict()\n\nchallenger_py.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   temp  damage  total  intact  prop_damaged  predicted_values\n0    53       5      6       1      0.833333          0.550479\n1    57       1      6       5      0.166667          0.340217\n2    58       1      6       5      0.166667          0.293476\n3    63       1      6       5      0.166667          0.123496\n4    66       0      6       6      0.000000          0.068598\n```\n\n\n:::\n:::\n\n\nThis would only give us the predicted values for the data we already have. Instead we want to extrapolate to what would have been predicted for a wider range of temperatures. Here, we use a range of $[25, 85]$ degrees Fahrenheit.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = pd.DataFrame({'temp': list(range(25, 86))})\n\nmodel[\"pred\"] = glm_chl_py.predict(model)\n\nmodel.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   temp      pred\n0    25  0.998087\n1    26  0.997626\n2    27  0.997055\n3    28  0.996347\n4    29  0.995469\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(challenger_py,\n         aes(x = \"temp\",\n             y = \"prop_damaged\")) +\n     geom_point() +\n     geom_line(model, aes(x = \"temp\", y = \"pred\"), colour = \"blue\", size = 1))\n     \np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-19-1.png){width=614}\n:::\n:::\n\n\n\n::: {.callout-note collapse=true}\n## Generating predicted values\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nAnother way of doing this it to generate a table with data for a range of temperatures, from 25 to 85 degrees Fahrenheit, in steps of 1. We can then use these data to generate the logistic curve, based on the fitted model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a table with sequential numbers ranging from 25 to 85\nmodel <- tibble(temp = seq(25, 85, by = 1)) %>% \n  # add a new column containing the predicted values\n  mutate(.pred = predict(glm_chl, newdata = ., type = \"response\"))\n\nggplot(model, aes(temp, .pred)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-20-3.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot the curve and the original data\nggplot(model, aes(temp, .pred)) +\n  geom_line(colour = \"blue\") +\n  geom_point(data = challenger, aes(temp, prop_damaged)) +\n  # add a vertical line at the disaster launch temperature\n  geom_vline(xintercept = 31, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nIt seems that there was a high probability of both o-rings failing at that launch temperature. One thing that the graph shows is that there is a lot of uncertainty involved in this model. We can tell, because the fit of the line is very poor at the lower temperature range. There is just very little data to work on, with the data point at 53 F having a large influence on the fit.\n\n## Python\n\nWe already did this above, since this is the most straightforward way of plotting the model in Python.\n\n:::\n:::\n:::\n\n## Exercises\n\n### Rats and levers {#sec-exr_levers}\n\n::: {.callout-exercise #ex-levers}\n{{< level 2 >}}\n\nThis exercises uses the dataset `levers.csv`.\n\nThese data are from a simple animal behavioural experiment. Prior to testing, rats were trained to press levers for food rewards. On each trial, there were two levers, and the \"correct\" lever is determined by an audio cue.\n\nThe rats could vary in three different ways, which may impact their task performance at testing:\n\n-   The `sex` of the rat\n-   The `age` of the rat (in weeks)\n-   Whether the rat experienced `stress` during the training phase (being exposed to the smell of a predator)\n\nThe researcher thinks that the effect of stress may differ between male and female rats.\n\nIn this exercise:\n\n1. Visualise these data\n2. Fit a model that captures the researcher's hypotheses\n\n:::: {.callout-answer collapse=\"true\"}\n#### Load and visualise\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevers <- read_csv(\"data/levers.csv\")\n\nhead(levers)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  rat_id stress_type sex    rat_age trials correct_presses prop_correct\n   <dbl> <chr>       <chr>    <dbl>  <dbl>           <dbl>        <dbl>\n1      1 control     female      13     20               7         0.35\n2      2 control     female      15     20              11         0.55\n3      3 control     female      11     20               5         0.25\n4      4 control     female      15     20               5         0.25\n5      5 stressed    female      13     20               8         0.4 \n6      6 stressed    male        14     20               8         0.4 \n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlevers = pd.read_csv(\"data/levers.csv\")\n\nlevers.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   rat_id stress_type     sex  rat_age  trials  correct_presses  prop_correct\n0       1     control  female       13      20                7          0.35\n1       2     control  female       15      20               11          0.55\n2       3     control  female       11      20                5          0.25\n3       4     control  female       15      20                5          0.25\n4       5    stressed  female       13      20                8          0.40\n```\n\n\n:::\n:::\n\n:::\n\nWe can see that this dataset contains quite a few columns. Key ones to pay attention to:\n\n-   `trials`; the total number of trials per rat - we're going to need this when fitting our model\n-   `correct_presses`; the number of successes (out of the total number of trials) - again, we'll need this for model fitting\n-   `prop_correct`; this is $successes/total$, which we'll use for plotting but not model fitting\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(levers, aes(x = rat_age, \n                   y = prop_correct,\n                   colour = sex)) +\n  facet_wrap(~ stress_type) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n(\n  ggplot(levers, aes(x='rat_age', y='prop_correct', colour='sex')) +\n  geom_point() +\n  facet_wrap('~stress_type')\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<plotnine.ggplot.ggplot object at 0x17abcf950>\n```\n\n\n:::\n:::\n\n:::\n\nThere's some visual evidence of an interaction here. In the `control` group, it seems like older rats perform a little better on the task, but there's not much effect of `sex`.\n\nMeanwhile, in the `stressed` group, the female rats seem to be performing better than the male ones.\n\n#### Fit a model\n\nLet's assess those relationships by fitting a logistic model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new variable for the number of incorrect presses\nlevers <- levers %>%\n  mutate(incorrect_presses = trials - correct_presses)\n\n# Fit the model\nglm_lev <- glm(cbind(correct_presses, incorrect_presses) ~ stress_type * sex + rat_age,\n               family = binomial,\n               data = levers)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Create a new variable for the number of incorrect presses\nlevers['incorrect_presses'] = levers['trials'] - levers['correct_presses']\n\nmodel = smf.glm(formula = \"correct_presses + incorrect_presses ~ stress_type * sex + rat_age\",\n                family = sm.families.Binomial(),\n                data = levers)\n\nglm_lev = model.fit()\n```\n:::\n\n:::\n\n#### Make model predictions\n\nUsing this model, let's make a prediction of the expected proportion of correct lever presses for:\n\n- a male rat\n- 8 weeks old\n- in the stressed condition\n\nFirst, we look at a summary of the model to extract the beta coefficients we need.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_lev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(correct_presses, incorrect_presses) ~ stress_type * \n    sex + rat_age, family = binomial, data = levers)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                 -2.51920    0.41956  -6.004 1.92e-09 ***\nstress_typestressed          0.93649    0.16166   5.793 6.92e-09 ***\nsexmale                      0.50633    0.18125   2.794  0.00521 ** \nrat_age                      0.13458    0.03197   4.210 2.56e-05 ***\nstress_typestressed:sexmale -1.43779    0.24921  -5.769 7.96e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 120.646  on 61  degrees of freedom\nResidual deviance:  60.331  on 57  degrees of freedom\nAIC: 275.8\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nglm_lev.summary()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>   <td>['correct_presses', 'incorrect_presses']</td> <th>  No. Observations:  </th>  <td>    62</td> \n</tr>\n<tr>\n  <th>Model:</th>                              <td>GLM</td>                   <th>  Df Residuals:      </th>  <td>    57</td> \n</tr>\n<tr>\n  <th>Model Family:</th>                    <td>Binomial</td>                 <th>  Df Model:          </th>  <td>     4</td> \n</tr>\n<tr>\n  <th>Link Function:</th>                     <td>Logit</td>                  <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                            <td>IRLS</td>                   <th>  Log-Likelihood:    </th> <td> -132.90</td>\n</tr>\n<tr>\n  <th>Date:</th>                        <td>Mon, 21 Jul 2025</td>             <th>  Deviance:          </th> <td>  60.331</td>\n</tr>\n<tr>\n  <th>Time:</th>                            <td>17:00:03</td>                 <th>  Pearson chi2:      </th>  <td>  59.5</td> \n</tr>\n<tr>\n  <th>No. Iterations:</th>                      <td>4</td>                    <th>  Pseudo R-squ. (CS):</th>  <td>0.6220</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>                 <td>nonrobust</td>                <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n                   <td></td>                      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>                           <td>   -2.5192</td> <td>    0.420</td> <td>   -6.004</td> <td> 0.000</td> <td>   -3.342</td> <td>   -1.697</td>\n</tr>\n<tr>\n  <th>stress_type[T.stressed]</th>             <td>    0.9365</td> <td>    0.162</td> <td>    5.793</td> <td> 0.000</td> <td>    0.620</td> <td>    1.253</td>\n</tr>\n<tr>\n  <th>sex[T.male]</th>                         <td>    0.5063</td> <td>    0.181</td> <td>    2.794</td> <td> 0.005</td> <td>    0.151</td> <td>    0.862</td>\n</tr>\n<tr>\n  <th>stress_type[T.stressed]:sex[T.male]</th> <td>   -1.4378</td> <td>    0.249</td> <td>   -5.769</td> <td> 0.000</td> <td>   -1.926</td> <td>   -0.949</td>\n</tr>\n<tr>\n  <th>rat_age</th>                             <td>    0.1346</td> <td>    0.032</td> <td>    4.210</td> <td> 0.000</td> <td>    0.072</td> <td>    0.197</td>\n</tr>\n</table>\n```\n\n:::\n:::\n\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_pred <- -2.519 + \n            0.937 * 1 +     # 1 for stressed\n            0.506 * 1 +     # 1 for male\n            -1.438 * 1 +    # 1 for stressed:male\n            0.135 * 8       # 8 weeks old\n\n# these will give identical results\nexp(lin_pred) / (1 + exp(lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1924762\n```\n\n\n:::\n\n```{.r .cell-code}\n1 / (1 + exp(-lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1924762\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlin_pred = (-2.519 + \n           0.937 * 1 +     # 1 for stressed\n           0.506 * 1 +     # 1 for male\n           -1.438 * 1 +    # 1 for stressed:male\n           0.135 * 8)       # 8 weeks old\n\n# these will give identical results\nmath.exp(lin_pred) / (1 + math.exp(lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.19247620289503575\n```\n\n\n:::\n\n```{.python .cell-code}\n1 / (1 + math.exp(-lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.19247620289503575\n```\n\n\n:::\n:::\n\n:::\n\nThis means we would expect an 8 week old stressed male rat to make approximately 19% correct button presses, on average.\n::::\n\n:::: {.callout-tip collapse=\"true\"}\n### Visualising glm_lev\n\nThis is not part of the exercise - it involves quite a bit of new code - but it's something you might want to do with your own data, so we'll show it here.\n\nWith multiple predictors, you will have multiple logistic curves. What do they look like, and how can you produce them?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe first thing we do is create a grid covering all the levels of our categorical predictor, and the full range of available values we have for the continuous predictor. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create prediction grid\nnew_data <- expand.grid(stress_type = levels(factor(levers$stress_type)),\n                        sex = levels(factor(levers$sex)),\n                        rat_age = seq(min(levers$rat_age), max(levers$rat_age), length.out = 100))\n\nhead(new_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  stress_type    sex  rat_age\n1     control female 9.000000\n2    stressed female 9.000000\n3     control   male 9.000000\n4    stressed   male 9.000000\n5     control female 9.080808\n6    stressed female 9.080808\n```\n\n\n:::\n:::\n\n\nThen, we use the `predict` function to figure out the expected proportion of correct responses at each combination of predictors that exists in that grid.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict proportion of correct responses\nnew_data$predicted_prob <- predict(glm_lev, newdata = new_data, type = \"response\")\n```\n:::\n\n\nWe can now use this grid of predictions to produce some nice lines, on top of the actual data points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(levers, aes(x = rat_age, y = prop_correct,\n                     color = stress_type, linetype = sex)) +\n  geom_point() +\n  geom_line(data = new_data, aes(y = predicted_prob), linewidth = 1)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-proportion_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nYou could facet this plot further if you wanted to, but all the lines together help make the picture quite clear - we can see the interaction between `stress:sex` quite clearly!\n\n## Python\n\nThe code to achieve this in Python very quickly becomes long, ugly and unwieldy.\n\nIf you're absolutely determined to produce plots like this, you might want to use an IDE that lets you use both R and Python, and switch briefly into R for this!\n:::\n::::\n:::\n\n\n### Stem cells {#sec-exr_stemcells}\n\n::: {.callout-exercise #ex-stemcells}\n{{< level 3 >}}\n\nFor this exercise, you will need the dataset `stemcells.csv`.\n\nThere's no worked answer for this dataset - we recommend that you compare and discuss your answer with a neighbour.\n\nIn this dataset, each row represents a unique culture or population of stem cells (a plate). Each plate is exposed to one of three different `growth_factor_conc` concentration levels (low, medium or high). \n\nThe researcher wanted to record how much of a particular bio-marker was being expressed in the plate, to quantify cell differentiation.\n\nShe measured this outcome in multiple ways:\n\n-   `marker_positive_cells`, the number of cells expressing the marker\n-   `prop_positive_cells`, the proportion of the `total_cells` count that have the marker\n-   `mean_marker_intensity`, the normalised average fluorescence intensity across the plate, measured on a scale between 0 and 1\n\nBetween plates, there was also variation in the `time` (days) before observation. This variable should be included as a control/covariate of no interest.\n\nYou should:\n\n1. Produce a scatterplot of the data\n2. Decide which outcome measurement is appropriate for a logistic regression\n3. Fit an appropriate logistic model (with two predictor variables)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstemcells <- read_csv(\"data/stemcells.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nstemcells = pd.read_csv(\"data/stemcells.csv\")\n```\n:::\n\n:::\n\n:::: {.callout-tip collapse=\"true\"}\n#### Hint #1\n\nRemember, you'll need to input & combine two values to make up your response variable.\n\nWe won't give away what those variables are (that defeats the point a bit!) but the appropriate code for fitting your logistic regression should look something like this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_cells <- glm(cbind(var, var) ~ growth_factor_conc + time, \n                 family = binomial,\n                 data = stemcells)\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmodel = smf.glm(formula = \"var + var ~ growth_factor_conc + time\",\n                family = sm.families.Binomial(),\n                data = stemcells)\n\nglm_cells = model.fit()\n```\n:::\n\n:::\n::::\n\n:::: {.callout-tip collapse=\"true\"}\n#### Hint #2\n\nIf you're struggling to figure out which of the outcome measures capture which information, here's a little visualisation:\n\n![Measuring markers of differentiation in stem cells](images/stem-cells.png){width=50%}\n::::\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key points\n\n-   We can use a logistic model for proportional response variables, just as we did with binary variables\n-   Proportional response variables are made up of a number of trials (success/failure), and should not be confused with fractions/percentages\n-   Using the equation of a logistic model, we can predict the expected proportion of \"successful\" trials\n:::\n",
    "supporting": [
      "glm-practical-logistic-proportion_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}