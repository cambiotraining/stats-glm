{
  "hash": "e721a03ca2ac0a4865def5fb2f6b64f5",
  "result": {
    "markdown": "---\ntitle: \"Binary response\"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {.callout-tip}\n## Learning outcomes\n\n**Questions**\n\n-   How do we analyse data with a binary outcome?\n-   Can we test if our model is any good?\n-   Be able to perform a logistic regression with a binary outcome\n-   Predict outcomes of new data, based on a defined model\n\n**Objectives**\n\n- Be able to analyse binary outcome data\n- Understand different methods of testing model fit\n- Be able to make model predictions\n:::\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n### Functions\n\n## Python\n\n### Libraries\n### Functions\n:::\n:::\n\nThe example in this section uses the following data set:\n\n`data/diabetes.csv`\n\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose test and the diastolic blood pressure for each of 767 women. The variables are called `test_result`, `glucose` and `diastolic`.\n\n## Load and visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read_csv(\"data/diabetes.csv\")\n```\n:::\n\n\n## Python\n:::\n\nLooking at the data, we can see that the `test_result` column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\n\nWe'll deal with that issue later.\n\nWe can plot the data, by outcome:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  ggplot(aes(x = factor(test_result), y = glucose)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\n\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea...\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  ggplot(aes(x = glucose, y = test_result)) +\n  geom_smooth(method = \"lm\") +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nOf course this is rubbish - we can't have test results outside the range of \\[0, 1\\].\n\nBut for the sake of exploration, let's look at the assumptions:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  lm(test_result ~ glucose, data = .) %>% \n  resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n```\n:::\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Python\n:::\n\nThey're ~~pretty~~ extremely bad.\n\n-   The response is not linear (Residual Plot, binary response plot, common sense).\n-   The residuals are not distributed normally (Q-Q Plot)\n-   The variance is not homogeneous across the predicted values (Location-Scale Plot)\n-   But - there is always a silver lining - we don't have influential data points.\n\n::: {.callout-note}\n## Viewing residuals\n\nAnother way of viewing the residuals (apart from the Q-Q plot) is as a dot-plot. In R, the `ggdist` and `distributional` packages are extremely useful for this kind of stuff.\n\nWhat I'm doing here is:\n\n-   define the model\n-   create a normal distribution with $\\mu = 0$ and $\\sigma = 0.415$ (I've extracted the $\\sigma$ value from the residuals with `rstatix::get_summary_stats()`)\n-   plot the residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes %>% \n  lm(test_result ~ glucose, data = .) %>%\n  resid() %>%\n  as_tibble() %>%\n  # rstatix::get_summary_stats()\n  ggplot(aes(x = value)) +\n  stat_dist_halfeye(aes(dist = dist_normal(0, 0.415)),\n                    orientation = \"horizontal\") +\n  stat_dotsinterval(aes(x = value),\n                    orientation = \"horizontal\",\n                    fill = \"firebrick\", scale = 1) +\n  labs(title = \"Linear model (diabetes)\", y = \"probability\", x = NULL)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThis again shows us that the residuals are really not normally distributed. If they were, then they would overlap much more closely with the distribution (in grey).\n:::\n\n## Creating a suitable model\n\nSo far we've established that using a simple linear model describe a potential relationship between `glucose` levels and the probability of getting a positive test result is not a good idea. So, what _can_ we do?\n\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the $[0, 1]$ range.\n\nThere are many mathematical functions that produce S-shaped graphs. The **logistic function** is one of them and well-suited to these kind of data.\n\nIn the most simple form a logistic function is written like this:\n\n$Y = \\frac{1}{1 + \\exp(-X)}$\n\n:::{.callout-important}\n## Euler's number\n\nIn mathematics, $\\rm e$ represents a constant of around 2.718. Another notation is $\\exp$, which is often used when notations become a bit cumbersome. Here, I exclusively use the $\\exp$ notation for consistency.\n:::\n\nWe can _generalise_ this, by writing it as follows:\n\n$Y = \\frac{1}{1 + \\exp-(\\beta_0 + \\beta_1X)}$\n\nNote that the $\\beta_0 + \\beta_1X$ part is identical to the formula of a straight line. We've come across this before when we were doing simple linear regression!\n\nThe rest of the function is what makes the straight line curve into its characteristic S shape. The middle of the S (where $Y = 0.5$) occurs when $X = \\frac{-b}{a}$.\n\n::: {.callout-important}\n## The logistic function\n\nThe shape of the logistic function is hugely influenced by the different parameter, in particular $\\beta_1$. The plots below show different situations, where $\\beta_0 = 0$ in all cases, but $\\beta_1$ varies.\n\nThe first plot shows the logistic function in its simplest form, with the others showing the effect of varying $\\beta_1$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n* when $\\beta_1 = 1$, this gives the simplest logistic function\n* when $\\beta_1 = 0$ gives a horizontal line, with $Y = 1/(1+exp(-\\beta_0X)$\n* when $\\beta_1$ is negative flips the curve around, so it slopes down\n* when $\\beta_1$ is very large then the curve becomes extremely steep\n:::\n\nWe can fit such an S-shaped curve to our `diabetes` data set, by creating a generalised linear model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nIn R we have a few options to do this, and by far the most familiar function would be `glm()`. Here we save the model in an object called `dia_glm`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndia_glm <- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n```\n:::\n\n\nThe format of this function is similar to that used by the `lm()` function for linear models. The important difference is that we must specify the _family_ of error distribution to use. For logistic regression we must set the family to **binomial**.\n\nIf you forget to set the `family` argument, then the `glm()` function will perform a standard linear model fit, identical to what the `lm()` function would do.\n\n## Python\n:::\n\n## Model output\n\nThat's the easy part done! The trickier part is interpreting the output. First of all, we'll get some summary information.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dia_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.1353  -0.7819  -0.5189   0.8269   2.2832  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.611732   0.442289  -12.69   <2e-16 ***\nglucose      0.039510   0.003398   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\n\n## Python\n:::\n\n## Exercise\n\n::: {.callout-tip collapse=\"true\"}\n## Answer\n::: {.panel-tabset group=\"language\"}\n## R\n## Python\n:::\n:::\n\n## Key points\n\n::: {.callout-note}\n-   We use a logistic regression to model a binary response\n-   We can feed new observations into the model and get probabilities for the outcome\n:::\n",
    "supporting": [
      "glm-practical-logistic-binary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}