{
  "hash": "f9433deeb8fca2ca7f5870d339f0aecd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Binary response\"\nlightbox: true\n---\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n::: callout-tip\n## Learning outcomes\n\n-   Be able to fit an appropriate GLM binary outcome data\n-   Be able to make model predictions\n:::\n\n## Libraries and functions\n\n:::: {.callout-note collapse=\"true\"}\n## Click to expand\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n### Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(ggResidpanel)\n```\n:::\n\n\n### Functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create diagnostic plots\nggResidpanel::resid_panel()\n```\n:::\n\n\n## Python\n\n### Libraries\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# A maths library\nimport math\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf\n\n# Needed for additional probability functionality\nfrom scipy.stats import *\n\n# Custom function to create diagnostic plots\n# if in your main working directory:\nfrom dgplots import *\n```\n:::\n\n\nNote: you can download the `dgplots` script [here](setup_files/dgplots.py).\n:::\n::::\n\nThe example in this section uses the following data set:\n\n`data/finches_early.csv`\n\nThese data come from an analysis of gene flow across two finch species [@lamichhaney2020]. They are slightly adapted here for illustrative purposes.\n\nThe data focus on two species, *Geospiza fortis* and *G. scandens*. The original measurements are split by a uniquely timed event: a particularly strong El Ni√±o event in 1983. This event changed the vegetation and food supply of the finches, allowing F1 hybrids of the two species to survive, whereas before 1983 they could not. The measurements are classed as `early` (pre-1983) and `late` (1983 onwards).\n\nHere we are looking only at the `early` data. We are specifically focussing on the beak shape classification, which we saw earlier in @fig-beak_shape_glm.\n\n## Load and visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nearly_finches <- read_csv(\"data/finches_early.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nearly_finches_py = pd.read_csv(\"data/finches_early.csv\")\n```\n:::\n\n:::\n\nLooking at the data, we can see that the `pointed_beak` column contains zeros and ones. These are actually yes/no classification outcomes and not numeric representations.\n\nWe'll have to deal with this soon. For now, we can plot the data:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(early_finches,\n       aes(x = factor(pointed_beak),\n          y = blength)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nWe could just give Python the `pointed_beak` data directly, but then it would view the values as numeric. Which doesn't really work, because we have two groups as such: those with a pointed beak (`1`), and those with a blunt one (`0`).\n\nWe can force Python to temporarily covert the data to a factor, by making the `pointed_beak` column an `object` type. We can do this directly inside the `ggplot()` function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(early_finches_py,\n         aes(x = early_finches_py.pointed_beak.astype(object),\n             y = \"blength\")) +\n     geom_boxplot())\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-9-1.png){width=614}\n:::\n:::\n\n:::\n\nIt looks as though the finches with blunt beaks generally have shorter beak lengths.\n\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point())\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-11-1.png){width=614}\n:::\n:::\n\n:::\n\nThis presents us with a bit of an issue. We could fit a linear regression model to these data, although we already know that this is a bad idea...\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(early_finches,\n       aes(x = blength, y = pointed_beak)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(early_finches_py,\n         aes(x = \"blength\",\n             y = \"pointed_beak\")) +\n     geom_point() +\n     geom_smooth(method = \"lm\",\n                 colour = \"blue\",\n                 se = False))\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-13-1.png){width=614}\n:::\n:::\n\n:::\n\nOf course this is rubbish - we can't have a beak classification outside the range of $[0, 1]$. It's either blunt (`0`) or pointed (`1`).\n\nBut for the sake of exploration, let's look at the assumptions:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_bks <- lm(pointed_beak ~ blength,\n             data = early_finches)\n\nresid_panel(lm_bks,\n            plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n            smoother = TRUE)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n\n## Python\n\nFirst, we create a linear model:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.ols(formula = \"pointed_beak ~ blength\",\n                data = early_finches_py)\n# and get the fitted parameters of the model\nlm_bks_py = model.fit()\n```\n:::\n\n\nNext, we can create the diagnostic plots:\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom dgplots import *\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndgplots(lm_bks_py)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-18-1.png){width=1152}\n:::\n:::\n\n\n:::\n\nThey're ~~pretty~~ extremely bad.\n\n-   The response is not linear (Residual Plot, binary response plot, common sense).\n-   The residuals do not appear to be distributed normally (Q-Q Plot)\n-   The variance is not homogeneous across the predicted values (Location-Scale Plot)\n-   But - there is always a silver lining - we don't have influential data points.\n\n## Creating a suitable model\n\nSo far we've established that using a simple linear model to describe a potential relationship between beak length and the probability of having a pointed beak is not a good idea. So, what *can* we do?\n\nOne of the ways we can deal with binary outcome data is by performing a logistic regression. Instead of fitting a straight line to our data, and performing a regression on that, we fit a line that has an S shape. This avoids the model making predictions outside the $[0, 1]$ range.\n\nWe described our standard linear relationship as follows:\n\n$Y = \\beta_0 + \\beta_1X$\n\nWe can now map this to our non-linear relationship via the **logistic link function**:\n\n$Y = \\frac{\\exp(\\beta_0 + \\beta_1X)}{1 + \\exp(\\beta_0 + \\beta_1X)}$\n\nNote that the $\\beta_0 + \\beta_1X$ part is identical to the formula of a straight line.\n\nThe rest of the function is what makes the straight line curve into its characteristic S shape.\n\n::: {.callout-note collapse=\"true\"}\n## Euler's number ($\\exp$): would you like to know more?\n\nIn mathematics, $\\rm e$ represents a constant of around 2.718. Another notation is $\\exp$, which is often used when notations become a bit cumbersome. Here, I exclusively use the $\\exp$ notation for consistency.\n:::\n\n::: callout-important\n## The logistic function\n\nThe shape of the logistic function is hugely influenced by the different parameters, in particular $\\beta_1$. The plots below show different situations, where $\\beta_0 = 0$ in all cases, but $\\beta_1$ varies.\n\nThe first plot shows the logistic function in its simplest form, with the others showing the effect of varying $\\beta_1$.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-19-3.png){width=672}\n:::\n:::\n\n\n-   when $\\beta_1 = 1$, this gives the simplest logistic function\n-   when $\\beta_1 = 0$ gives a horizontal line, with $Y = \\frac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}$\n-   when $\\beta_1$ is negative flips the curve around, so it slopes down\n-   when $\\beta_1$ is very large then the curve becomes extremely steep\n:::\n\nWe can fit such an S-shaped curve to our `early_finches` data set, by creating a generalised linear model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nIn R we have a few options to do this, and by far the most familiar function would be `glm()`. Here we save the model in an object called `glm_bks`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_bks <- glm(pointed_beak ~ blength,\n               family = binomial,\n               data = early_finches)\n```\n:::\n\n\nThe format of this function is similar to that used by the `lm()` function for linear models. The important difference is that we must specify the *family* of error distribution to use. For logistic regression we must set the family to **binomial**.\n\nIf you forget to set the `family` argument, then the `glm()` function will perform a standard linear model fit, identical to what the `lm()` function would do.\n\n## Python\n\nIn Python we have a few options to do this, and by far the most familiar function would be `glm()`. Here we save the model in an object called `glm_bks_py`:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.glm(formula = \"pointed_beak ~ blength\",\n                family = sm.families.Binomial(),\n                data = early_finches_py)\n# and get the fitted parameters of the model\nglm_bks_py = model.fit()\n```\n:::\n\n\nThe format of this function is similar to that used by the `ols()` function for linear models. The important difference is that we must specify the *family* of error distribution to use. For logistic regression we must set the family to **binomial**. This is buried deep inside the `statsmodels` package and needs to be defined as `sm.families.Binomial()`.\n:::\n\n## Model output\n\nThat's the easy part done! The trickier part is interpreting the output. First of all, we'll get some summary information.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_bks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = pointed_beak ~ blength, family = binomial, data = early_finches)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  -43.410     15.250  -2.847  0.00442 **\nblength        3.387      1.193   2.839  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 84.5476  on 60  degrees of freedom\nResidual deviance:  9.1879  on 59  degrees of freedom\nAIC: 13.188\n\nNumber of Fisher Scoring iterations: 8\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_bks_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:           pointed_beak   No. Observations:                   61\nModel:                            GLM   Df Residuals:                       59\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -4.5939\nDate:                Tue, 22 Jul 2025   Deviance:                       9.1879\nTime:                        08:25:49   Pearson chi2:                     15.1\nNo. Iterations:                     8   Pseudo R-squ. (CS):             0.7093\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept    -43.4096     15.250     -2.847      0.004     -73.298     -13.521\nblength        3.3866      1.193      2.839      0.005       1.049       5.724\n==============================================================================\n```\n\n\n:::\n:::\n\n:::\n\nThere‚Äôs a lot to unpack here, but let's start with what we're familiar with: coefficients!\n\n## Parameter interpretation\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe coefficients or parameters can be found in the `Coefficients` block. The main numbers to extract from the output are the two numbers underneath `Estimate.Std`:\n\n```         \nCoefficients:\n            Estimate Std.\n(Intercept)  -43.410\nblength        3.387 \n```\n\n## Python\n\nRight at the bottom is a table showing the model coefficients. The main numbers to extract from the output are the two numbers in the `coef` column:\n\n```         \n======================\n                 coef\n----------------------\nIntercept    -43.4096\nblength        3.3866\n======================\n```\n:::\n\nThese are the coefficients of the logistic model equation and need to be placed in the correct equation if we want to be able to calculate the probability of having a pointed beak for a given beak length.\n\nThe $p$ values at the end of each coefficient row merely show whether that particular coefficient is significantly different from zero. This is similar to the $p$ values obtained in the summary output of a linear model. As with continuous predictors in simple models, these $p$ values can be used to decide whether that predictor is important (so in this case beak length appears to be significant). However, these $p$ values aren‚Äôt great to work with when we have multiple predictor variables, or when we have categorical predictors with multiple levels (since the output will give us a $p$ value for each level rather than for the predictor as a whole).\n\nWe can use the coefficients to calculate the probability of having a pointed beak for a given beak length:\n\n$$ P(pointed \\ beak) = \\frac{\\exp(-43.41 + 3.39 \\times blength)}{1 + \\exp(-43.41 + 3.39 \\times blength)} $$\n\nHaving this formula means that we can calculate the probability of having a pointed beak for any beak length. How do we work this out in practice?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(-43.41 + 3.39 * 15) / (1 + exp(-43.41 + 3.39 * 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9994131\n```\n\n\n:::\n:::\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(-43.41 + 3.39 * 10) / (1 + exp(-43.41 + 3.39 * 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.410155e-05\n```\n\n\n:::\n:::\n\n\n## Python\n\nWell, the probability of having a pointed beak if the beak length is large (for example 15 mm) can be calculated as follows:\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# import the math library\nimport math\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmath.exp(-43.41 + 3.39 * 15) / (1 + math.exp(-43.41 + 3.39 * 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.9994130595039192\n```\n\n\n:::\n:::\n\n\nIf the beak length is small (for example 10 mm), the probability of having a pointed beak is extremely low:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmath.exp(-43.41 + 3.39 * 10) / (1 + math.exp(-43.41 + 3.39 * 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n7.410155028945912e-05\n```\n\n\n:::\n:::\n\n:::\n\nWe can calculate the the probabilities for all our observed values and if we do that then we can see that the larger the beak length is, the higher the probability that a beak shape would be pointed. I'm visualising this together with the logistic curve, where the blue points are the calculated probabilities:\n\n:::: {.callout-note collapse=\"true\"}\n## Code available here\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_bks |> \n  augment(type.predict = \"response\") |> \n  ggplot() +\n  geom_point(aes(x = blength, y = pointed_beak)) +\n  geom_line(aes(x = blength, y = .fitted),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = blength, y = .fitted),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(early_finches_py) +\n  geom_point(aes(x = \"blength\", y = \"pointed_beak\")) +\n  geom_line(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n            linetype = \"dashed\",\n            colour = \"blue\") +\n  geom_point(aes(x = \"blength\", y = glm_bks_py.fittedvalues),\n             colour = \"blue\", alpha = 0.5) +\n  labs(x = \"beak length (mm)\",\n       y = \"Probability\"))\n\np.show()\n```\n:::\n\n:::\n::::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Predicted probabilities for beak classification](glm-practical-logistic-binary_files/figure-html/fig-beak_class_glm_probs-2.png){#fig-beak_class_glm_probs width=672}\n:::\n:::\n\n\nThe graph shows us that, based on the data that we have and the model we used to make predictions about our response variable, the probability of seeing a pointed beak increases with beak length.\n\nShort beaks are more closely associated with the bluntly shaped beaks, whereas long beaks are more closely associated with the pointed shape. It's also clear that there is a range of beak lengths (around 13 mm) where the probability of getting one shape or another is much more even.\n\n## Influential observations\n\nBy this point, if we were fitting a linear model, we would want to check whether the statistical assumptions have been met.\n\nThe same is true for a generalised linear model. However, as explained in the background chapter, we can't really use the standard diagnostic plots to assess assumptions. (And the assumptions of a GLM are not the same as a linear model.)\n\nThere will be a whole chapter later on that focuses on assumptions and how to check them.\n\nFor now, there is one thing that we can do that might be familiar: look for influential points using the Cook‚Äôs distance plot.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid_panel(glm_bks, plots = \"cookd\")\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nAs always, there are different ways of doing this. Here we extract the Cook's d values from the `glm` object and put them in a Pandas DataFrame. We can then use that to plot them in a lollipop or stem plot.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# extract the Cook's distances\nglm_bks_py_resid = pd.DataFrame(glm_bks_py.\n                                get_influence().\n                                summary_frame()[\"cooks_d\"])\n\n# add row index \nglm_bks_py_resid['obs'] = glm_bks_py_resid.reset_index().index\n```\n:::\n\n\nWe now have two columns:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nglm_bks_py_resid.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        cooks_d  obs\n0  1.854360e-07    0\n1  3.388262e-07    1\n2  3.217960e-05    2\n3  1.194847e-05    3\n4  6.643975e-06    4\n```\n\n\n:::\n:::\n\n\nWe can use these to create the plot:\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(glm_bks_py_resid,\n         aes(x = \"obs\",\n             y = \"cooks_d\")) +\n     geom_segment(aes(x = \"obs\", y = \"cooks_d\", xend = \"obs\", yend = 0)) +\n     geom_point())\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-35-1.png){width=614}\n:::\n:::\n\n:::\n\nThis shows that there are no very obvious influential points. You could regard point `34` as potentially influential (it's got a Cook's distance of around `0.8`), but I'm not overly worried.\n\nIf we were worried, we'd remove the troublesome data point, re-run the analysis and see if that changes the statistical outcome. If so, then our entire (statistical) conclusion hinges on one data point, which is not a very robust bit of research. If it *doesn't* change our significance, then all is well, even though that data point is influential.\n\n## Exercises\n\n### Diabetes {#sec-exr_diabetes}\n\n::: {.callout-exercise #ex-diabetes}\n#### Diabetes\n\n{{< level 2 >}}\n\nFor this exercise we'll be using the data from `data/diabetes.csv`.\n\nThis is a data set comprising 768 observations of three variables (one dependent and two predictor variables). This records the results of a diabetes test result as a binary variable (1 is a positive result, 0 is a negative result), along with the result of a glucose tolerance test and the diastolic blood pressure for each of 768 women. The variables are called `test_result`, `glucose` and `diastolic`.\n\nWe want to look at the relationship between `glucose` tolerance and diabetes test results. To investigate this, do the following:\n\n1.  Load and visualise the data\n2.  Create a suitable model\n3.  Calculate the probability of a positive diabetes test result for a glucose tolerance test value of `glucose = 150`\n\n::: {.callout-answer collapse=\"true\"}\n#### Load and visualise the data\n\nFirst we load the data, then we visualise it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiabetes <- read_csv(\"data/diabetes.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndiabetes_py = pd.read_csv(\"data/diabetes.csv\")\n```\n:::\n\n:::\n\nLooking at the data, we can see that the `test_result` column contains zeros and ones. These are yes/no test result outcomes and not actually numeric representations.\n\nWe'll have to deal with this soon. For now, we can plot the data, by outcome:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(diabetes,\n       aes(x = factor(test_result),\n           y = glucose)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n## Python\n\nWe could just give Python the `test_result` data directly, but then it would view the values as numeric. Which doesn't really work, because we have two groups as such: those with a negative diabetes test result, and those with a positive one.\n\nWe can force Python to temporarily covert the data to a factor, by making the `test_result` column an `object` type. We can do this directly inside the `ggplot()` function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(diabetes_py,\n         aes(x = diabetes_py.test_result.astype(object),\n             y = \"glucose\")) +\n     geom_boxplot())\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-39-1.png){width=614}\n:::\n:::\n\n:::\n\nIt looks as though the patients with a positive diabetes test have slightly higher glucose levels than those with a negative diabetes test.\n\nWe can visualise that differently by plotting all the data points as a classic binary response plot:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(diabetes,\n       aes(x = glucose,\n           y = test_result)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-40-3.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(diabetes_py,\n         aes(x = \"glucose\",\n             y = \"test_result\")) +\n     geom_point())\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-41-1.png){width=614}\n:::\n:::\n\n:::\n\n#### Create a suitable model\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe'll use the `glm()` function to create a generalised linear model. Here we save the model in an object called `glm_dia`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_dia <- glm(test_result ~ glucose,\n               family = binomial,\n               data = diabetes)\n```\n:::\n\n\nThe format of this function is similar to that used by the `lm()` function for linear models. The important difference is that we must specify the *family* of error distribution to use. For logistic regression we must set the family to **binomial**.\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create a linear model\nmodel = smf.glm(formula = \"test_result ~ glucose\",\n                family = sm.families.Binomial(),\n                data = diabetes_py)\n# and get the fitted parameters of the model\nglm_dia_py = model.fit()\n```\n:::\n\n:::\n\nLet's look at the model parameters:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm_dia)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = test_result ~ glucose, family = binomial, data = diabetes)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -5.611732   0.442289  -12.69   <2e-16 ***\nglucose      0.039510   0.003398   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 936.6  on 727  degrees of freedom\nResidual deviance: 752.2  on 726  degrees of freedom\nAIC: 756.2\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(glm_dia_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:            test_result   No. Observations:                  728\nModel:                            GLM   Df Residuals:                      726\nModel Family:                Binomial   Df Model:                            1\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -376.10\nDate:                Tue, 22 Jul 2025   Deviance:                       752.20\nTime:                        08:25:50   Pearson chi2:                     713.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2238\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -5.6117      0.442    -12.688      0.000      -6.479      -4.745\nglucose        0.0395      0.003     11.628      0.000       0.033       0.046\n==============================================================================\n```\n\n\n:::\n:::\n\n:::\n\nRight now, we're interested in the coefficients (we'll look at significance more in subsequent chapters).\n\nWe have an intercept of `-5.61` and `0.0395` for `glucose`. We can use these coefficients to write a formula that describes the potential relationship between the probability of having a positive test result, dependent on the glucose tolerance level value:\n\n$$ P(positive \\ test\\ result) = \\frac{\\exp(-5.61 + 0.04 \\times glucose)}{1 + \\exp(-5.61 + 0.04 \\times glucose)} $$\n\n#### Calculating probabilities\n\nUsing the formula above, we can now calculate the probability of having a positive test result, for a given `glucose` value. If we do this for `glucose = 150`, we get the following:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(-5.61 + 0.04 * 150) / (1 + exp(-5.61 + 0.04 * 150))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5962827\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmath.exp(-5.61 + 0.04 * 150) / (1 + math.exp(-5.61 + 0.04 * 150))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5962826992967878\n```\n\n\n:::\n:::\n\n:::\n\nThis tells us that the probability of having a positive diabetes test result, given a glucose tolerance level of 150 is around 60 %.\n:::::::::\n::::::::::\n\n### Aphids {#sec-exr_aphids}\n\n::::::::: {.callout-exercise #ex-aphids}\n#### Aphids\n\n{{< level 2 >}}\n\nIn this exercise we'll use the `data/aphids.csv` dataset.\n\nEach row of these data represents a unique rose plant. For each plant, the researcher recorded:\n\n-   The number of unbloomed `buds`\n-   Which type of cultivated variety (`cultivar`) the rose was (`mozart` or `harmonie`)\n-   Whether or not aphids were present\n\nYou should:\n\n1.  Load and visualise the data\n2.  Fit an appropriate model\n3.  Calculate the probability of aphids being present on a `harmonie` rose with 5 unbloomed `buds`\n\n:::::::: {.callout-answer collapse=\"true\"}\n#### Load and visualise\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\naphids <- read_csv(\"data/aphids.csv\")\n```\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\naphids = pd.read_csv(\"data/aphids.csv\")\n```\n:::\n\n:::\n\nThere are multiple ways we could visualise these data, but let's try a scatterplot here:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(aphids,\n       aes(x = factor(aphids_present),\n           y = factor(buds),\n           colour = cultivar)) +\n  geom_jitter(width = 0.3)\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\np = (ggplot(aphids, aes(x = aphids.aphids_present.astype(object),\n                    y = aphids.buds.astype(object),\n                    colour = \"cultivar\")) +\n   geom_jitter(width = 0.3))\n\np.show()\n```\n\n::: {.cell-output-display}\n![](glm-practical-logistic-binary_files/figure-html/unnamed-chunk-51-1.png){width=614}\n:::\n:::\n\n:::\n\nThe plot gives the impression that the `mozart` roses have aphids present more often than the `harmonie` roses, but it's hard to tell whether there's an effect of `buds` from this graph.\n\n#### Fit a logistic model\n\nTo quantify the relationship(s), let's fit a logistic model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_aphids <- glm(aphids_present ~ buds + cultivar,\n                  family = binomial,\n                  data = aphids)\n\nsummary(glm_aphids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = aphids_present ~ buds + cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)   \n(Intercept)     -2.0687     0.7483  -2.765  0.00570 **\nbuds             0.2067     0.1262   1.638  0.10149   \ncultivarmozart   1.9621     0.6439   3.047  0.00231 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.666  on 51  degrees of freedom\nAIC: 66.666\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create the model\nmodel = smf.glm(formula = \"aphids_present ~ buds + cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \n# extract fitted parameters\nglm_aphids = model.fit()\n\n# print summary of model\nprint(glm_aphids.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:         aphids_present   No. Observations:                   54\nModel:                            GLM   Df Residuals:                       51\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -30.333\nDate:                Tue, 22 Jul 2025   Deviance:                       60.666\nTime:                        08:25:51   Pearson chi2:                     54.8\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2140\nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          z      P>|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nIntercept             -2.0687      0.748     -2.765      0.006      -3.535      -0.602\ncultivar[T.mozart]     1.9621      0.644      3.047      0.002       0.700       3.224\nbuds                   0.2067      0.126      1.638      0.101      -0.041       0.454\n======================================================================================\n```\n\n\n:::\n:::\n\n:::\n\nIf you were paying close attention when you were learning about regular linear modelling, you might be thinking - isn't it possible that there's an interaction between our two predictors?\n\nYes, it is!\n\nWe can adapt our logistic model to include a third predictor, the `buds:cultivar` interaction. This works exactly like it would in a regular linear model - the syntax should be familiar already.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm_aphids2 <- glm(aphids_present ~ buds * cultivar,\n                  family = binomial,\n                  data = aphids)\n\nsummary(glm_aphids2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = aphids_present ~ buds * cultivar, family = binomial, \n    data = aphids)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)  \n(Intercept)         -1.86699    0.93702  -1.992   0.0463 *\nbuds                 0.16533    0.17365   0.952   0.3411  \ncultivarmozart       1.58251    1.27226   1.244   0.2136  \nbuds:cultivarmozart  0.08758    0.25647   0.342   0.7327  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.670  on 53  degrees of freedom\nResidual deviance: 60.548  on 50  degrees of freedom\nAIC: 68.548\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# create the model\nmodel = smf.glm(formula = \"aphids_present ~ buds * cultivar\",\n                family = sm.families.Binomial(),\n                data = aphids)\n                \n# extract fitted parameters\nglm_aphids2 = model.fit()\n\n# print summary of model\nprint(glm_aphids2.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:         aphids_present   No. Observations:                   54\nModel:                            GLM   Df Residuals:                       50\nModel Family:                Binomial   Df Model:                            3\nLink Function:                  Logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -30.274\nDate:                Tue, 22 Jul 2025   Deviance:                       60.548\nTime:                        08:25:51   Pearson chi2:                     54.3\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.2157\nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept                  -1.8670      0.937     -1.992      0.046      -3.704      -0.030\ncultivar[T.mozart]          1.5825      1.272      1.244      0.214      -0.911       4.076\nbuds                        0.1653      0.174      0.952      0.341      -0.175       0.506\nbuds:cultivar[T.mozart]     0.0876      0.256      0.342      0.733      -0.415       0.590\n===========================================================================================\n```\n\n\n:::\n:::\n\n:::\n\nThe question of which of these models is **better** is something we'll tackle in one of the later chapters.\n\n#### Calculate the probability\n\nFinally, let's use the coefficients from our model to calculate the probability of aphids being present under specific values of our predictors, specifically:\n\n-   `buds` = 8\n-   `cultivar` = `harmonie`\n\nTo keep things simple, we'll use the coefficients from our additive model, but you can easily adapt this code to include the interaction with little effort.\n\nSince we've got two predictors, we're going to be efficient and do this in two stages. First, we'll build the \"linear predictor\" (the linear bit of our equation), and then we'll embed it inside the inverse link function.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlin_pred <- -2.069 + 0.267 * 8 + 1.962 * 0\n\n# these will give identical results\nexp(lin_pred) / (1 + exp(lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5167437\n```\n\n\n:::\n\n```{.r .cell-code}\n1 / (1 + exp(-lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5167437\n```\n\n\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlin_pred = -2.069 + 0.267 * 8 + 1.962 * 0\n\n# these will give identical results\nmath.exp(lin_pred) / (1 + math.exp(lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5167437369156502\n```\n\n\n:::\n\n```{.python .cell-code}\n1 / (1 + math.exp(-lin_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.5167437369156502\n```\n\n\n:::\n:::\n\n:::\n\nWe write a `0` for the `cultivar` variable, because `harmonie` is the reference group (and therefore no adjustment is needed). If we had been making a prediction for the `mozart` rose, we would write a `1` here instead.\n::::::::\n:::::::::\n\n## Summary\n\n::: callout-tip\n#### Key points\n\n-   We use a logistic regression to model a binary response\n-   This uses a logit link function\n-   We can feed new observations into the model and make predictions about the expected likelihood of \"success\", given certain values of the predictor variable(s)\n:::\n",
    "supporting": [
      "glm-practical-logistic-binary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}